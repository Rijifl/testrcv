{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ba5353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# =============================================================================\n",
    "# INTEGRATED BAYESIAN OPTIMIZATION PIPELINE FOR CHEMICAL EXPERIMENTS\n",
    "# =============================================================================\n",
    "# =============================================================================\n",
    "#\n",
    "# WORKFLOW:\n",
    "#   Phase 1: Feature Selection (Cells 1-26)\n",
    "#            → Checkpoint saved\n",
    "#   Phase 2: Bayesian Optimization (Cells 27+)\n",
    "#            → Iterative experiment proposal\n",
    "#\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 1: IMPORTS\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import RidgeCV, LassoCV\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.model_selection import LeaveOneOut, cross_val_score\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel, ConstantKernel, Matern\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "from itertools import combinations\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Create output directory\n",
    "OUTPUT_DIR = Path('bo_pipeline_output')\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"INTEGRATED BAYESIAN OPTIMIZATION PIPELINE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(\"✓ Imports complete\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# =============================================================================\n",
    "# PHASE 1: FEATURE SCREENING\n",
    "# =============================================================================\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PHASE 1: FEATURE SCREENING FOR BAYESIAN OPTIMIZATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 2: CONFIGURATION - PHASE 1\n",
    "# =============================================================================\n",
    "\n",
    "# ┌─────────────────────────────────────────────────────────────────────────┐\n",
    "# │ MODIFY THIS SECTION FOR YOUR DATA                                       │\n",
    "# └─────────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "# Data file settings\n",
    "PHASE1_DATA_FILE = 'data.xlsx'  # Your Excel file with initial experiments\n",
    "PHASE1_SHEET_NAME = 'data'      # Sheet name\n",
    "HEADER_ROW = 5                   # Row number where headers are (0-indexed)\n",
    "\n",
    "# Data structure settings\n",
    "SPLIT_KEYWORD = \"PREDICTED OPTIMUM RUNS\"  # Keyword to split data (or None)\n",
    "STOP_FEATURE = \"Batch ID\"                  # Column name where features end\n",
    "\n",
    "# Response settings\n",
    "RESPONSE_COLUMN = \"Downy Leak\"   # Your response variable name\n",
    "MAXIMIZE_RESPONSE = False        # True if higher is better, False if lower is better\n",
    "\n",
    "# Feature selection settings\n",
    "TARGET_FEATURES = 4              # Target number of features for BO (3-5 recommended)\n",
    "\n",
    "# Thresholds for feature selection\n",
    "CORRELATION_STRONG = 0.4\n",
    "CORRELATION_MODERATE = 0.2\n",
    "VIP_IMPORTANT = 1.0\n",
    "VIP_MODERATE = 0.8\n",
    "MULTICOLLINEARITY_THRESHOLD = 0.7\n",
    "INTERACTION_THRESHOLD = 0.3\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PHASE 1 CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Data file: {PHASE1_DATA_FILE}\")\n",
    "print(f\"  Response: {RESPONSE_COLUMN}\")\n",
    "print(f\"  Target features: {TARGET_FEATURES}\")\n",
    "print(f\"  Optimization: {'Maximize' if MAXIMIZE_RESPONSE else 'Minimize'}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 3: LOAD AND CLEAN DATA\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LOADING DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load Excel file\n",
    "xls = pd.ExcelFile(PHASE1_DATA_FILE, engine='openpyxl')\n",
    "df = pd.read_excel(xls, sheet_name=PHASE1_SHEET_NAME, header=HEADER_ROW)\n",
    "\n",
    "print(f\"✓ Loaded sheet: '{PHASE1_SHEET_NAME}'\")\n",
    "print(f\"  Initial shape: {df.shape}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 4: SPLIT DATAFRAME AT KEYWORD (if applicable)\n",
    "# =============================================================================\n",
    "\n",
    "if SPLIT_KEYWORD:\n",
    "    split_index = df.index[df['Run'] == SPLIT_KEYWORD].tolist()\n",
    "    \n",
    "    if split_index:\n",
    "        split_index = split_index[0]\n",
    "        df_initial = df.iloc[:split_index]\n",
    "        df_optimum = df.iloc[split_index+1:]\n",
    "        print(f\"✓ Split at '{SPLIT_KEYWORD}'\")\n",
    "        print(f\"  df_initial: {len(df_initial)} rows\")\n",
    "        print(f\"  df_optimum: {len(df_optimum)} rows\")\n",
    "    else:\n",
    "        print(f\"⚠️ '{SPLIT_KEYWORD}' not found, using all data\")\n",
    "        df_initial = df.copy()\n",
    "        df_optimum = pd.DataFrame()\n",
    "else:\n",
    "    df_initial = df.copy()\n",
    "    df_optimum = pd.DataFrame()\n",
    "    print(\"✓ No split keyword specified, using all data\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 5: CLEAN DATAFRAMES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CLEANING DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Clean df_initial\n",
    "df_initial = df_initial.drop(index=0, errors='ignore')  # Drop units row if present\n",
    "df_initial = df_initial.dropna(how='all')\n",
    "df_initial = df_initial.reset_index(drop=True)\n",
    "\n",
    "# Clean df_optimum if exists\n",
    "if len(df_optimum) > 0:\n",
    "    df_optimum = df_optimum.dropna(how='all')\n",
    "    df_optimum = df_optimum.reset_index(drop=True)\n",
    "\n",
    "# For Phase 1 feature selection, use only initial experiments\n",
    "# (df_optimum might have been run with different feature sets)\n",
    "df_phase1 = df_initial.copy()\n",
    "\n",
    "print(f\"✓ Using {len(df_phase1)} experiments for feature selection\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 6: DEFINE FEATURE COLUMNS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"IDENTIFYING FEATURES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "columns = df_phase1.columns.tolist()\n",
    "\n",
    "if STOP_FEATURE and STOP_FEATURE in columns:\n",
    "    feature_list = columns[:columns.index(STOP_FEATURE)]\n",
    "    print(f\"✓ Features up to '{STOP_FEATURE}':\")\n",
    "else:\n",
    "    feature_list = [c for c in columns if c != RESPONSE_COLUMN]\n",
    "    print(f\"⚠️ '{STOP_FEATURE}' not found, using all columns except response\")\n",
    "\n",
    "# Remove 'Run' or index columns if present\n",
    "feature_list = [f for f in feature_list if f.lower() not in ['run', 'index', 'unnamed: 0']]\n",
    "\n",
    "for i, feat in enumerate(feature_list, 1):\n",
    "    print(f\"    {i}. {feat}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 7: IDENTIFY FEATURE TYPES (Binary vs Continuous)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FEATURE TYPE CLASSIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get numeric columns from feature list\n",
    "numeric_features = df_phase1[feature_list].select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_features = [c for c in numeric_features if c != RESPONSE_COLUMN]\n",
    "\n",
    "# Classify each feature\n",
    "binary_cols = []\n",
    "continuous_cols = []\n",
    "binary_mappings = {}\n",
    "\n",
    "print(\"\\nClassification (Binary: 2 unique values, Continuous: 3+ values):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for col in numeric_features:\n",
    "    n_unique = df_phase1[col].nunique()\n",
    "    \n",
    "    if n_unique == 2:\n",
    "        binary_cols.append(col)\n",
    "        unique_vals = sorted(df_phase1[col].dropna().unique())\n",
    "        mapping = {unique_vals[0]: 0, unique_vals[1]: 1}\n",
    "        df_phase1[col] = df_phase1[col].map(mapping)\n",
    "        binary_mappings[col] = mapping\n",
    "        print(f\"  {col}: BINARY\")\n",
    "        print(f\"      {unique_vals[0]} → 0, {unique_vals[1]} → 1\")\n",
    "    else:\n",
    "        continuous_cols.append(col)\n",
    "        print(f\"  {col}: CONTINUOUS ({n_unique} unique values)\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(f\"SUMMARY: {len(binary_cols)} binary, {len(continuous_cols)} continuous\")\n",
    "\n",
    "BINARY_FEATURES = binary_cols\n",
    "CONTINUOUS_FEATURES = continuous_cols\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 8: PREPARE X AND y\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA PREPARATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "feature_cols = BINARY_FEATURES + CONTINUOUS_FEATURES\n",
    "\n",
    "# Create X and y\n",
    "X = df_phase1[feature_cols].copy()\n",
    "y = df_phase1[RESPONSE_COLUMN].copy()\n",
    "\n",
    "# Drop rows with missing response\n",
    "valid = ~y.isnull()\n",
    "n_dropped = (~valid).sum()\n",
    "X = X[valid].reset_index(drop=True)\n",
    "y = y[valid].reset_index(drop=True)\n",
    "\n",
    "# Store original (unscaled)\n",
    "X_original = X.copy()\n",
    "y_original = y.copy()\n",
    "\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  Samples: {len(X)}\")\n",
    "if n_dropped > 0:\n",
    "    print(f\"  Dropped (missing response): {n_dropped}\")\n",
    "\n",
    "print(f\"\\nFeatures ({len(feature_cols)} total):\")\n",
    "print(f\"  Binary: {len(BINARY_FEATURES)}\")\n",
    "print(f\"  Continuous: {len(CONTINUOUS_FEATURES)}\")\n",
    "print(f\"\\nResponse: {RESPONSE_COLUMN}\")\n",
    "print(f\"Samples/Features ratio: {len(X)/len(feature_cols):.1f}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 9: STANDARDIZE FEATURES (for Phase 1 analysis)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STANDARDIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "scaler_phase1 = StandardScaler()\n",
    "X_scaled = X.copy()\n",
    "\n",
    "if CONTINUOUS_FEATURES:\n",
    "    X_scaled[CONTINUOUS_FEATURES] = scaler_phase1.fit_transform(X[CONTINUOUS_FEATURES])\n",
    "    print(f\"✓ Standardized {len(CONTINUOUS_FEATURES)} continuous features\")\n",
    "\n",
    "# For modeling: scale binary to [-1, +1]\n",
    "X_model = X_scaled.copy()\n",
    "for col in BINARY_FEATURES:\n",
    "    X_model[col] = X_model[col] * 2 - 1\n",
    "\n",
    "print(f\"✓ Binary features: 0/1 (scaled to [-1,+1] for modeling)\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 10: VISUAL INSPECTION - SCATTER PLOTS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"VISUAL INSPECTION: Feature vs Response\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "n_feat = len(feature_cols)\n",
    "n_cols_plot = min(4, n_feat)\n",
    "n_rows_plot = int(np.ceil(n_feat / n_cols_plot))\n",
    "\n",
    "fig, axes = plt.subplots(n_rows_plot, n_cols_plot, figsize=(4*n_cols_plot, 3.5*n_rows_plot))\n",
    "axes = axes.flatten() if n_feat > 1 else [axes]\n",
    "\n",
    "for i, col in enumerate(feature_cols):\n",
    "    ax = axes[i]\n",
    "    corr = X[col].corr(y)\n",
    "    \n",
    "    if col in BINARY_FEATURES:\n",
    "        for val in [0, 1]:\n",
    "            data = y[X[col] == val]\n",
    "            ax.boxplot([data], positions=[val], widths=0.6)\n",
    "        ax.set_xticks([0, 1])\n",
    "        ax.set_xlabel(f'{col} (binary)')\n",
    "    else:\n",
    "        ax.scatter(X[col], y, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "        z = np.polyfit(X[col], y, 1)\n",
    "        p = np.poly1d(z)\n",
    "        x_line = np.linspace(X[col].min(), X[col].max(), 100)\n",
    "        ax.plot(x_line, p(x_line), 'r--', linewidth=2)\n",
    "        ax.set_xlabel(col)\n",
    "    \n",
    "    ax.set_ylabel(RESPONSE_COLUMN)\n",
    "    ax.set_title(f'r = {corr:.3f}', fontsize=10)\n",
    "\n",
    "for j in range(i+1, len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "\n",
    "plt.suptitle(f'Features vs {RESPONSE_COLUMN}', fontsize=12, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'phase1_scatter_plots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 11: RESPONSE DISTRIBUTION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RESPONSE DISTRIBUTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1 = axes[0]\n",
    "ax1.hist(y, bins=15, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "ax1.axvline(y.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {y.mean():.2f}')\n",
    "ax1.axvline(y.median(), color='orange', linestyle='--', linewidth=2, label=f'Median: {y.median():.2f}')\n",
    "ax1.set_xlabel(RESPONSE_COLUMN)\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Response Distribution')\n",
    "ax1.legend()\n",
    "\n",
    "ax2 = axes[1]\n",
    "ax2.boxplot(y, vert=True)\n",
    "ax2.set_ylabel(RESPONSE_COLUMN)\n",
    "ax2.set_title('Response Box Plot')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'phase1_response_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nResponse statistics:\")\n",
    "print(f\"  Min:    {y.min():.4f}\")\n",
    "print(f\"  Max:    {y.max():.4f}\")\n",
    "print(f\"  Mean:   {y.mean():.4f}\")\n",
    "print(f\"  Median: {y.median():.4f}\")\n",
    "print(f\"  Std:    {y.std():.4f}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 12: METHOD 1 - CORRELATION ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"METHOD 1: PEARSON CORRELATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "correlations = X.corrwith(y)\n",
    "\n",
    "corr_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'correlation': correlations.values,\n",
    "    'abs_corr': np.abs(correlations.values),\n",
    "    'type': ['binary' if f in BINARY_FEATURES else 'continuous' for f in feature_cols],\n",
    "    'direction': ['Positive' if c > 0 else 'Negative' for c in correlations.values]\n",
    "}).sort_values('abs_corr', ascending=False).reset_index(drop=True)\n",
    "\n",
    "corr_df['rank_corr'] = range(1, len(corr_df) + 1)\n",
    "\n",
    "def corr_strength(r):\n",
    "    if abs(r) >= CORRELATION_STRONG: return 'Strong'\n",
    "    elif abs(r) >= CORRELATION_MODERATE: return 'Moderate'\n",
    "    else: return 'Weak'\n",
    "\n",
    "corr_df['strength'] = corr_df['correlation'].apply(corr_strength)\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "print(corr_df[['rank_corr', 'feature', 'correlation', 'direction', 'strength', 'type']].to_string(index=False))\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1 = axes[0]\n",
    "colors = ['forestgreen' if c > 0 else 'crimson' for c in corr_df['correlation']]\n",
    "ax1.barh(corr_df['feature'][::-1], corr_df['abs_corr'][::-1], color=colors[::-1])\n",
    "ax1.axvline(x=CORRELATION_STRONG, color='green', linestyle='--', linewidth=2, label=f'Strong ({CORRELATION_STRONG})')\n",
    "ax1.axvline(x=CORRELATION_MODERATE, color='orange', linestyle='--', linewidth=1.5, label=f'Moderate ({CORRELATION_MODERATE})')\n",
    "ax1.set_xlabel('|Correlation|')\n",
    "ax1.set_title('Feature-Response Correlation')\n",
    "ax1.legend(loc='lower right')\n",
    "\n",
    "ax2 = axes[1]\n",
    "ax2.barh(corr_df['feature'][::-1], corr_df['correlation'][::-1], color=colors[::-1])\n",
    "ax2.axvline(x=0, color='black', linewidth=1)\n",
    "ax2.set_xlabel('Correlation (with sign)')\n",
    "ax2.set_title('Direction of Effect')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'phase1_correlation_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 13: METHOD 2 - LASSO (Automatic Selection)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"METHOD 2: LASSO REGRESSION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "lasso = LassoCV(cv=5, max_iter=10000, random_state=RANDOM_STATE)\n",
    "lasso.fit(X_model, y)\n",
    "\n",
    "lasso_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'coefficient': lasso.coef_,\n",
    "    'abs_coef': np.abs(lasso.coef_),\n",
    "    'selected': lasso.coef_ != 0,\n",
    "    'type': ['binary' if f in BINARY_FEATURES else 'continuous' for f in feature_cols]\n",
    "}).sort_values('abs_coef', ascending=False).reset_index(drop=True)\n",
    "\n",
    "lasso_df['rank_lasso'] = range(1, len(lasso_df) + 1)\n",
    "\n",
    "print(f\"\\nLasso alpha: {lasso.alpha_:.4f}\")\n",
    "print(f\"Features selected: {lasso_df['selected'].sum()}/{len(lasso_df)}\")\n",
    "print(\"\\nResults:\")\n",
    "print(lasso_df[['rank_lasso', 'feature', 'coefficient', 'selected', 'type']].to_string(index=False))\n",
    "\n",
    "selected_by_lasso = lasso_df[lasso_df['selected']]['feature'].tolist()\n",
    "print(f\"\\n✓ Lasso selected: {selected_by_lasso}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['forestgreen' if s else 'lightgray' for s in lasso_df['selected']]\n",
    "plt.barh(lasso_df['feature'][::-1], lasso_df['abs_coef'][::-1], color=colors[::-1])\n",
    "plt.xlabel('|Coefficient|')\n",
    "plt.title('Lasso Coefficients (Green = Selected)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'phase1_lasso_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 14: METHOD 3 - PLS WITH VIP SCORES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"METHOD 3: PLS (VIP Scores)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "max_comp = min(5, len(feature_cols), len(X) - 1)\n",
    "cv_scores = []\n",
    "\n",
    "print(\"Finding optimal components...\")\n",
    "for n in range(1, max_comp + 1):\n",
    "    scores = cross_val_score(PLSRegression(n_components=n), X_model, y, cv=5, scoring='r2')\n",
    "    cv_scores.append(scores.mean())\n",
    "    print(f\"  {n} components: CV R² = {scores.mean():.4f}\")\n",
    "\n",
    "optimal_comp = np.argmax(cv_scores) + 1\n",
    "print(f\"\\n✓ Optimal: {optimal_comp} components (CV R² = {max(cv_scores):.4f})\")\n",
    "\n",
    "pls = PLSRegression(n_components=optimal_comp)\n",
    "pls.fit(X_model, y)\n",
    "\n",
    "def calc_vip(model):\n",
    "    t, w, q = model.x_scores_, model.x_weights_, model.y_loadings_\n",
    "    m, p = w.shape\n",
    "    ss = np.sum(t**2, axis=0) * q.flatten()**2\n",
    "    total_ss = np.sum(ss)\n",
    "    vip = np.zeros(m)\n",
    "    for i in range(m):\n",
    "        weight = sum((w[i,j]**2) * ss[j] / np.sum(w[:,j]**2) for j in range(p))\n",
    "        vip[i] = np.sqrt(m * weight / total_ss)\n",
    "    return vip\n",
    "\n",
    "vip_scores = calc_vip(pls)\n",
    "\n",
    "pls_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'VIP': vip_scores,\n",
    "    'type': ['binary' if f in BINARY_FEATURES else 'continuous' for f in feature_cols]\n",
    "}).sort_values('VIP', ascending=False).reset_index(drop=True)\n",
    "\n",
    "pls_df['rank_pls'] = range(1, len(pls_df) + 1)\n",
    "\n",
    "def vip_category(v):\n",
    "    if v >= VIP_IMPORTANT: return 'Important'\n",
    "    elif v >= VIP_MODERATE: return 'Moderate'\n",
    "    else: return 'Less Important'\n",
    "\n",
    "pls_df['category'] = pls_df['VIP'].apply(vip_category)\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "print(pls_df[['rank_pls', 'feature', 'VIP', 'category', 'type']].to_string(index=False))\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1 = axes[0]\n",
    "colors = ['darkgreen' if v >= VIP_IMPORTANT else 'orange' if v >= VIP_MODERATE else 'lightcoral' \n",
    "          for v in pls_df['VIP']]\n",
    "ax1.barh(pls_df['feature'][::-1], pls_df['VIP'][::-1], color=colors[::-1])\n",
    "ax1.axvline(x=VIP_IMPORTANT, color='green', linestyle='--', linewidth=2, label=f'Important ({VIP_IMPORTANT})')\n",
    "ax1.axvline(x=VIP_MODERATE, color='orange', linestyle='--', linewidth=1.5, label=f'Moderate ({VIP_MODERATE})')\n",
    "ax1.set_xlabel('VIP Score')\n",
    "ax1.set_title('PLS Variable Importance')\n",
    "ax1.legend()\n",
    "\n",
    "ax2 = axes[1]\n",
    "ax2.plot(range(1, max_comp + 1), cv_scores, 'bo-', linewidth=2, markersize=8)\n",
    "ax2.axvline(x=optimal_comp, color='red', linestyle='--', label=f'Optimal = {optimal_comp}')\n",
    "ax2.set_xlabel('Number of Components')\n",
    "ax2.set_ylabel('CV R²')\n",
    "ax2.set_title('PLS Component Selection')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'phase1_pls_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 15: INTERACTION SCREENING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"INTERACTION SCREENING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "top_n = min(6, len(feature_cols))\n",
    "top_features = corr_df.head(top_n)['feature'].tolist()\n",
    "\n",
    "interaction_results = []\n",
    "\n",
    "for f1, f2 in combinations(top_features, 2):\n",
    "    median_f2 = X[f2].median()\n",
    "    low_f2 = X[f2] <= median_f2\n",
    "    high_f2 = X[f2] > median_f2\n",
    "    \n",
    "    if low_f2.sum() >= 3 and high_f2.sum() >= 3:\n",
    "        corr_low = X.loc[low_f2, f1].corr(y[low_f2])\n",
    "        corr_high = X.loc[high_f2, f1].corr(y[high_f2])\n",
    "        \n",
    "        if not np.isnan(corr_low) and not np.isnan(corr_high):\n",
    "            interaction_strength = abs(corr_high - corr_low)\n",
    "            \n",
    "            interaction_results.append({\n",
    "                'interaction': f'{f1} × {f2}',\n",
    "                'feature_1': f1,\n",
    "                'feature_2': f2,\n",
    "                'corr_low_f2': corr_low,\n",
    "                'corr_high_f2': corr_high,\n",
    "                'strength': interaction_strength,\n",
    "                'significant': interaction_strength > INTERACTION_THRESHOLD\n",
    "            })\n",
    "\n",
    "if interaction_results:\n",
    "    interaction_df = pd.DataFrame(interaction_results).sort_values('strength', ascending=False)\n",
    "    \n",
    "    print(\"\\nInteraction Analysis:\")\n",
    "    print(interaction_df[['interaction', 'corr_low_f2', 'corr_high_f2', 'strength', 'significant']].to_string(index=False))\n",
    "    \n",
    "    strong_interactions = interaction_df[interaction_df['significant']]\n",
    "    \n",
    "    if len(strong_interactions) > 0:\n",
    "        print(f\"\\n⚠️ POTENTIAL INTERACTIONS (strength > {INTERACTION_THRESHOLD}):\")\n",
    "        features_with_interactions = set()\n",
    "        for _, row in strong_interactions.iterrows():\n",
    "            print(f\"  {row['interaction']}: {row['strength']:.3f}\")\n",
    "            features_with_interactions.add(row['feature_1'])\n",
    "            features_with_interactions.add(row['feature_2'])\n",
    "        features_with_interactions = list(features_with_interactions)\n",
    "    else:\n",
    "        print(f\"\\n✓ No strong interactions detected\")\n",
    "        features_with_interactions = []\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    colors = ['crimson' if s else 'steelblue' for s in interaction_df['significant']]\n",
    "    plt.barh(interaction_df['interaction'][::-1], interaction_df['strength'][::-1], color=colors[::-1])\n",
    "    plt.axvline(x=INTERACTION_THRESHOLD, color='red', linestyle='--', linewidth=2)\n",
    "    plt.xlabel('Interaction Strength')\n",
    "    plt.title('Interaction Screening')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_DIR / 'phase1_interactions.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"✓ No interactions to analyze\")\n",
    "    interaction_df = pd.DataFrame()\n",
    "    strong_interactions = pd.DataFrame()\n",
    "    features_with_interactions = []\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 16: MULTICOLLINEARITY CHECK\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MULTICOLLINEARITY CHECK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "feature_corr = X.corr()\n",
    "\n",
    "high_corr_pairs = []\n",
    "for i in range(len(feature_cols)):\n",
    "    for j in range(i+1, len(feature_cols)):\n",
    "        r = feature_corr.iloc[i, j]\n",
    "        if abs(r) > MULTICOLLINEARITY_THRESHOLD:\n",
    "            high_corr_pairs.append({\n",
    "                'feature_1': feature_cols[i],\n",
    "                'feature_2': feature_cols[j],\n",
    "                'correlation': r\n",
    "            })\n",
    "\n",
    "if high_corr_pairs:\n",
    "    print(f\"⚠️ HIGHLY CORRELATED PAIRS (|r| > {MULTICOLLINEARITY_THRESHOLD}):\")\n",
    "    for pair in high_corr_pairs:\n",
    "        print(f\"  {pair['feature_1']} ↔ {pair['feature_2']}: r = {pair['correlation']:.3f}\")\n",
    "else:\n",
    "    print(\"✓ No highly correlated feature pairs found\")\n",
    "\n",
    "# Heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "mask = np.triu(np.ones_like(feature_corr, dtype=bool), k=0)\n",
    "sns.heatmap(feature_corr, annot=True, cmap='RdBu_r', center=0, fmt='.2f',\n",
    "            mask=mask, square=True, linewidths=0.5)\n",
    "plt.title('Feature-Feature Correlations')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'phase1_multicollinearity.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22690a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 17: CONSENSUS RANKING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CONSENSUS RANKING (All Methods)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "consensus = corr_df[['feature', 'rank_corr', 'correlation', 'direction', 'type', 'strength']].merge(\n",
    "    lasso_df[['feature', 'rank_lasso', 'coefficient', 'selected']], on='feature'\n",
    ").merge(\n",
    "    pls_df[['feature', 'rank_pls', 'VIP', 'category']], on='feature'\n",
    ")\n",
    "\n",
    "consensus['avg_rank'] = consensus[['rank_corr', 'rank_lasso', 'rank_pls']].mean(axis=1)\n",
    "consensus = consensus.sort_values('avg_rank').reset_index(drop=True)\n",
    "consensus['final_rank'] = range(1, len(consensus) + 1)\n",
    "\n",
    "def count_top_k(row, k=3):\n",
    "    count = 0\n",
    "    if row['rank_corr'] <= k: count += 1\n",
    "    if row['rank_lasso'] <= k: count += 1\n",
    "    if row['rank_pls'] <= k: count += 1\n",
    "    return count\n",
    "\n",
    "consensus['methods_top3'] = consensus.apply(lambda r: count_top_k(r, 3), axis=1)\n",
    "consensus['has_interaction'] = consensus['feature'].isin(features_with_interactions)\n",
    "\n",
    "print(\"\\nConsensus Ranking:\")\n",
    "display_cols = ['final_rank', 'feature', 'type', 'correlation', 'VIP', 'selected', \n",
    "                'avg_rank', 'methods_top3', 'has_interaction']\n",
    "print(consensus[display_cols].to_string(index=False))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 18: CONSENSUS VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CONSENSUS VISUALIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Heatmap of rankings\n",
    "ax1 = axes[0, 0]\n",
    "heatmap_data = consensus.set_index('feature')[['rank_corr', 'rank_lasso', 'rank_pls']]\n",
    "heatmap_data.columns = ['Correlation', 'Lasso', 'PLS']\n",
    "sns.heatmap(heatmap_data, annot=True, fmt='.0f', cmap='RdYlGn_r', ax=ax1,\n",
    "            cbar_kws={'label': 'Rank (lower=better)'})\n",
    "ax1.set_title('Rankings Across Methods')\n",
    "\n",
    "# Method agreement\n",
    "ax2 = axes[0, 1]\n",
    "colors = ['darkgreen' if a >= 3 else 'orange' if a >= 2 else 'lightcoral' \n",
    "          for a in consensus['methods_top3']]\n",
    "ax2.barh(consensus['feature'][::-1], consensus['methods_top3'][::-1], color=colors[::-1])\n",
    "ax2.axvline(x=2, color='orange', linestyle='--', linewidth=2)\n",
    "ax2.set_xlabel('Methods Ranking Feature in Top 3')\n",
    "ax2.set_title('Method Agreement')\n",
    "\n",
    "# Average rank\n",
    "ax3 = axes[1, 0]\n",
    "colors = ['steelblue' if t == 'binary' else 'forestgreen' for t in consensus['type']]\n",
    "ax3.barh(consensus['feature'][::-1], consensus['avg_rank'][::-1], color=colors[::-1])\n",
    "ax3.set_xlabel('Average Rank (lower = better)')\n",
    "ax3.set_title('Consensus Ranking')\n",
    "ax3.invert_xaxis()\n",
    "\n",
    "# Correlation vs VIP\n",
    "ax4 = axes[1, 1]\n",
    "for _, row in consensus.iterrows():\n",
    "    color = 'steelblue' if row['type'] == 'binary' else 'forestgreen'\n",
    "    marker = 's' if row['has_interaction'] else 'o'\n",
    "    ax4.scatter(abs(row['correlation']), row['VIP'], c=color, s=100, marker=marker, \n",
    "                edgecolors='black', linewidth=0.5)\n",
    "    ax4.annotate(row['feature'], (abs(row['correlation']), row['VIP']), \n",
    "                 fontsize=8, ha='left', va='bottom')\n",
    "\n",
    "ax4.axhline(y=VIP_IMPORTANT, color='green', linestyle='--', alpha=0.7)\n",
    "ax4.axvline(x=CORRELATION_STRONG, color='blue', linestyle='--', alpha=0.7)\n",
    "ax4.set_xlabel('|Correlation|')\n",
    "ax4.set_ylabel('VIP Score')\n",
    "ax4.set_title('Correlation vs VIP')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'phase1_consensus.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 19: AUTOMATIC FEATURE RECOMMENDATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FEATURE RECOMMENDATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def score_feature(row):\n",
    "    score = 0\n",
    "    if abs(row['correlation']) >= CORRELATION_STRONG: score += 3\n",
    "    elif abs(row['correlation']) >= CORRELATION_MODERATE: score += 2\n",
    "    if row['VIP'] >= VIP_IMPORTANT: score += 3\n",
    "    elif row['VIP'] >= VIP_MODERATE: score += 2\n",
    "    if row['selected']: score += 2\n",
    "    score += row['methods_top3']\n",
    "    if row['has_interaction']: score += 2\n",
    "    return score\n",
    "\n",
    "consensus['score'] = consensus.apply(score_feature, axis=1)\n",
    "consensus = consensus.sort_values('score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nFeature Scores:\")\n",
    "print(consensus[['feature', 'type', 'correlation', 'VIP', 'selected', 'has_interaction', 'score']].to_string(index=False))\n",
    "\n",
    "# Generate recommendations\n",
    "recommended = []\n",
    "reasons = {}\n",
    "\n",
    "for _, row in consensus.iterrows():\n",
    "    include = False\n",
    "    reason = []\n",
    "    \n",
    "    if row['score'] >= 6:\n",
    "        include = True\n",
    "        reason.append(f\"High score ({row['score']})\")\n",
    "    if abs(row['correlation']) >= CORRELATION_STRONG:\n",
    "        include = True\n",
    "        reason.append(\"Strong correlation\")\n",
    "    if row['VIP'] >= VIP_IMPORTANT:\n",
    "        include = True\n",
    "        reason.append(f\"VIP ≥ {VIP_IMPORTANT}\")\n",
    "    if row['has_interaction'] and row['score'] >= 4:\n",
    "        include = True\n",
    "        reason.append(\"Part of interaction\")\n",
    "    \n",
    "    if include and len(recommended) < 6:\n",
    "        recommended.append(row['feature'])\n",
    "        reasons[row['feature']] = ', '.join(reason)\n",
    "\n",
    "# Ensure minimum\n",
    "MIN_FEATURES = 3\n",
    "if len(recommended) < MIN_FEATURES:\n",
    "    for _, row in consensus.iterrows():\n",
    "        if row['feature'] not in recommended:\n",
    "            recommended.append(row['feature'])\n",
    "            reasons[row['feature']] = \"Added to meet minimum\"\n",
    "        if len(recommended) >= MIN_FEATURES:\n",
    "            break\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"RECOMMENDED ({len(recommended)} features):\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "for feat in recommended:\n",
    "    row = consensus[consensus['feature'] == feat].iloc[0]\n",
    "    print(f\"\\n  ✓ {feat} ({row['type']})\")\n",
    "    print(f\"      Correlation: {row['correlation']:.3f}\")\n",
    "    print(f\"      VIP: {row['VIP']:.2f}\")\n",
    "    print(f\"      Reason: {reasons[feat]}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 20: MANUAL FEATURE SELECTION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FEATURE SELECTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ┌─────────────────────────────────────────────────────────────────────────┐\n",
    "# │ OPTION 1: Use automatic recommendations (DEFAULT)                       │\n",
    "# └─────────────────────────────────────────────────────────────────────────┘\n",
    "selected_features = recommended[:TARGET_FEATURES]\n",
    "\n",
    "# ┌─────────────────────────────────────────────────────────────────────────┐\n",
    "# │ OPTION 2: Manual override - UNCOMMENT AND MODIFY IF NEEDED             │\n",
    "# └─────────────────────────────────────────────────────────────────────────┘\n",
    "# selected_features = [\n",
    "#     'Your_Feature_1',  # Keep from recommendations\n",
    "#     'Your_Feature_2',  # Keep from recommendations\n",
    "#     'Your_Feature_3',  # Keep from recommendations\n",
    "#     'Your_New_Feature', # YOUR MANUAL OVERRIDE - replace one you don't like\n",
    "# ]\n",
    "\n",
    "print(f\"\\nSelected features ({len(selected_features)}):\")\n",
    "for i, feat in enumerate(selected_features, 1):\n",
    "    row = consensus[consensus['feature'] == feat].iloc[0]\n",
    "    print(f\"  {i}. {feat} ({row['type']}) - corr: {row['correlation']:.3f}, VIP: {row['VIP']:.2f}\")\n",
    "\n",
    "# Validation checks\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"VALIDATION CHECKS:\")\n",
    "\n",
    "# Check multicollinearity\n",
    "if len(selected_features) > 1:\n",
    "    sel_corr = X[selected_features].corr()\n",
    "    issues = []\n",
    "    for i in range(len(selected_features)):\n",
    "        for j in range(i+1, len(selected_features)):\n",
    "            r = sel_corr.iloc[i, j]\n",
    "            if abs(r) > MULTICOLLINEARITY_THRESHOLD:\n",
    "                issues.append(f\"{selected_features[i]} ↔ {selected_features[j]}: r={r:.2f}\")\n",
    "    \n",
    "    if issues:\n",
    "        print(f\"  ⚠️ Multicollinearity detected:\")\n",
    "        for issue in issues:\n",
    "            print(f\"      {issue}\")\n",
    "    else:\n",
    "        print(f\"  ✓ No multicollinearity issues\")\n",
    "\n",
    "# Check broken interactions\n",
    "if len(strong_interactions) > 0:\n",
    "    broken = []\n",
    "    for _, row in strong_interactions.iterrows():\n",
    "        f1, f2 = row['feature_1'], row['feature_2']\n",
    "        if (f1 in selected_features) != (f2 in selected_features):\n",
    "            broken.append((f1, f2))\n",
    "    \n",
    "    if broken:\n",
    "        print(f\"  ⚠️ Broken interactions:\")\n",
    "        for f1, f2 in broken:\n",
    "            in1 = \"IN\" if f1 in selected_features else \"OUT\"\n",
    "            in2 = \"IN\" if f2 in selected_features else \"OUT\"\n",
    "            print(f\"      {f1} ({in1}) × {f2} ({in2})\")\n",
    "    else:\n",
    "        print(f\"  ✓ No broken interactions\")\n",
    "else:\n",
    "    print(f\"  ✓ No interactions to check\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 21: VALIDATE WITH LOO-CV (LEAKAGE-FREE)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"VALIDATION: Leave-One-Out Cross-Validation (Leakage-Free)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Prepare selected feature data\n",
    "X_sel = X[selected_features].copy()\n",
    "\n",
    "# Identify which selected features are continuous\n",
    "sel_continuous = [f for f in selected_features if f in CONTINUOUS_FEATURES]\n",
    "sel_binary = [f for f in selected_features if f in BINARY_FEATURES]\n",
    "\n",
    "# LOO-CV with proper scaling inside the loop (NO LEAKAGE)\n",
    "loo_preds = []\n",
    "loo_actual = []\n",
    "\n",
    "for train_idx, test_idx in LeaveOneOut().split(X_sel):\n",
    "    X_train = X_sel.iloc[train_idx].copy()\n",
    "    X_test = X_sel.iloc[test_idx].copy()\n",
    "    y_train = y.iloc[train_idx]\n",
    "    y_test = y.iloc[test_idx]\n",
    "    \n",
    "    # Scale ONLY on training data\n",
    "    if sel_continuous:\n",
    "        scaler_cv = StandardScaler()\n",
    "        X_train[sel_continuous] = scaler_cv.fit_transform(X_train[sel_continuous])\n",
    "        X_test[sel_continuous] = scaler_cv.transform(X_test[sel_continuous])\n",
    "    \n",
    "    # Convert binary to [-1, +1]\n",
    "    for col in sel_binary:\n",
    "        X_train[col] = X_train[col] * 2 - 1\n",
    "        X_test[col] = X_test[col] * 2 - 1\n",
    "    \n",
    "    # Fit model\n",
    "    model = RidgeCV(alphas=[0.1, 1, 10, 100], cv=3)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    loo_preds.append(model.predict(X_test)[0])\n",
    "    loo_actual.append(y_test.values[0])\n",
    "\n",
    "loo_preds = np.array(loo_preds)\n",
    "loo_actual = np.array(loo_actual)\n",
    "\n",
    "# Metrics\n",
    "loo_r2 = r2_score(loo_actual, loo_preds)\n",
    "loo_rmse = np.sqrt(mean_squared_error(loo_actual, loo_preds))\n",
    "loo_mae = np.mean(np.abs(loo_actual - loo_preds))\n",
    "\n",
    "print(f\"\\nLOO-CV Results ({len(selected_features)} features):\")\n",
    "print(f\"  R²:   {loo_r2:.4f}\")\n",
    "print(f\"  RMSE: {loo_rmse:.4f}\")\n",
    "print(f\"  MAE:  {loo_mae:.4f}\")\n",
    "\n",
    "print(f\"\\nInterpretation:\")\n",
    "if loo_r2 > 0.5:\n",
    "    print(\"  ✓ Good signal - features are predictive\")\n",
    "elif loo_r2 > 0.2:\n",
    "    print(\"  ⚠️ Moderate signal - GP in BO can likely improve\")\n",
    "elif loo_r2 > 0:\n",
    "    print(\"  ⚠️ Weak linear signal - may be non-linear relationships\")\n",
    "else:\n",
    "    print(\"  ⚠️ No linear signal - review feature selection\")\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "ax1 = axes[0]\n",
    "ax1.scatter(loo_actual, loo_preds, alpha=0.7, edgecolors='black', linewidth=0.5)\n",
    "lims = [min(loo_actual.min(), loo_preds.min()), max(loo_actual.max(), loo_preds.max())]\n",
    "ax1.plot(lims, lims, 'r--', linewidth=2, label='Perfect')\n",
    "ax1.set_xlabel('Actual')\n",
    "ax1.set_ylabel('Predicted')\n",
    "ax1.set_title(f'LOO-CV: R² = {loo_r2:.4f}')\n",
    "ax1.legend()\n",
    "\n",
    "ax2 = axes[1]\n",
    "residuals = loo_actual - loo_preds\n",
    "ax2.hist(residuals, bins=12, edgecolor='black', alpha=0.7)\n",
    "ax2.axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "ax2.set_xlabel('Residual')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title(f'Residuals: Mean={residuals.mean():.3f}')\n",
    "\n",
    "ax3 = axes[2]\n",
    "ax3.scatter(loo_preds, residuals, alpha=0.7, edgecolors='black', linewidth=0.5)\n",
    "ax3.axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "ax3.set_xlabel('Predicted')\n",
    "ax3.set_ylabel('Residual')\n",
    "ax3.set_title('Residuals vs Predicted')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'phase1_validation.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 22: DEFINE BO SEARCH SPACE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BAYESIAN OPTIMIZATION SEARCH SPACE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "bo_bounds = []\n",
    "for feat in selected_features:\n",
    "    if feat in BINARY_FEATURES:\n",
    "        bo_bounds.append({\n",
    "            'feature': feat,\n",
    "            'type': 'binary',\n",
    "            'min': 0,\n",
    "            'max': 1,\n",
    "            'observed_min': 0,\n",
    "            'observed_max': 1\n",
    "        })\n",
    "    else:\n",
    "        feat_min = X_original[feat].min()\n",
    "        feat_max = X_original[feat].max()\n",
    "        feat_range = feat_max - feat_min\n",
    "        margin = 0.1 * feat_range\n",
    "        bo_bounds.append({\n",
    "            'feature': feat,\n",
    "            'type': 'continuous',\n",
    "            'min': round(feat_min - margin, 4),\n",
    "            'max': round(feat_max + margin, 4),\n",
    "            'observed_min': feat_min,\n",
    "            'observed_max': feat_max\n",
    "        })\n",
    "\n",
    "bo_bounds_df = pd.DataFrame(bo_bounds)\n",
    "\n",
    "print(\"\\nSearch Space Bounds:\")\n",
    "print(bo_bounds_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"EFFECT DIRECTIONS:\")\n",
    "for feat in selected_features:\n",
    "    row = consensus[consensus['feature'] == feat].iloc[0]\n",
    "    direction = row['correlation']\n",
    "    \n",
    "    if MAXIMIZE_RESPONSE:\n",
    "        suggest = \"HIGH\" if direction > 0 else \"LOW\"\n",
    "    else:\n",
    "        suggest = \"LOW\" if direction > 0 else \"HIGH\"\n",
    "    \n",
    "    print(f\"  {feat}: {'Positive' if direction > 0 else 'Negative'} effect → Suggest {suggest}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 23: CREATE COMPREHENSIVE FEATURE INFO\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPILING FEATURE INFORMATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "feature_info = []\n",
    "\n",
    "for feat in selected_features:\n",
    "    row = consensus[consensus['feature'] == feat].iloc[0]\n",
    "    bounds_row = bo_bounds_df[bo_bounds_df['feature'] == feat].iloc[0]\n",
    "    \n",
    "    info = {\n",
    "        'feature': feat,\n",
    "        'type': row['type'],\n",
    "        'correlation': row['correlation'],\n",
    "        'correlation_strength': row['strength'],\n",
    "        'direction': row['direction'],\n",
    "        'lasso_coefficient': row['coefficient'],\n",
    "        'lasso_selected': row['selected'],\n",
    "        'vip_score': row['VIP'],\n",
    "        'vip_category': row['category'],\n",
    "        'consensus_rank': int(row['final_rank']),\n",
    "        'methods_top3': int(row['methods_top3']),\n",
    "        'has_interaction': row['has_interaction'],\n",
    "        'feature_score': int(row['score']),\n",
    "        'bound_min': bounds_row['min'],\n",
    "        'bound_max': bounds_row['max'],\n",
    "        'observed_min': bounds_row['observed_min'],\n",
    "        'observed_max': bounds_row['observed_max'],\n",
    "    }\n",
    "    \n",
    "    # Add binary mapping if applicable\n",
    "    if feat in binary_mappings:\n",
    "        mapping = binary_mappings[feat]\n",
    "        info['binary_value_0'] = list(mapping.keys())[0]\n",
    "        info['binary_value_1'] = list(mapping.keys())[1]\n",
    "    else:\n",
    "        info['binary_value_0'] = None\n",
    "        info['binary_value_1'] = None\n",
    "    \n",
    "    feature_info.append(info)\n",
    "\n",
    "feature_info_df = pd.DataFrame(feature_info)\n",
    "\n",
    "print(\"\\nSelected Feature Details:\")\n",
    "print(feature_info_df.to_string(index=False))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 24: SAVE PHASE 1 CHECKPOINT\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SAVING PHASE 1 CHECKPOINT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "checkpoint_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Create checkpoint dictionary with all relevant information\n",
    "phase1_checkpoint = {\n",
    "    'metadata': {\n",
    "        'checkpoint_time': checkpoint_time,\n",
    "        'phase1_data_file': PHASE1_DATA_FILE,\n",
    "        'phase1_sheet_name': PHASE1_SHEET_NAME,\n",
    "        'response_column': RESPONSE_COLUMN,\n",
    "        'maximize_response': MAXIMIZE_RESPONSE,\n",
    "        'n_initial_experiments': len(X),\n",
    "        'n_total_features': len(feature_cols),\n",
    "        'n_selected_features': len(selected_features),\n",
    "        'target_features': TARGET_FEATURES,\n",
    "    },\n",
    "    'thresholds': {\n",
    "        'correlation_strong': CORRELATION_STRONG,\n",
    "        'correlation_moderate': CORRELATION_MODERATE,\n",
    "        'vip_important': VIP_IMPORTANT,\n",
    "        'vip_moderate': VIP_MODERATE,\n",
    "        'multicollinearity_threshold': MULTICOLLINEARITY_THRESHOLD,\n",
    "        'interaction_threshold': INTERACTION_THRESHOLD,\n",
    "    },\n",
    "    'selected_features': selected_features,\n",
    "    'all_features': feature_cols,\n",
    "    'binary_features': BINARY_FEATURES,\n",
    "    'continuous_features': CONTINUOUS_FEATURES,\n",
    "    'selected_binary': [f for f in selected_features if f in BINARY_FEATURES],\n",
    "    'selected_continuous': [f for f in selected_features if f in CONTINUOUS_FEATURES],\n",
    "    'binary_mappings': binary_mappings,\n",
    "    'validation_metrics': {\n",
    "        'loo_r2': float(loo_r2),\n",
    "        'loo_rmse': float(loo_rmse),\n",
    "        'loo_mae': float(loo_mae),\n",
    "    },\n",
    "    'response_stats': {\n",
    "        'min': float(y.min()),\n",
    "        'max': float(y.max()),\n",
    "        'mean': float(y.mean()),\n",
    "        'median': float(y.median()),\n",
    "        'std': float(y.std()),\n",
    "    },\n",
    "    'interactions': {\n",
    "        'features_with_interactions': features_with_interactions,\n",
    "        'strong_interactions': strong_interactions.to_dict('records') if len(strong_interactions) > 0 else [],\n",
    "    },\n",
    "    'multicollinearity': {\n",
    "        'high_correlation_pairs': high_corr_pairs,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Save as JSON (human-readable)\n",
    "checkpoint_json_path = OUTPUT_DIR / 'phase1_checkpoint.json'\n",
    "with open(checkpoint_json_path, 'w') as f:\n",
    "    json.dump(phase1_checkpoint, f, indent=2, default=str)\n",
    "print(f\"✓ Saved: {checkpoint_json_path}\")\n",
    "\n",
    "# Save as pickle (preserves all Python objects)\n",
    "checkpoint_pkl_path = OUTPUT_DIR / 'phase1_checkpoint.pkl'\n",
    "with open(checkpoint_pkl_path, 'wb') as f:\n",
    "    pickle.dump(phase1_checkpoint, f)\n",
    "print(f\"✓ Saved: {checkpoint_pkl_path}\")\n",
    "\n",
    "# Save feature info DataFrame\n",
    "feature_info_path = OUTPUT_DIR / 'phase1_feature_info.csv'\n",
    "feature_info_df.to_csv(feature_info_path, index=False)\n",
    "print(f\"✓ Saved: {feature_info_path}\")\n",
    "\n",
    "# Save bounds DataFrame\n",
    "bounds_path = OUTPUT_DIR / 'phase1_bounds.csv'\n",
    "bo_bounds_df.to_csv(bounds_path, index=False)\n",
    "print(f\"✓ Saved: {bounds_path}\")\n",
    "\n",
    "# Save consensus ranking (all features)\n",
    "consensus_path = OUTPUT_DIR / 'phase1_consensus_ranking.csv'\n",
    "consensus.to_csv(consensus_path, index=False)\n",
    "print(f\"✓ Saved: {consensus_path}\")\n",
    "\n",
    "# Save original Phase 1 data (for reference)\n",
    "phase1_data_path = OUTPUT_DIR / 'phase1_original_data.csv'\n",
    "df_phase1.to_csv(phase1_data_path, index=False)\n",
    "print(f\"✓ Saved: {phase1_data_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CHECKPOINT CONTENTS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\"\"\n",
    "Files saved to: {OUTPUT_DIR}/\n",
    "\n",
    "1. phase1_checkpoint.json    - Human-readable checkpoint\n",
    "2. phase1_checkpoint.pkl     - Python pickle checkpoint\n",
    "3. phase1_feature_info.csv   - Selected feature details\n",
    "4. phase1_bounds.csv         - BO search space bounds\n",
    "5. phase1_consensus_ranking.csv - All features ranked\n",
    "6. phase1_original_data.csv  - Original experiment data\n",
    "7. Various PNG plots         - Visualization exports\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 25: PHASE 1 SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 1 COMPLETE: FEATURE SCREENING SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\"\"\n",
    "DATA ANALYZED:\n",
    "  Total samples: {len(X)}\n",
    "  Total features: {len(feature_cols)}\n",
    "    - Binary: {len(BINARY_FEATURES)}\n",
    "    - Continuous: {len(CONTINUOUS_FEATURES)}\n",
    "\n",
    "METHODS USED:\n",
    "  1. Pearson Correlation Analysis\n",
    "  2. Lasso Regression (automatic selection)\n",
    "  3. PLS with VIP Scores\n",
    "  4. Interaction Screening\n",
    "  5. Multicollinearity Check\n",
    "\n",
    "SELECTED FOR BAYESIAN OPTIMIZATION ({len(selected_features)} features):\n",
    "\"\"\")\n",
    "\n",
    "for feat in selected_features:\n",
    "    row = consensus[consensus['feature'] == feat].iloc[0]\n",
    "    bounds_row = bo_bounds_df[bo_bounds_df['feature'] == feat].iloc[0]\n",
    "    int_flag = \" ⚡\" if row['has_interaction'] else \"\"\n",
    "    print(f\"  • {feat} ({row['type']}){int_flag}\")\n",
    "    print(f\"      Correlation: {row['correlation']:+.3f} ({row['strength']})\")\n",
    "    print(f\"      VIP Score: {row['VIP']:.2f} ({row['category']})\")\n",
    "    print(f\"      Lasso: {'Selected' if row['selected'] else 'Not selected'}\")\n",
    "    print(f\"      Bounds: [{bounds_row['min']:.4f}, {bounds_row['max']:.4f}]\")\n",
    "\n",
    "if features_with_interactions:\n",
    "    print(f\"\\n  ⚡ = Part of detected interaction\")\n",
    "\n",
    "print(f\"\"\"\n",
    "VALIDATION (Leakage-Free LOO-CV):\n",
    "  R²:   {loo_r2:.4f}\n",
    "  RMSE: {loo_rmse:.4f}\n",
    "  MAE:  {loo_mae:.4f}\n",
    "\n",
    "CHECKPOINT SAVED:\n",
    "  Location: {OUTPUT_DIR}/\n",
    "  Use phase1_checkpoint.json or .pkl to load settings for Phase 2\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 26: INSTRUCTIONS FOR NEXT STEPS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"NEXT STEPS: PREPARE FOR PHASE 2\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\"\"\n",
    "┌─────────────────────────────────────────────────────────────────────────┐\n",
    "│ WHAT TO DO NOW:                                                         │\n",
    "├─────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                         │\n",
    "│ 1. REVIEW SELECTED FEATURES                                             │\n",
    "│    Current selection: {selected_features}\n",
    "│                                                                         │\n",
    "│ 2. IF YOU WANT TO CHANGE A FEATURE:                                     │\n",
    "│    → Go back to Cell 20                                                 │\n",
    "│    → Uncomment the manual override section                              │\n",
    "│    → Modify the list                                                    │\n",
    "│    → Re-run Cells 20-25 to update checkpoint                           │\n",
    "│                                                                         │\n",
    "│ 3. DESIGN NEW EXPERIMENTS                                               │\n",
    "│    → Use ONLY the {len(selected_features)} selected features                                     │\n",
    "│    → Recommended: 30 experiments                                        │\n",
    "│    → Design type: Latin Hypercube or space-filling                     │\n",
    "│    → Use bounds from phase1_bounds.csv                                 │\n",
    "│                                                                         │\n",
    "│ 4. RUN EXPERIMENTS IN LAB                                               │\n",
    "│    → Execute the 30 new experiments                                    │\n",
    "│    → Record results in Excel                                           │\n",
    "│                                                                         │\n",
    "│ 5. PREPARE PHASE 2 DATA FILE                                            │\n",
    "│    → Create Excel/CSV with columns:                                    │\n",
    "│      {selected_features + [RESPONSE_COLUMN]}\n",
    "│    → Save as 'phase2_experiments.xlsx' (or update config)             │\n",
    "│                                                                         │\n",
    "│ 6. CONTINUE TO PHASE 2                                                  │\n",
    "│    → Run cells starting from Cell 27                                   │\n",
    "│                                                                         │\n",
    "└─────────────────────────────────────────────────────────────────────────┘\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"END OF PHASE 1 - PROCEED TO PHASE 2 WHEN NEW DATA IS READY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# =============================================================================\n",
    "# PHASE 2: BAYESIAN OPTIMIZATION\n",
    "# =============================================================================\n",
    "# =============================================================================\n",
    "\n",
    "# ┌─────────────────────────────────────────────────────────────────────────┐\n",
    "# │ RUN CELLS BELOW ONLY AFTER:                                            │\n",
    "# │   1. You have selected your final features (Phase 1)                   │\n",
    "# │   2. You have run new experiments with those features                  │\n",
    "# │   3. You have created a new data file with results                     │\n",
    "# └─────────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 27: PHASE 2 CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PHASE 2: BAYESIAN OPTIMIZATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ┌─────────────────────────────────────────────────────────────────────────┐\n",
    "# │ MODIFY THIS SECTION FOR YOUR NEW DATA                                   │\n",
    "# └─────────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "# Phase 2 data file (your NEW experiments with selected features only)\n",
    "PHASE2_DATA_FILE = 'phase2_experiments.xlsx'  # UPDATE THIS\n",
    "PHASE2_SHEET_NAME = 'Sheet1'                   # UPDATE THIS\n",
    "\n",
    "# BO Settings\n",
    "BATCH_SIZE = 5                    # Number of experiments per iteration\n",
    "EXPLORATION_WEIGHT = 2.0          # Kappa for UCB (higher = more exploration)\n",
    "N_OPTIMIZER_RESTARTS = 20         # Restarts for acquisition optimization\n",
    "MIN_DISTANCE_BETWEEN_POINTS = 0.3 # Minimum distance between batch points (scaled)\n",
    "\n",
    "# Convergence settings\n",
    "MAX_ITERATIONS = 10               # Maximum BO iterations\n",
    "IMPROVEMENT_THRESHOLD = 0.01      # Stop if improvement < this for N iterations\n",
    "PATIENCE = 3                      # Number of iterations without improvement before stopping\n",
    "\n",
    "print(\"Phase 2 Configuration:\")\n",
    "print(f\"  Data file: {PHASE2_DATA_FILE}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Exploration weight (κ): {EXPLORATION_WEIGHT}\")\n",
    "print(f\"  Max iterations: {MAX_ITERATIONS}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 28: LOAD PHASE 1 CHECKPOINT\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LOADING PHASE 1 CHECKPOINT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint_path = OUTPUT_DIR / 'phase1_checkpoint.pkl'\n",
    "\n",
    "if checkpoint_path.exists():\n",
    "    with open(checkpoint_path, 'rb') as f:\n",
    "        phase1_checkpoint = pickle.load(f)\n",
    "    print(f\"✓ Loaded checkpoint from: {checkpoint_path}\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Phase 1 checkpoint not found at {checkpoint_path}. Run Phase 1 first.\")\n",
    "\n",
    "# Extract key information\n",
    "SELECTED_FEATURES = phase1_checkpoint['selected_features']\n",
    "BINARY_FEATURES_SELECTED = phase1_checkpoint['selected_binary']\n",
    "CONTINUOUS_FEATURES_SELECTED = phase1_checkpoint['selected_continuous']\n",
    "RESPONSE_COLUMN = phase1_checkpoint['metadata']['response_column']\n",
    "MAXIMIZE_RESPONSE = phase1_checkpoint['metadata']['maximize_response']\n",
    "BINARY_MAPPINGS = phase1_checkpoint['binary_mappings']\n",
    "\n",
    "print(f\"\\nRestored settings:\")\n",
    "print(f\"  Selected features: {SELECTED_FEATURES}\")\n",
    "print(f\"    Binary: {BINARY_FEATURES_SELECTED}\")\n",
    "print(f\"    Continuous: {CONTINUOUS_FEATURES_SELECTED}\")\n",
    "print(f\"  Response: {RESPONSE_COLUMN}\")\n",
    "print(f\"  Objective: {'Maximize' if MAXIMIZE_RESPONSE else 'Minimize'}\")\n",
    "\n",
    "# Load bounds\n",
    "bounds_df = pd.read_csv(OUTPUT_DIR / 'phase1_bounds.csv')\n",
    "print(f\"\\n✓ Loaded bounds:\")\n",
    "print(bounds_df.to_string(index=False))\n",
    "\n",
    "# Load feature info\n",
    "feature_info_df = pd.read_csv(OUTPUT_DIR / 'phase1_feature_info.csv')\n",
    "print(f\"\\n✓ Loaded feature info\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 29: LOAD PHASE 2 DATA (NEW EXPERIMENTS)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LOADING PHASE 2 DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load new experiment data\n",
    "try:\n",
    "    if PHASE2_DATA_FILE.endswith('.csv'):\n",
    "        df_phase2 = pd.read_csv(PHASE2_DATA_FILE)\n",
    "    else:\n",
    "        df_phase2 = pd.read_excel(PHASE2_DATA_FILE, sheet_name=PHASE2_SHEET_NAME)\n",
    "    print(f\"✓ Loaded: {PHASE2_DATA_FILE}\")\n",
    "    print(f\"  Shape: {df_phase2.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"⚠️ File not found: {PHASE2_DATA_FILE}\")\n",
    "    print(f\"   Please create this file with your new experiment results.\")\n",
    "    print(f\"   Required columns: {SELECTED_FEATURES + [RESPONSE_COLUMN]}\")\n",
    "    raise\n",
    "\n",
    "# Validate columns\n",
    "missing_cols = [c for c in SELECTED_FEATURES + [RESPONSE_COLUMN] if c not in df_phase2.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"Missing columns in Phase 2 data: {missing_cols}\")\n",
    "\n",
    "print(f\"✓ All required columns present\")\n",
    "\n",
    "# Extract X and y\n",
    "X_phase2 = df_phase2[SELECTED_FEATURES].copy()\n",
    "y_phase2 = df_phase2[RESPONSE_COLUMN].copy()\n",
    "\n",
    "# Handle missing values\n",
    "valid = ~y_phase2.isnull() & X_phase2.notna().all(axis=1)\n",
    "n_dropped = (~valid).sum()\n",
    "X_phase2 = X_phase2[valid].reset_index(drop=True)\n",
    "y_phase2 = y_phase2[valid].reset_index(drop=True)\n",
    "\n",
    "if n_dropped > 0:\n",
    "    print(f\"  Dropped {n_dropped} rows with missing values\")\n",
    "\n",
    "print(f\"\\n  Valid experiments: {len(X_phase2)}\")\n",
    "\n",
    "# Store original data\n",
    "X_original_phase2 = X_phase2.copy()\n",
    "y_original_phase2 = y_phase2.copy()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 30: VALIDATE PHASE 2 DATA\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PHASE 2 DATA VALIDATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nFeature Ranges:\")\n",
    "for feat in SELECTED_FEATURES:\n",
    "    bounds_row = bounds_df[bounds_df['feature'] == feat].iloc[0]\n",
    "    actual_min = X_phase2[feat].min()\n",
    "    actual_max = X_phase2[feat].max()\n",
    "    expected_min = bounds_row['min']\n",
    "    expected_max = bounds_row['max']\n",
    "    \n",
    "    in_bounds = (actual_min >= expected_min * 0.9) and (actual_max <= expected_max * 1.1)\n",
    "    status = \"✓\" if in_bounds else \"⚠️\"\n",
    "    \n",
    "    print(f\"  {status} {feat}:\")\n",
    "    print(f\"      Actual: [{actual_min:.4f}, {actual_max:.4f}]\")\n",
    "    print(f\"      Expected: [{expected_min:.4f}, {expected_max:.4f}]\")\n",
    "\n",
    "print(f\"\\nResponse ({RESPONSE_COLUMN}):\")\n",
    "print(f\"  Min:  {y_phase2.min():.4f}\")\n",
    "print(f\"  Max:  {y_phase2.max():.4f}\")\n",
    "print(f\"  Mean: {y_phase2.mean():.4f}\")\n",
    "print(f\"  Std:  {y_phase2.std():.4f}\")\n",
    "\n",
    "# Compare with Phase 1 response stats\n",
    "phase1_stats = phase1_checkpoint['response_stats']\n",
    "print(f\"\\nComparison with Phase 1:\")\n",
    "print(f\"  Phase 1 range: [{phase1_stats['min']:.4f}, {phase1_stats['max']:.4f}]\")\n",
    "print(f\"  Phase 2 range: [{y_phase2.min():.4f}, {y_phase2.max():.4f}]\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 31: PREPARE BOUNDS ARRAY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PREPARING SEARCH BOUNDS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "bounds_dict = {}\n",
    "bounds_array = []\n",
    "\n",
    "for feat in SELECTED_FEATURES:\n",
    "    bounds_row = bounds_df[bounds_df['feature'] == feat].iloc[0]\n",
    "    lb = bounds_row['min']\n",
    "    ub = bounds_row['max']\n",
    "    bounds_dict[feat] = (lb, ub)\n",
    "    bounds_array.append([lb, ub])\n",
    "\n",
    "bounds_array = np.array(bounds_array)\n",
    "\n",
    "print(\"Search bounds (original scale):\")\n",
    "for feat in SELECTED_FEATURES:\n",
    "    lb, ub = bounds_dict[feat]\n",
    "    print(f\"  {feat}: [{lb:.4f}, {ub:.4f}]\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 32: DEFINE SCALING FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DEFINING SCALING FUNCTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class Phase2Scaler:\n",
    "    \"\"\"\n",
    "    Scaler that handles both continuous standardization and binary encoding.\n",
    "    Fits only on training data to prevent leakage.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, continuous_features, binary_features, all_features):\n",
    "        self.continuous_features = continuous_features\n",
    "        self.binary_features = binary_features\n",
    "        self.all_features = all_features\n",
    "        self.scaler = StandardScaler()\n",
    "        self.is_fitted = False\n",
    "        \n",
    "    def fit(self, X):\n",
    "        \"\"\"Fit scaler on training data only.\"\"\"\n",
    "        if self.continuous_features:\n",
    "            self.scaler.fit(X[self.continuous_features])\n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform data using fitted scaler.\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Scaler not fitted. Call fit() first.\")\n",
    "        \n",
    "        X_transformed = X.copy()\n",
    "        \n",
    "        # Scale continuous features\n",
    "        if self.continuous_features:\n",
    "            X_transformed[self.continuous_features] = self.scaler.transform(X[self.continuous_features])\n",
    "        \n",
    "        # Convert binary to [-1, +1]\n",
    "        for col in self.binary_features:\n",
    "            X_transformed[col] = X_transformed[col] * 2 - 1\n",
    "        \n",
    "        return X_transformed\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"Fit and transform in one step.\"\"\"\n",
    "        return self.fit(X).transform(X)\n",
    "    \n",
    "    def inverse_transform_point(self, x_scaled):\n",
    "        \"\"\"Convert a single scaled point back to original space.\"\"\"\n",
    "        x_original = np.array(x_scaled).copy()\n",
    "        \n",
    "        for i, feat in enumerate(self.all_features):\n",
    "            if feat in self.binary_features:\n",
    "                # Convert [-1, +1] back to [0, 1]\n",
    "                x_original[i] = (x_original[i] + 1) / 2\n",
    "            elif feat in self.continuous_features:\n",
    "                # Inverse standardization\n",
    "                cont_idx = self.continuous_features.index(feat)\n",
    "                x_original[i] = x_original[i] * self.scaler.scale_[cont_idx] + self.scaler.mean_[cont_idx]\n",
    "        \n",
    "        return x_original\n",
    "    \n",
    "    def transform_point(self, x_original):\n",
    "        \"\"\"Convert a single original point to scaled space.\"\"\"\n",
    "        x_scaled = np.array(x_original).copy()\n",
    "        \n",
    "        for i, feat in enumerate(self.all_features):\n",
    "            if feat in self.binary_features:\n",
    "                # Convert [0, 1] to [-1, +1]\n",
    "                x_scaled[i] = x_scaled[i] * 2 - 1\n",
    "            elif feat in self.continuous_features:\n",
    "                # Standardization\n",
    "                cont_idx = self.continuous_features.index(feat)\n",
    "                x_scaled[i] = (x_scaled[i] - self.scaler.mean_[cont_idx]) / self.scaler.scale_[cont_idx]\n",
    "        \n",
    "        return x_scaled\n",
    "    \n",
    "    def get_scaled_bounds(self, bounds_array):\n",
    "        \"\"\"Convert bounds to scaled space.\"\"\"\n",
    "        scaled_bounds = []\n",
    "        \n",
    "        for i, feat in enumerate(self.all_features):\n",
    "            if feat in self.binary_features:\n",
    "                scaled_bounds.append([-1, 1])\n",
    "            else:\n",
    "                cont_idx = self.continuous_features.index(feat)\n",
    "                lb_scaled = (bounds_array[i, 0] - self.scaler.mean_[cont_idx]) / self.scaler.scale_[cont_idx]\n",
    "                ub_scaled = (bounds_array[i, 1] - self.scaler.mean_[cont_idx]) / self.scaler.scale_[cont_idx]\n",
    "                scaled_bounds.append([lb_scaled, ub_scaled])\n",
    "        \n",
    "        return np.array(scaled_bounds)\n",
    "\n",
    "print(\"✓ Phase2Scaler class defined\")\n",
    "print(\"  Handles continuous standardization and binary encoding\")\n",
    "print(\"  Prevents data leakage by fitting only on training data\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 33: DEFINE ACQUISITION FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DEFINING ACQUISITION FUNCTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def expected_improvement(X_new, gp, y_best, xi=0.01, minimize=True):\n",
    "    \"\"\"\n",
    "    Expected Improvement acquisition function.\n",
    "    \n",
    "    Args:\n",
    "        X_new: Points to evaluate (n_points, n_features)\n",
    "        gp: Fitted GP model\n",
    "        y_best: Best observed value\n",
    "        xi: Exploration-exploitation trade-off parameter\n",
    "        minimize: If True, we want to find minimum\n",
    "    \n",
    "    Returns:\n",
    "        EI values (n_points,)\n",
    "    \"\"\"\n",
    "    X_new = np.atleast_2d(X_new)\n",
    "    mu, sigma = gp.predict(X_new, return_std=True)\n",
    "    sigma = np.maximum(sigma, 1e-8)\n",
    "    \n",
    "    if minimize:\n",
    "        improvement = y_best - mu - xi\n",
    "    else:\n",
    "        improvement = mu - y_best - xi\n",
    "    \n",
    "    Z = improvement / sigma\n",
    "    ei = improvement * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "    ei[sigma < 1e-8] = 0.0\n",
    "    \n",
    "    return ei\n",
    "\n",
    "\n",
    "def lower_confidence_bound(X_new, gp, kappa=2.0):\n",
    "    \"\"\"\n",
    "    Lower Confidence Bound for minimization.\n",
    "    \n",
    "    Args:\n",
    "        X_new: Points to evaluate\n",
    "        gp: Fitted GP model\n",
    "        kappa: Exploration weight (higher = more exploration)\n",
    "    \n",
    "    Returns:\n",
    "        LCB values (lower is better for minimization)\n",
    "    \"\"\"\n",
    "    X_new = np.atleast_2d(X_new)\n",
    "    mu, sigma = gp.predict(X_new, return_std=True)\n",
    "    return mu - kappa * sigma\n",
    "\n",
    "\n",
    "def upper_confidence_bound(X_new, gp, kappa=2.0):\n",
    "    \"\"\"\n",
    "    Upper Confidence Bound for maximization.\n",
    "    \n",
    "    Args:\n",
    "        X_new: Points to evaluate\n",
    "        gp: Fitted GP model\n",
    "        kappa: Exploration weight (higher = more exploration)\n",
    "    \n",
    "    Returns:\n",
    "        UCB values (higher is better for maximization)\n",
    "    \"\"\"\n",
    "    X_new = np.atleast_2d(X_new)\n",
    "    mu, sigma = gp.predict(X_new, return_std=True)\n",
    "    return mu + kappa * sigma\n",
    "\n",
    "\n",
    "def probability_of_improvement(X_new, gp, y_best, xi=0.01, minimize=True):\n",
    "    \"\"\"\n",
    "    Probability of Improvement acquisition function.\n",
    "    \"\"\"\n",
    "    X_new = np.atleast_2d(X_new)\n",
    "    mu, sigma = gp.predict(X_new, return_std=True)\n",
    "    sigma = np.maximum(sigma, 1e-8)\n",
    "    \n",
    "    if minimize:\n",
    "        Z = (y_best - mu - xi) / sigma\n",
    "    else:\n",
    "        Z = (mu - y_best - xi) / sigma\n",
    "    \n",
    "    return norm.cdf(Z)\n",
    "\n",
    "\n",
    "print(\"✓ Acquisition functions defined:\")\n",
    "print(\"  • Expected Improvement (EI)\")\n",
    "print(\"  • Lower Confidence Bound (LCB) - for minimization\")\n",
    "print(\"  • Upper Confidence Bound (UCB) - for maximization\")\n",
    "print(\"  • Probability of Improvement (PI)\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 34: DEFINE BATCH SELECTION FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DEFINING BATCH SELECTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def optimize_acquisition_batch(gp, bounds_scaled, n_batch, y_best, minimize=True, \n",
    "                                kappa=2.0, n_restarts=20, min_distance=0.3):\n",
    "    \"\"\"\n",
    "    Find a batch of diverse points that optimize the acquisition function.\n",
    "    \n",
    "    Uses greedy selection with diversity constraint.\n",
    "    \"\"\"\n",
    "    \n",
    "    def negative_lcb(x):\n",
    "        \"\"\"Negative LCB for scipy minimization.\"\"\"\n",
    "        x = x.reshape(1, -1)\n",
    "        if minimize:\n",
    "            return lower_confidence_bound(x, gp, kappa)[0]\n",
    "        else:\n",
    "            return -upper_confidence_bound(x, gp, kappa)[0]\n",
    "    \n",
    "    def negative_ei(x):\n",
    "        \"\"\"Negative EI for scipy minimization.\"\"\"\n",
    "        x = x.reshape(1, -1)\n",
    "        return -expected_improvement(x, gp, y_best, minimize=minimize)[0]\n",
    "    \n",
    "    n_features = bounds_scaled.shape[0]\n",
    "    selected_points = []\n",
    "    selected_acq_values = []\n",
    "    \n",
    "    for batch_idx in range(n_batch):\n",
    "        best_candidates = []\n",
    "        \n",
    "        # Multi-start optimization\n",
    "        for _ in range(n_restarts):\n",
    "            # Random starting point\n",
    "            x0 = np.random.uniform(bounds_scaled[:, 0], bounds_scaled[:, 1])\n",
    "            \n",
    "            try:\n",
    "                # Use EI for first point, LCB for diversity\n",
    "                if batch_idx == 0:\n",
    "                    result = minimize(negative_ei, x0, method='L-BFGS-B', \n",
    "                                     bounds=bounds_scaled)\n",
    "                else:\n",
    "                    result = minimize(negative_lcb, x0, method='L-BFGS-B',\n",
    "                                     bounds=bounds_scaled)\n",
    "                \n",
    "                if result.success:\n",
    "                    # Check distance to already selected points\n",
    "                    if selected_points:\n",
    "                        distances = [np.linalg.norm(result.x - p) for p in selected_points]\n",
    "                        if min(distances) >= min_distance:\n",
    "                            best_candidates.append((result.fun, result.x))\n",
    "                    else:\n",
    "                        best_candidates.append((result.fun, result.x))\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        # Select best candidate\n",
    "        if best_candidates:\n",
    "            best_candidates.sort(key=lambda x: x[0])\n",
    "            selected_points.append(best_candidates[0][1])\n",
    "            selected_acq_values.append(best_candidates[0][0])\n",
    "        else:\n",
    "            # Fallback: random point with distance constraint\n",
    "            for _ in range(100):\n",
    "                x_random = np.random.uniform(bounds_scaled[:, 0], bounds_scaled[:, 1])\n",
    "                if not selected_points or min(np.linalg.norm(x_random - p) for p in selected_points) >= min_distance:\n",
    "                    selected_points.append(x_random)\n",
    "                    selected_acq_values.append(np.inf)\n",
    "                    break\n",
    "    \n",
    "    return np.array(selected_points), np.array(selected_acq_values)\n",
    "\n",
    "print(\"✓ Batch selection function defined\")\n",
    "print(\"  Uses greedy selection with diversity constraints\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 35: INITIALIZE BO TRACKING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"INITIALIZING BAYESIAN OPTIMIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize tracking variables\n",
    "bo_history = {\n",
    "    'iterations': [],\n",
    "    'best_values': [],\n",
    "    'proposed_points': [],\n",
    "    'observed_responses': [],\n",
    "    'gp_params': [],\n",
    "}\n",
    "\n",
    "# Current data\n",
    "X_current = X_phase2.copy()\n",
    "y_current = y_phase2.copy()\n",
    "\n",
    "# Current best\n",
    "if MAXIMIZE_RESPONSE:\n",
    "    current_best = y_current.max()\n",
    "    current_best_idx = y_current.idxmax()\n",
    "else:\n",
    "    current_best = y_current.min()\n",
    "    current_best_idx = y_current.idxmin()\n",
    "\n",
    "print(f\"\\nInitial state:\")\n",
    "print(f\"  Training samples: {len(X_current)}\")\n",
    "print(f\"  Current best {'maximum' if MAXIMIZE_RESPONSE else 'minimum'}: {current_best:.4f}\")\n",
    "print(f\"\\n  Best conditions:\")\n",
    "for feat in SELECTED_FEATURES:\n",
    "    print(f\"    {feat}: {X_current.loc[current_best_idx, feat]:.4f}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 36: RUN SINGLE BO ITERATION (Template)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BAYESIAN OPTIMIZATION - ITERATION 1\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "iteration = 1\n",
    "\n",
    "# Step 1: Fit scaler on current data (NO LEAKAGE)\n",
    "print(\"\\n[Step 1] Fitting scaler on training data...\")\n",
    "scaler_bo = Phase2Scaler(\n",
    "    continuous_features=CONTINUOUS_FEATURES_SELECTED,\n",
    "    binary_features=BINARY_FEATURES_SELECTED,\n",
    "    all_features=SELECTED_FEATURES\n",
    ")\n",
    "X_scaled = scaler_bo.fit_transform(X_current)\n",
    "scaled_bounds = scaler_bo.get_scaled_bounds(bounds_array)\n",
    "\n",
    "print(f\"  ✓ Scaler fitted on {len(X_current)} samples\")\n",
    "\n",
    "# Step 2: Fit Gaussian Process\n",
    "print(\"\\n[Step 2] Fitting Gaussian Process...\")\n",
    "kernel = (\n",
    "    ConstantKernel(1.0, constant_value_bounds=(1e-3, 1e3)) * \n",
    "    Matern(length_scale=np.ones(len(SELECTED_FEATURES)), \n",
    "           length_scale_bounds=(1e-2, 1e2), \n",
    "           nu=2.5) + \n",
    "    WhiteKernel(noise_level=0.1, noise_level_bounds=(1e-5, 1e1))\n",
    ")\n",
    "\n",
    "gp = GaussianProcessRegressor(\n",
    "    kernel=kernel,\n",
    "    n_restarts_optimizer=10,\n",
    "    normalize_y=True,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "gp.fit(X_scaled.values, y_current.values)\n",
    "\n",
    "print(f\"  ✓ GP fitted\")\n",
    "print(f\"  Log-marginal-likelihood: {gp.log_marginal_likelihood_value_:.3f}\")\n",
    "\n",
    "# Step 3: Validate GP with LOO-CV (leakage-free)\n",
    "print(\"\\n[Step 3] Validating GP (LOO-CV)...\")\n",
    "loo_preds_gp = []\n",
    "loo_actual_gp = []\n",
    "loo_stds_gp = []\n",
    "\n",
    "for train_idx, test_idx in LeaveOneOut().split(X_current):\n",
    "    # Fit scaler on training fold only\n",
    "    scaler_temp = Phase2Scaler(\n",
    "        continuous_features=CONTINUOUS_FEATURES_SELECTED,\n",
    "        binary_features=BINARY_FEATURES_SELECTED,\n",
    "        all_features=SELECTED_FEATURES\n",
    "    )\n",
    "    X_train_scaled = scaler_temp.fit_transform(X_current.iloc[train_idx])\n",
    "    X_test_scaled = scaler_temp.transform(X_current.iloc[test_idx])\n",
    "    \n",
    "    # Fit GP on training fold\n",
    "    gp_temp = GaussianProcessRegressor(\n",
    "        kernel=kernel.clone_with_theta(kernel.theta),\n",
    "        n_restarts_optimizer=5,\n",
    "        normalize_y=True,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    gp_temp.fit(X_train_scaled.values, y_current.iloc[train_idx].values)\n",
    "    \n",
    "    # Predict on test\n",
    "    pred, std = gp_temp.predict(X_test_scaled.values, return_std=True)\n",
    "    loo_preds_gp.append(pred[0])\n",
    "    loo_stds_gp.append(std[0])\n",
    "    loo_actual_gp.append(y_current.iloc[test_idx].values[0])\n",
    "\n",
    "loo_preds_gp = np.array(loo_preds_gp)\n",
    "loo_stds_gp = np.array(loo_stds_gp)\n",
    "loo_actual_gp = np.array(loo_actual_gp)\n",
    "\n",
    "gp_r2 = r2_score(loo_actual_gp, loo_preds_gp)\n",
    "gp_rmse = np.sqrt(mean_squared_error(loo_actual_gp, loo_preds_gp))\n",
    "\n",
    "# Calibration\n",
    "z_score = 1.96\n",
    "in_ci = np.sum(np.abs(loo_actual_gp - loo_preds_gp) <= z_score * loo_stds_gp)\n",
    "coverage = in_ci / len(loo_actual_gp)\n",
    "\n",
    "print(f\"  LOO-CV R²: {gp_r2:.4f}\")\n",
    "print(f\"  LOO-CV RMSE: {gp_rmse:.4f}\")\n",
    "print(f\"  95% CI Coverage: {coverage:.1%}\")\n",
    "\n",
    "# Step 4: Optimize acquisition function\n",
    "print(\"\\n[Step 4] Optimizing acquisition function...\")\n",
    "next_points_scaled, acq_values = optimize_acquisition_batch(\n",
    "    gp=gp,\n",
    "    bounds_scaled=scaled_bounds,\n",
    "    n_batch=BATCH_SIZE,\n",
    "    y_best=current_best,\n",
    "    minimize=not MAXIMIZE_RESPONSE,\n",
    "    kappa=EXPLORATION_WEIGHT,\n",
    "    n_restarts=N_OPTIMIZER_RESTARTS,\n",
    "    min_distance=MIN_DISTANCE_BETWEEN_POINTS\n",
    ")\n",
    "\n",
    "print(f\"  ✓ Found {len(next_points_scaled)} candidate points\")\n",
    "\n",
    "# Step 5: Convert to original scale\n",
    "print(\"\\n[Step 5] Converting to original scale...\")\n",
    "next_experiments = []\n",
    "\n",
    "for i, x_scaled in enumerate(next_points_scaled):\n",
    "    x_orig = scaler_bo.inverse_transform_point(x_scaled)\n",
    "    \n",
    "    row = {'Experiment_ID': f'Iter{iteration}_Exp{i+1}'}\n",
    "    \n",
    "    for j, feat in enumerate(SELECTED_FEATURES):\n",
    "        if feat in BINARY_FEATURES_SELECTED:\n",
    "            row[feat] = int(round(np.clip(x_orig[j], 0, 1)))\n",
    "        else:\n",
    "            bounds_row = bounds_df[bounds_df['feature'] == feat].iloc[0]\n",
    "            row[feat] = round(np.clip(x_orig[j], bounds_row['min'], bounds_row['max']), 4)\n",
    "    \n",
    "    # Add predictions\n",
    "    mu, sigma = gp.predict(x_scaled.reshape(1, -1), return_std=True)\n",
    "    row['GP_Predicted_Mean'] = round(mu[0], 4)\n",
    "    row['GP_Predicted_Std'] = round(sigma[0], 4)\n",
    "    row['Acquisition_Rank'] = i + 1\n",
    "    \n",
    "    next_experiments.append(row)\n",
    "\n",
    "next_experiments_df = pd.DataFrame(next_experiments)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PROPOSED EXPERIMENTS FOR ITERATION 1\")\n",
    "print(\"=\" * 60)\n",
    "print(next_experiments_df.to_string(index=False))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 37: VISUALIZE GP PREDICTIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GP VISUALIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "n_features = len(SELECTED_FEATURES)\n",
    "fig, axes = plt.subplots(2, n_features, figsize=(4*n_features, 8))\n",
    "\n",
    "# Row 1: 1D slices of GP\n",
    "for i, feat in enumerate(SELECTED_FEATURES):\n",
    "    ax = axes[0, i] if n_features > 1 else axes[0]\n",
    "    \n",
    "    if feat in BINARY_FEATURES_SELECTED:\n",
    "        # Binary: show predictions at 0 and 1\n",
    "        x_base = X_scaled.mean().values.copy()\n",
    "        \n",
    "        preds = []\n",
    "        stds = []\n",
    "        for val in [-1, 1]:  # Scaled values\n",
    "            x_test = x_base.copy()\n",
    "            x_test[i] = val\n",
    "            mu, sigma = gp.predict(x_test.reshape(1, -1), return_std=True)\n",
    "            preds.append(mu[0])\n",
    "            stds.append(sigma[0])\n",
    "        \n",
    "        ax.bar([0, 1], preds, yerr=[1.96*s for s in stds], capsize=5, alpha=0.7, color='steelblue')\n",
    "        ax.set_xticks([0, 1])\n",
    "        ax.set_xlabel(f'{feat}')\n",
    "        \n",
    "    else:\n",
    "        # Continuous: show prediction curve\n",
    "        bounds_row = bounds_df[bounds_df['feature'] == feat].iloc[0]\n",
    "        x_grid_orig = np.linspace(bounds_row['min'], bounds_row['max'], 50)\n",
    "        \n",
    "        x_base = X_scaled.mean().values.copy()\n",
    "        preds = []\n",
    "        stds = []\n",
    "        \n",
    "        for val in x_grid_orig:\n",
    "            x_test = x_base.copy()\n",
    "            x_test[i] = scaler_bo.transform_point(\n",
    "                [val if j == i else X_current[SELECTED_FEATURES[j]].mean() \n",
    "                 for j in range(len(SELECTED_FEATURES))]\n",
    "            )[i]\n",
    "            mu, sigma = gp.predict(x_test.reshape(1, -1), return_std=True)\n",
    "            preds.append(mu[0])\n",
    "            stds.append(sigma[0])\n",
    "        \n",
    "        preds = np.array(preds)\n",
    "        stds = np.array(stds)\n",
    "        \n",
    "        ax.plot(x_grid_orig, preds, 'b-', linewidth=2)\n",
    "        ax.fill_between(x_grid_orig, preds - 1.96*stds, preds + 1.96*stds, alpha=0.3)\n",
    "        ax.scatter(X_current[feat], y_current, c='red', s=50, zorder=5, label='Observed')\n",
    "        \n",
    "        # Mark proposed points\n",
    "        for _, row in next_experiments_df.iterrows():\n",
    "            ax.axvline(row[feat], color='green', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        ax.set_xlabel(feat)\n",
    "    \n",
    "    ax.set_ylabel(RESPONSE_COLUMN)\n",
    "    ax.set_title(f'GP Prediction: {feat}')\n",
    "\n",
    "# Row 2: Validation plots\n",
    "ax_val1 = axes[1, 0] if n_features > 1 else axes[1]\n",
    "ax_val1.errorbar(loo_actual_gp, loo_preds_gp, yerr=1.96*loo_stds_gp, \n",
    "                  fmt='o', alpha=0.7, capsize=3)\n",
    "lims = [min(loo_actual_gp.min(), loo_preds_gp.min()), \n",
    "        max(loo_actual_gp.max(), loo_preds_gp.max())]\n",
    "ax_val1.plot(lims, lims, 'r--', linewidth=2)\n",
    "ax_val1.set_xlabel('Actual')\n",
    "ax_val1.set_ylabel('Predicted')\n",
    "ax_val1.set_title(f'LOO-CV: R² = {gp_r2:.3f}')\n",
    "\n",
    "if n_features > 1:\n",
    "    ax_val2 = axes[1, 1]\n",
    "    residuals_gp = loo_actual_gp - loo_preds_gp\n",
    "    ax_val2.hist(residuals_gp, bins=15, edgecolor='black', alpha=0.7)\n",
    "    ax_val2.axvline(0, color='red', linestyle='--')\n",
    "    ax_val2.set_xlabel('Residual')\n",
    "    ax_val2.set_ylabel('Count')\n",
    "    ax_val2.set_title('Residual Distribution')\n",
    "\n",
    "# Hide extra subplots\n",
    "for j in range(2, n_features):\n",
    "    axes[1, j].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / f'phase2_iteration{iteration}_gp.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 38: EXPORT PROPOSED EXPERIMENTS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EXPORTING PROPOSED EXPERIMENTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Export for lab\n",
    "export_cols = ['Experiment_ID'] + SELECTED_FEATURES + ['GP_Predicted_Mean', 'GP_Predicted_Std']\n",
    "export_df = next_experiments_df[export_cols].copy()\n",
    "\n",
    "# Add column for recording results\n",
    "export_df[RESPONSE_COLUMN] = ''\n",
    "\n",
    "export_path = OUTPUT_DIR / f'phase2_iteration{iteration}_experiments.csv'\n",
    "export_df.to_csv(export_path, index=False)\n",
    "\n",
    "print(f\"✓ Saved: {export_path}\")\n",
    "print(f\"\\nInstructions:\")\n",
    "print(f\"  1. Run these {len(export_df)} experiments in your lab\")\n",
    "print(f\"  2. Record the {RESPONSE_COLUMN} values in the last column\")\n",
    "print(f\"  3. Save the file\")\n",
    "print(f\"  4. Run the next cell to continue optimization\")\n",
    "\n",
    "# Also export as Excel for convenience\n",
    "export_excel_path = OUTPUT_DIR / f'phase2_iteration{iteration}_experiments.xlsx'\n",
    "export_df.to_excel(export_excel_path, index=False)\n",
    "print(f\"✓ Also saved as Excel: {export_excel_path}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 39: UPDATE BO HISTORY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"UPDATING BO HISTORY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "bo_history['iterations'].append(iteration)\n",
    "bo_history['best_values'].append(current_best)\n",
    "bo_history['proposed_points'].append(next_experiments_df.to_dict('records'))\n",
    "bo_history['gp_params'].append({\n",
    "    'log_marginal_likelihood': gp.log_marginal_likelihood_value_,\n",
    "    'kernel_params': str(gp.kernel_),\n",
    "    'loo_r2': gp_r2,\n",
    "    'loo_rmse': gp_rmse,\n",
    "    'ci_coverage': coverage,\n",
    "})\n",
    "\n",
    "# Save history\n",
    "history_path = OUTPUT_DIR / 'phase2_bo_history.pkl'\n",
    "with open(history_path, 'wb') as f:\n",
    "    pickle.dump(bo_history, f)\n",
    "\n",
    "history_json_path = OUTPUT_DIR / 'phase2_bo_history.json'\n",
    "with open(history_json_path, 'w') as f:\n",
    "    json.dump(bo_history, f, indent=2, default=str)\n",
    "\n",
    "print(f\"✓ Saved: {history_path}\")\n",
    "print(f\"✓ Saved: {history_json_path}\")\n",
    "\n",
    "print(f\"\\nIteration {iteration} Summary:\")\n",
    "print(f\"  Current best: {current_best:.4f}\")\n",
    "print(f\"  GP LOO-CV R²: {gp_r2:.4f}\")\n",
    "print(f\"  Proposed {len(next_experiments_df)} new experiments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df63eb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 40: ITERATION SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"ITERATION {iteration} COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\"\"\n",
    "CURRENT STATE:\n",
    "  Training samples: {len(X_current)}\n",
    "  Best observed {RESPONSE_COLUMN}: {current_best:.4f}\n",
    "\n",
    "GP MODEL QUALITY:\n",
    "  LOO-CV R²: {gp_r2:.4f}\n",
    "  LOO-CV RMSE: {gp_rmse:.4f}\n",
    "  95% CI Coverage: {coverage:.1%}\n",
    "\n",
    "PROPOSED EXPERIMENTS:\n",
    "  Count: {len(next_experiments_df)}\n",
    "  Saved to: {export_path}\n",
    "\n",
    "NEXT STEPS:\n",
    "  1. Run the proposed experiments in lab\n",
    "  2. Record results in the exported CSV/Excel file\n",
    "  3. Run Cell 41 to input results and continue\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 41: INPUT NEW RESULTS AND CONTINUE\n",
    "# =============================================================================\n",
    "\n",
    "# ┌─────────────────────────────────────────────────────────────────────────┐\n",
    "# │ RUN THIS CELL AFTER YOU HAVE COMPLETED THE PROPOSED EXPERIMENTS        │\n",
    "# │ AND RECORDED THE RESULTS                                                │\n",
    "# └─────────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"INPUT NEW EXPERIMENTAL RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Option 1: Load from file (recommended)\n",
    "results_file = OUTPUT_DIR / f'phase2_iteration{iteration}_experiments.csv'\n",
    "\n",
    "print(f\"\\nLooking for results in: {results_file}\")\n",
    "\n",
    "if results_file.exists():\n",
    "    new_results = pd.read_csv(results_file)\n",
    "    \n",
    "    # Check if results are filled in\n",
    "    if new_results[RESPONSE_COLUMN].isna().all() or (new_results[RESPONSE_COLUMN] == '').all():\n",
    "        print(f\"\\n⚠️ No results found in {RESPONSE_COLUMN} column!\")\n",
    "        print(f\"   Please fill in the experimental results and re-run this cell.\")\n",
    "        NEW_DATA_READY = False\n",
    "    else:\n",
    "        # Convert response to numeric\n",
    "        new_results[RESPONSE_COLUMN] = pd.to_numeric(new_results[RESPONSE_COLUMN], errors='coerce')\n",
    "        \n",
    "        # Check for valid results\n",
    "        valid_results = new_results[~new_results[RESPONSE_COLUMN].isna()]\n",
    "        \n",
    "        if len(valid_results) == 0:\n",
    "            print(f\"\\n⚠️ No valid numeric results found!\")\n",
    "            NEW_DATA_READY = False\n",
    "        else:\n",
    "            print(f\"\\n✓ Found {len(valid_results)} valid results\")\n",
    "            print(f\"\\nNew experimental results:\")\n",
    "            print(valid_results[SELECTED_FEATURES + [RESPONSE_COLUMN]].to_string(index=False))\n",
    "            NEW_DATA_READY = True\n",
    "else:\n",
    "    print(f\"\\n⚠️ Results file not found: {results_file}\")\n",
    "    print(f\"   Please ensure you've saved the results file.\")\n",
    "    NEW_DATA_READY = False\n",
    "\n",
    "# Option 2: Manual input (alternative)\n",
    "# Uncomment and modify if you prefer manual entry\n",
    "# new_results = pd.DataFrame({\n",
    "#     'Feature_A': [value1, value2, ...],\n",
    "#     'Feature_B': [value1, value2, ...],\n",
    "#     RESPONSE_COLUMN: [result1, result2, ...],\n",
    "# })\n",
    "# NEW_DATA_READY = True\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 42: UPDATE DATASET WITH NEW RESULTS\n",
    "# =============================================================================\n",
    "\n",
    "# ┌─────────────────────────────────────────────────────────────────────────┐\n",
    "# │ RUN THIS CELL ONLY IF NEW_DATA_READY = True                            │\n",
    "# └─────────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "if 'NEW_DATA_READY' in dir() and NEW_DATA_READY:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"UPDATING DATASET\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Extract new data points\n",
    "    X_new = valid_results[SELECTED_FEATURES].copy()\n",
    "    y_new = valid_results[RESPONSE_COLUMN].copy()\n",
    "    \n",
    "    # Append to current dataset\n",
    "    X_current = pd.concat([X_current, X_new], ignore_index=True)\n",
    "    y_current = pd.concat([y_current, y_new], ignore_index=True)\n",
    "    \n",
    "    # Update best\n",
    "    if MAXIMIZE_RESPONSE:\n",
    "        new_best = y_current.max()\n",
    "        new_best_idx = y_current.idxmax()\n",
    "    else:\n",
    "        new_best = y_current.min()\n",
    "        new_best_idx = y_current.idxmin()\n",
    "    \n",
    "    improvement = current_best - new_best if not MAXIMIZE_RESPONSE else new_best - current_best\n",
    "    \n",
    "    print(f\"\\nDataset updated:\")\n",
    "    print(f\"  Previous samples: {len(X_current) - len(X_new)}\")\n",
    "    print(f\"  New samples: {len(X_new)}\")\n",
    "    print(f\"  Total samples: {len(X_current)}\")\n",
    "    \n",
    "    print(f\"\\nBest value update:\")\n",
    "    print(f\"  Previous best: {current_best:.4f}\")\n",
    "    print(f\"  New best: {new_best:.4f}\")\n",
    "    print(f\"  Improvement: {improvement:.4f}\")\n",
    "    \n",
    "    if improvement > 0:\n",
    "        print(f\"\\n✓ IMPROVEMENT FOUND!\")\n",
    "        print(f\"  New best conditions:\")\n",
    "        for feat in SELECTED_FEATURES:\n",
    "            print(f\"    {feat}: {X_current.loc[new_best_idx, feat]:.4f}\")\n",
    "    else:\n",
    "        print(f\"\\n⚠️ No improvement in this iteration\")\n",
    "    \n",
    "    # Update tracking\n",
    "    bo_history['observed_responses'].append(y_new.tolist())\n",
    "    \n",
    "    # Update current best\n",
    "    previous_best = current_best\n",
    "    current_best = new_best\n",
    "    current_best_idx = new_best_idx\n",
    "    \n",
    "    # Save updated data\n",
    "    updated_data = X_current.copy()\n",
    "    updated_data[RESPONSE_COLUMN] = y_current.values\n",
    "    updated_data.to_csv(OUTPUT_DIR / 'phase2_all_data.csv', index=False)\n",
    "    print(f\"\\n✓ Saved updated dataset: {OUTPUT_DIR / 'phase2_all_data.csv'}\")\n",
    "    \n",
    "    # Increment iteration\n",
    "    iteration += 1\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"READY FOR ITERATION {iteration}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Run Cell 43 to continue optimization or Cell 44 to check convergence\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n⚠️ NEW_DATA_READY is False. Please complete Cell 41 first.\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 43: RUN NEXT BO ITERATION\n",
    "# =============================================================================\n",
    "\n",
    "# ┌─────────────────────────────────────────────────────────────────────────┐\n",
    "# │ RUN THIS CELL TO PERFORM ANOTHER BO ITERATION                          │\n",
    "# │ This is essentially a repeat of Cells 36-40 with updated data          │\n",
    "# └─────────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"BAYESIAN OPTIMIZATION - ITERATION {iteration}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Step 1: Fit scaler on current data (NO LEAKAGE)\n",
    "print(\"\\n[Step 1] Fitting scaler on training data...\")\n",
    "scaler_bo = Phase2Scaler(\n",
    "    continuous_features=CONTINUOUS_FEATURES_SELECTED,\n",
    "    binary_features=BINARY_FEATURES_SELECTED,\n",
    "    all_features=SELECTED_FEATURES\n",
    ")\n",
    "X_scaled = scaler_bo.fit_transform(X_current)\n",
    "scaled_bounds = scaler_bo.get_scaled_bounds(bounds_array)\n",
    "print(f\"  ✓ Scaler fitted on {len(X_current)} samples\")\n",
    "\n",
    "# Step 2: Fit Gaussian Process\n",
    "print(\"\\n[Step 2] Fitting Gaussian Process...\")\n",
    "kernel = (\n",
    "    ConstantKernel(1.0, constant_value_bounds=(1e-3, 1e3)) * \n",
    "    Matern(length_scale=np.ones(len(SELECTED_FEATURES)), \n",
    "           length_scale_bounds=(1e-2, 1e2), \n",
    "           nu=2.5) + \n",
    "    WhiteKernel(noise_level=0.1, noise_level_bounds=(1e-5, 1e1))\n",
    ")\n",
    "\n",
    "gp = GaussianProcessRegressor(\n",
    "    kernel=kernel,\n",
    "    n_restarts_optimizer=10,\n",
    "    normalize_y=True,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "gp.fit(X_scaled.values, y_current.values)\n",
    "print(f\"  ✓ GP fitted\")\n",
    "print(f\"  Log-marginal-likelihood: {gp.log_marginal_likelihood_value_:.3f}\")\n",
    "\n",
    "# Step 3: Validate GP (leakage-free LOO-CV)\n",
    "print(\"\\n[Step 3] Validating GP (LOO-CV)...\")\n",
    "loo_preds_gp = []\n",
    "loo_actual_gp = []\n",
    "loo_stds_gp = []\n",
    "\n",
    "for train_idx, test_idx in LeaveOneOut().split(X_current):\n",
    "    scaler_temp = Phase2Scaler(\n",
    "        continuous_features=CONTINUOUS_FEATURES_SELECTED,\n",
    "        binary_features=BINARY_FEATURES_SELECTED,\n",
    "        all_features=SELECTED_FEATURES\n",
    "    )\n",
    "    X_train_scaled = scaler_temp.fit_transform(X_current.iloc[train_idx])\n",
    "    X_test_scaled = scaler_temp.transform(X_current.iloc[test_idx])\n",
    "    \n",
    "    gp_temp = GaussianProcessRegressor(\n",
    "        kernel=kernel.clone_with_theta(kernel.theta),\n",
    "        n_restarts_optimizer=5,\n",
    "        normalize_y=True,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    gp_temp.fit(X_train_scaled.values, y_current.iloc[train_idx].values)\n",
    "    \n",
    "    pred, std = gp_temp.predict(X_test_scaled.values, return_std=True)\n",
    "    loo_preds_gp.append(pred[0])\n",
    "    loo_stds_gp.append(std[0])\n",
    "    loo_actual_gp.append(y_current.iloc[test_idx].values[0])\n",
    "\n",
    "loo_preds_gp = np.array(loo_preds_gp)\n",
    "loo_stds_gp = np.array(loo_stds_gp)\n",
    "loo_actual_gp = np.array(loo_actual_gp)\n",
    "\n",
    "gp_r2 = r2_score(loo_actual_gp, loo_preds_gp)\n",
    "gp_rmse = np.sqrt(mean_squared_error(loo_actual_gp, loo_preds_gp))\n",
    "in_ci = np.sum(np.abs(loo_actual_gp - loo_preds_gp) <= 1.96 * loo_stds_gp)\n",
    "coverage = in_ci / len(loo_actual_gp)\n",
    "\n",
    "print(f\"  LOO-CV R²: {gp_r2:.4f}\")\n",
    "print(f\"  LOO-CV RMSE: {gp_rmse:.4f}\")\n",
    "print(f\"  95% CI Coverage: {coverage:.1%}\")\n",
    "\n",
    "# Step 4: Optimize acquisition function\n",
    "print(\"\\n[Step 4] Optimizing acquisition function...\")\n",
    "next_points_scaled, acq_values = optimize_acquisition_batch(\n",
    "    gp=gp,\n",
    "    bounds_scaled=scaled_bounds,\n",
    "    n_batch=BATCH_SIZE,\n",
    "    y_best=current_best,\n",
    "    minimize=not MAXIMIZE_RESPONSE,\n",
    "    kappa=EXPLORATION_WEIGHT,\n",
    "    n_restarts=N_OPTIMIZER_RESTARTS,\n",
    "    min_distance=MIN_DISTANCE_BETWEEN_POINTS\n",
    ")\n",
    "print(f\"  ✓ Found {len(next_points_scaled)} candidate points\")\n",
    "\n",
    "# Step 5: Convert to original scale\n",
    "print(\"\\n[Step 5] Converting to original scale...\")\n",
    "next_experiments = []\n",
    "\n",
    "for i, x_scaled in enumerate(next_points_scaled):\n",
    "    x_orig = scaler_bo.inverse_transform_point(x_scaled)\n",
    "    \n",
    "    row = {'Experiment_ID': f'Iter{iteration}_Exp{i+1}'}\n",
    "    \n",
    "    for j, feat in enumerate(SELECTED_FEATURES):\n",
    "        if feat in BINARY_FEATURES_SELECTED:\n",
    "            row[feat] = int(round(np.clip(x_orig[j], 0, 1)))\n",
    "        else:\n",
    "            bounds_row = bounds_df[bounds_df['feature'] == feat].iloc[0]\n",
    "            row[feat] = round(np.clip(x_orig[j], bounds_row['min'], bounds_row['max']), 4)\n",
    "    \n",
    "    mu, sigma = gp.predict(x_scaled.reshape(1, -1), return_std=True)\n",
    "    row['GP_Predicted_Mean'] = round(mu[0], 4)\n",
    "    row['GP_Predicted_Std'] = round(sigma[0], 4)\n",
    "    row['Acquisition_Rank'] = i + 1\n",
    "    \n",
    "    next_experiments.append(row)\n",
    "\n",
    "next_experiments_df = pd.DataFrame(next_experiments)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"PROPOSED EXPERIMENTS FOR ITERATION {iteration}\")\n",
    "print(\"=\" * 60)\n",
    "print(next_experiments_df.to_string(index=False))\n",
    "\n",
    "# Export\n",
    "export_cols = ['Experiment_ID'] + SELECTED_FEATURES + ['GP_Predicted_Mean', 'GP_Predicted_Std']\n",
    "export_df = next_experiments_df[export_cols].copy()\n",
    "export_df[RESPONSE_COLUMN] = ''\n",
    "\n",
    "export_path = OUTPUT_DIR / f'phase2_iteration{iteration}_experiments.csv'\n",
    "export_df.to_csv(export_path, index=False)\n",
    "export_df.to_excel(OUTPUT_DIR / f'phase2_iteration{iteration}_experiments.xlsx', index=False)\n",
    "\n",
    "print(f\"\\n✓ Saved: {export_path}\")\n",
    "\n",
    "# Update history\n",
    "bo_history['iterations'].append(iteration)\n",
    "bo_history['best_values'].append(current_best)\n",
    "bo_history['proposed_points'].append(next_experiments_df.to_dict('records'))\n",
    "bo_history['gp_params'].append({\n",
    "    'log_marginal_likelihood': gp.log_marginal_likelihood_value_,\n",
    "    'kernel_params': str(gp.kernel_),\n",
    "    'loo_r2': gp_r2,\n",
    "    'loo_rmse': gp_rmse,\n",
    "    'ci_coverage': coverage,\n",
    "})\n",
    "\n",
    "# Save history\n",
    "with open(OUTPUT_DIR / 'phase2_bo_history.pkl', 'wb') as f:\n",
    "    pickle.dump(bo_history, f)\n",
    "with open(OUTPUT_DIR / 'phase2_bo_history.json', 'w') as f:\n",
    "    json.dump(bo_history, f, indent=2, default=str)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "ax1 = axes[0]\n",
    "ax1.errorbar(loo_actual_gp, loo_preds_gp, yerr=1.96*loo_stds_gp, fmt='o', alpha=0.7, capsize=3)\n",
    "lims = [min(loo_actual_gp.min(), loo_preds_gp.min()), max(loo_actual_gp.max(), loo_preds_gp.max())]\n",
    "ax1.plot(lims, lims, 'r--', linewidth=2)\n",
    "ax1.set_xlabel('Actual')\n",
    "ax1.set_ylabel('Predicted')\n",
    "ax1.set_title(f'LOO-CV: R² = {gp_r2:.3f}')\n",
    "\n",
    "ax2 = axes[1]\n",
    "ax2.plot(bo_history['iterations'], bo_history['best_values'], 'bo-', linewidth=2, markersize=8)\n",
    "ax2.set_xlabel('Iteration')\n",
    "ax2.set_ylabel(f'Best {RESPONSE_COLUMN}')\n",
    "ax2.set_title('Optimization Progress')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "ax3 = axes[2]\n",
    "r2_values = [p['loo_r2'] for p in bo_history['gp_params']]\n",
    "ax3.plot(bo_history['iterations'], r2_values, 'go-', linewidth=2, markersize=8)\n",
    "ax3.set_xlabel('Iteration')\n",
    "ax3.set_ylabel('LOO-CV R²')\n",
    "ax3.set_title('GP Model Quality')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / f'phase2_iteration{iteration}_summary.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ITERATION {iteration} COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Current best: {current_best:.4f}\")\n",
    "print(f\"Run experiments, then go back to Cell 41 to input results\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 44: CHECK CONVERGENCE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CONVERGENCE CHECK\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if len(bo_history['best_values']) >= 2:\n",
    "    best_values = bo_history['best_values']\n",
    "    iterations = bo_history['iterations']\n",
    "    \n",
    "    # Calculate improvements\n",
    "    improvements = []\n",
    "    for i in range(1, len(best_values)):\n",
    "        if MAXIMIZE_RESPONSE:\n",
    "            imp = best_values[i] - best_values[i-1]\n",
    "        else:\n",
    "            imp = best_values[i-1] - best_values[i]\n",
    "        improvements.append(imp)\n",
    "    \n",
    "    print(f\"\\nOptimization History:\")\n",
    "    print(\"-\" * 50)\n",
    "    for i, (it, bv) in enumerate(zip(iterations, best_values)):\n",
    "        imp_str = f\" (Δ = {improvements[i-1]:+.4f})\" if i > 0 else \"\"\n",
    "        print(f\"  Iteration {it}: Best = {bv:.4f}{imp_str}\")\n",
    "    \n",
    "    # Check convergence criteria\n",
    "    print(f\"\\nConvergence Analysis:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Criterion 1: Recent improvement\n",
    "    recent_improvements = improvements[-PATIENCE:] if len(improvements) >= PATIENCE else improvements\n",
    "    max_recent_improvement = max(recent_improvements) if recent_improvements else 0\n",
    "    \n",
    "    converged_improvement = max_recent_improvement < IMPROVEMENT_THRESHOLD\n",
    "    print(f\"  Max improvement in last {PATIENCE} iterations: {max_recent_improvement:.4f}\")\n",
    "    print(f\"  Threshold: {IMPROVEMENT_THRESHOLD}\")\n",
    "    print(f\"  Converged by improvement? {'YES ✓' if converged_improvement else 'NO'}\")\n",
    "    \n",
    "    # Criterion 2: Relative improvement\n",
    "    total_range = max(best_values) - min(best_values) if len(best_values) > 1 else 0\n",
    "    relative_improvement = max_recent_improvement / (total_range + 1e-8)\n",
    "    \n",
    "    converged_relative = relative_improvement < 0.01\n",
    "    print(f\"\\n  Relative improvement: {relative_improvement:.2%}\")\n",
    "    print(f\"  Converged by relative improvement? {'YES ✓' if converged_relative else 'NO'}\")\n",
    "    \n",
    "    # Criterion 3: GP uncertainty at best point\n",
    "    if 'gp' in dir():\n",
    "        x_best = X_current.iloc[current_best_idx].values\n",
    "        x_best_scaled = scaler_bo.transform_point(x_best)\n",
    "        _, sigma_best = gp.predict(x_best_scaled.reshape(1, -1), return_std=True)\n",
    "        \n",
    "        relative_uncertainty = sigma_best[0] / (y_current.std() + 1e-8)\n",
    "        converged_uncertainty = relative_uncertainty < 0.1\n",
    "        \n",
    "        print(f\"\\n  GP uncertainty at best point: {sigma_best[0]:.4f}\")\n",
    "        print(f\"  Relative to response std: {relative_uncertainty:.2%}\")\n",
    "        print(f\"  Converged by uncertainty? {'YES ✓' if converged_uncertainty else 'NO'}\")\n",
    "    else:\n",
    "        converged_uncertainty = False\n",
    "    \n",
    "    # Overall convergence\n",
    "    n_criteria_met = sum([converged_improvement, converged_relative, converged_uncertainty])\n",
    "    CONVERGED = n_criteria_met >= 2\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"CONVERGENCE STATUS: {'CONVERGED ✓' if CONVERGED else 'NOT CONVERGED'}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"  Criteria met: {n_criteria_met}/3\")\n",
    "    \n",
    "    if CONVERGED:\n",
    "        print(f\"\\n  Recommendation: Proceed to confirmation runs (Cell 45)\")\n",
    "    else:\n",
    "        print(f\"\\n  Recommendation: Continue optimization (Cell 43)\")\n",
    "        print(f\"  Iterations completed: {len(iterations)}\")\n",
    "        print(f\"  Max iterations: {MAX_ITERATIONS}\")\n",
    "        \n",
    "        if len(iterations) >= MAX_ITERATIONS:\n",
    "            print(f\"\\n  ⚠️ Maximum iterations reached!\")\n",
    "            print(f\"     Consider increasing MAX_ITERATIONS or accepting current best\")\n",
    "else:\n",
    "    print(\"Not enough iterations to check convergence.\")\n",
    "    print(\"Complete at least 2 iterations first.\")\n",
    "    CONVERGED = False\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 45: FINAL RESULTS AND CONFIRMATION RUNS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\"\"\n",
    "OPTIMIZATION COMPLETE\n",
    "\n",
    "BEST RESULT FOUND:\n",
    "  {RESPONSE_COLUMN}: {current_best:.4f}\n",
    "  \n",
    "OPTIMAL CONDITIONS:\n",
    "\"\"\")\n",
    "\n",
    "for feat in SELECTED_FEATURES:\n",
    "    value = X_current.loc[current_best_idx, feat]\n",
    "    bounds_row = bounds_df[bounds_df['feature'] == feat].iloc[0]\n",
    "    \n",
    "    if feat in BINARY_FEATURES_SELECTED:\n",
    "        # Decode binary\n",
    "        if feat in BINARY_MAPPINGS:\n",
    "            mapping = BINARY_MAPPINGS[feat]\n",
    "            original_value = [k for k, v in mapping.items() if v == value][0]\n",
    "            print(f\"  {feat}: {int(value)} (original: {original_value})\")\n",
    "        else:\n",
    "            print(f\"  {feat}: {int(value)}\")\n",
    "    else:\n",
    "        print(f\"  {feat}: {value:.4f}\")\n",
    "        print(f\"      Bounds: [{bounds_row['min']:.4f}, {bounds_row['max']:.4f}]\")\n",
    "\n",
    "# GP prediction at optimum\n",
    "if 'gp' in dir() and 'scaler_bo' in dir():\n",
    "    x_opt = X_current.loc[current_best_idx, SELECTED_FEATURES].values\n",
    "    x_opt_scaled = scaler_bo.transform_point(x_opt)\n",
    "    mu_opt, sigma_opt = gp.predict(x_opt_scaled.reshape(1, -1), return_std=True)\n",
    "    \n",
    "    print(f\"\\nGP PREDICTION AT OPTIMUM:\")\n",
    "    print(f\"  Predicted mean: {mu_opt[0]:.4f}\")\n",
    "    print(f\"  Predicted std: {sigma_opt[0]:.4f}\")\n",
    "    print(f\"  95% CI: [{mu_opt[0] - 1.96*sigma_opt[0]:.4f}, {mu_opt[0] + 1.96*sigma_opt[0]:.4f}]\")\n",
    "\n",
    "# Generate confirmation runs\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"CONFIRMATION RUNS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "n_confirmation = 5\n",
    "confirmation_runs = []\n",
    "\n",
    "for i in range(n_confirmation):\n",
    "    run = {'Run_ID': f'Confirm_{i+1}'}\n",
    "    for feat in SELECTED_FEATURES:\n",
    "        run[feat] = X_current.loc[current_best_idx, feat]\n",
    "    run[f'Expected_{RESPONSE_COLUMN}'] = current_best\n",
    "    confirmation_runs.append(run)\n",
    "\n",
    "confirmation_df = pd.DataFrame(confirmation_runs)\n",
    "confirmation_df[f'Actual_{RESPONSE_COLUMN}'] = ''\n",
    "\n",
    "print(f\"\\nRun {n_confirmation} confirmation experiments at optimal conditions:\")\n",
    "print(confirmation_df.to_string(index=False))\n",
    "\n",
    "# Save confirmation runs\n",
    "confirmation_path = OUTPUT_DIR / 'phase2_confirmation_runs.csv'\n",
    "confirmation_df.to_csv(confirmation_path, index=False)\n",
    "confirmation_df.to_excel(OUTPUT_DIR / 'phase2_confirmation_runs.xlsx', index=False)\n",
    "\n",
    "print(f\"\\n✓ Saved: {confirmation_path}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 46: OPTIMIZATION SUMMARY REPORT\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"OPTIMIZATION SUMMARY REPORT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Plot optimization history\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Best value over iterations\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(bo_history['iterations'], bo_history['best_values'], 'bo-', linewidth=2, markersize=10)\n",
    "ax1.set_xlabel('Iteration')\n",
    "ax1.set_ylabel(f'Best {RESPONSE_COLUMN}')\n",
    "ax1.set_title('Optimization Progress')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add improvement annotations\n",
    "for i in range(1, len(bo_history['best_values'])):\n",
    "    if MAXIMIZE_RESPONSE:\n",
    "        imp = bo_history['best_values'][i] - bo_history['best_values'][i-1]\n",
    "    else:\n",
    "        imp = bo_history['best_values'][i-1] - bo_history['best_values'][i]\n",
    "    if imp > 0:\n",
    "        ax1.annotate(f'+{imp:.3f}', \n",
    "                     xy=(bo_history['iterations'][i], bo_history['best_values'][i]),\n",
    "                     xytext=(5, 10), textcoords='offset points', fontsize=8, color='green')\n",
    "\n",
    "# Plot 2: GP R² over iterations\n",
    "ax2 = axes[0, 1]\n",
    "r2_values = [p['loo_r2'] for p in bo_history['gp_params']]\n",
    "ax2.plot(bo_history['iterations'], r2_values, 'go-', linewidth=2, markersize=10)\n",
    "ax2.set_xlabel('Iteration')\n",
    "ax2.set_ylabel('LOO-CV R²')\n",
    "ax2.set_title('GP Model Quality Over Time')\n",
    "ax2.set_ylim([0, 1])\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: All observed data\n",
    "ax3 = axes[1, 0]\n",
    "ax3.hist(y_current, bins=20, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "ax3.axvline(current_best, color='red', linewidth=2, linestyle='--', label=f'Best: {current_best:.4f}')\n",
    "ax3.set_xlabel(RESPONSE_COLUMN)\n",
    "ax3.set_ylabel('Frequency')\n",
    "ax3.set_title(f'Distribution of All Observed {RESPONSE_COLUMN}')\n",
    "ax3.legend()\n",
    "\n",
    "# Plot 4: Feature importances (from correlation)\n",
    "ax4 = axes[1, 1]\n",
    "correlations_final = X_current.corrwith(y_current)\n",
    "colors = ['forestgreen' if c > 0 else 'crimson' for c in correlations_final]\n",
    "ax4.barh(SELECTED_FEATURES, np.abs(correlations_final), color=colors)\n",
    "ax4.set_xlabel('|Correlation|')\n",
    "ax4.set_title('Feature-Response Correlations (Final Dataset)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'phase2_final_summary.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(f\"\"\"\n",
    "SUMMARY STATISTICS:\n",
    "───────────────────────────────────────────────────────────────\n",
    "\n",
    "EXPERIMENT COUNT:\n",
    "  Phase 1 (feature selection): {phase1_checkpoint['metadata']['n_initial_experiments']}\n",
    "  Phase 2 (optimization): {len(X_current)}\n",
    "  Total experiments: {phase1_checkpoint['metadata']['n_initial_experiments'] + len(X_current)}\n",
    "\n",
    "OPTIMIZATION:\n",
    "  Iterations completed: {len(bo_history['iterations'])}\n",
    "  Starting best: {bo_history['best_values'][0]:.4f}\n",
    "  Final best: {current_best:.4f}\n",
    "  Total improvement: {abs(bo_history['best_values'][0] - current_best):.4f}\n",
    "\n",
    "GP MODEL (Final):\n",
    "  LOO-CV R²: {bo_history['gp_params'][-1]['loo_r2']:.4f}\n",
    "  LOO-CV RMSE: {bo_history['gp_params'][-1]['loo_rmse']:.4f}\n",
    "  95% CI Coverage: {bo_history['gp_params'][-1]['ci_coverage']:.1%}\n",
    "\n",
    "OPTIMAL CONDITIONS:\n",
    "\"\"\")\n",
    "\n",
    "for feat in SELECTED_FEATURES:\n",
    "    value = X_current.loc[current_best_idx, feat]\n",
    "    print(f\"  {feat}: {value:.4f}\" if feat not in BINARY_FEATURES_SELECTED else f\"  {feat}: {int(value)}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "FILES GENERATED:\n",
    "  • {OUTPUT_DIR}/phase1_checkpoint.json - Phase 1 settings\n",
    "  • {OUTPUT_DIR}/phase1_feature_info.csv - Feature details\n",
    "  • {OUTPUT_DIR}/phase2_all_data.csv - All experimental data\n",
    "  • {OUTPUT_DIR}/phase2_bo_history.json - Optimization history\n",
    "  • {OUTPUT_DIR}/phase2_confirmation_runs.csv - Confirmation experiments\n",
    "  • Various PNG plots\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 47: EXPORT FINAL REPORT\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EXPORTING FINAL REPORT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create comprehensive final report\n",
    "final_report = {\n",
    "    'optimization_summary': {\n",
    "        'objective': 'Maximize' if MAXIMIZE_RESPONSE else 'Minimize',\n",
    "        'response_variable': RESPONSE_COLUMN,\n",
    "        'final_best_value': float(current_best),\n",
    "        'total_iterations': len(bo_history['iterations']),\n",
    "        'total_experiments_phase2': len(X_current),\n",
    "    },\n",
    "    'optimal_conditions': {\n",
    "        feat: float(X_current.loc[current_best_idx, feat]) if feat not in BINARY_FEATURES_SELECTED \n",
    "              else int(X_current.loc[current_best_idx, feat])\n",
    "        for feat in SELECTED_FEATURES\n",
    "    },\n",
    "    'feature_selection': {\n",
    "        'method': 'Consensus of Correlation, Lasso, and PLS-VIP',\n",
    "        'selected_features': SELECTED_FEATURES,\n",
    "        'binary_features': BINARY_FEATURES_SELECTED,\n",
    "        'continuous_features': CONTINUOUS_FEATURES_SELECTED,\n",
    "    },\n",
    "    'optimization_history': {\n",
    "        'iterations': bo_history['iterations'],\n",
    "        'best_values': bo_history['best_values'],\n",
    "    },\n",
    "    'final_gp_model': bo_history['gp_params'][-1] if bo_history['gp_params'] else {},\n",
    "    'bounds': bounds_df.to_dict('records'),\n",
    "    'binary_mappings': {k: {str(k2): v2 for k2, v2 in v.items()} \n",
    "                        for k, v in BINARY_MAPPINGS.items() if k in SELECTED_FEATURES},\n",
    "}\n",
    "\n",
    "# Save final report\n",
    "report_path = OUTPUT_DIR / 'final_optimization_report.json'\n",
    "with open(report_path, 'w') as f:\n",
    "    json.dump(final_report, f, indent=2, default=str)\n",
    "\n",
    "print(f\"✓ Saved: {report_path}\")\n",
    "\n",
    "# Save all data\n",
    "all_data_path = OUTPUT_DIR / 'all_experimental_data.csv'\n",
    "final_data = X_current.copy()\n",
    "final_data[RESPONSE_COLUMN] = y_current.values\n",
    "final_data['Source'] = ['Phase2'] * len(final_data)\n",
    "final_data.to_csv(all_data_path, index=False)\n",
    "\n",
    "print(f\"✓ Saved: {all_data_path}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "{'='*70}\n",
    "BAYESIAN OPTIMIZATION PIPELINE COMPLETE\n",
    "{'='*70}\n",
    "\n",
    "Next Steps:\n",
    "1. Run confirmation experiments from: phase2_confirmation_runs.csv\n",
    "2. Compare confirmation results to predicted optimal\n",
    "3. If confirmation successful → implement optimal conditions\n",
    "4. If confirmation fails → review and potentially continue optimization\n",
    "\n",
    "Thank you for using the Integrated BO Pipeline!\n",
    "{'='*70}\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
