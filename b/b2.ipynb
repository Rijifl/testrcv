{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d81de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 1: IMPORTS\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import Ridge, RidgeCV, LassoCV\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.model_selection import LeaveOneOut, cross_val_score\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ML FEATURE SCREENING FOR BAYESIAN OPTIMIZATION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "Purpose: Screen features from observational data before BO\n",
    "\n",
    "Workflow:\n",
    "  1. This pipeline → Select 3-5 important features\n",
    "  2. Bayesian Optimization → Find optimal values for selected features\n",
    "\n",
    "Appropriate when:\n",
    "  - Data is observational (not designed factorial)\n",
    "  - ~20-50 initial experiments\n",
    "  - Want quick screening without formal DOE inference\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b68c9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 2: CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "FILE_PATH = \"your_data.xlsx\"\n",
    "RESPONSE_COLUMN = \"yield\"\n",
    "EXCLUDE_COLUMNS = [\"experiment_id\"]\n",
    "\n",
    "# Feature selection targets\n",
    "TARGET_FEATURES_FOR_BO = 4         # Aim for 3-5 features\n",
    "MIN_FEATURES = 3                   # Don't go below this\n",
    "MAX_FEATURES = 6                   # Don't exceed this\n",
    "\n",
    "# Thresholds\n",
    "CORRELATION_STRONG = 0.4\n",
    "CORRELATION_MODERATE = 0.2\n",
    "VIP_IMPORTANT = 1.0\n",
    "VIP_MODERATE = 0.8\n",
    "MULTICOLLINEARITY_THRESHOLD = 0.7\n",
    "\n",
    "# BO Configuration\n",
    "MAXIMIZE_RESPONSE = True           # True if higher response is better\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Response: {RESPONSE_COLUMN}\")\n",
    "print(f\"  Target features for BO: {TARGET_FEATURES_FOR_BO}\")\n",
    "print(f\"  Optimization direction: {'Maximize' if MAXIMIZE_RESPONSE else 'Minimize'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d9a1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 3: LOAD DATA\n",
    "# =============================================================================\n",
    "\n",
    "try:\n",
    "    df = pd.read_excel(FILE_PATH)\n",
    "    print(f\"✓ Loaded: {df.shape[0]} samples, {df.shape[1]} columns\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Creating example observational data...\")\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    n = 30\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'experiment_id': range(1, n+1),\n",
    "        'temperature_C': np.random.uniform(50, 90, n),\n",
    "        'pressure_bar': np.random.uniform(1, 5, n),\n",
    "        'pH': np.random.uniform(5, 9, n),\n",
    "        'concentration_M': np.random.uniform(0.1, 1.0, n),\n",
    "        'reaction_time_min': np.random.uniform(15, 60, n),\n",
    "        'catalyst_loading_g': np.random.uniform(0.5, 3.0, n),\n",
    "        'stirring_rpm': np.random.uniform(200, 600, n),\n",
    "        'solvent_ratio': np.random.uniform(0.3, 0.7, n),\n",
    "        'humidity_pct': np.random.uniform(40, 60, n),\n",
    "        'particle_size_um': np.random.uniform(20, 80, n),\n",
    "    })\n",
    "    \n",
    "    # True relationships (unknown to experimenter)\n",
    "    df['yield'] = (\n",
    "        35 +\n",
    "        0.4 * df['temperature_C'] +\n",
    "        3.0 * df['catalyst_loading_g'] +\n",
    "        -2.5 * df['pH'] +\n",
    "        0.15 * df['reaction_time_min'] +\n",
    "        0.05 * df['temperature_C'] * df['catalyst_loading_g'] / 10 +  # Interaction\n",
    "        np.random.normal(0, 4, n)\n",
    "    ).clip(20, 95)\n",
    "    \n",
    "    print(f\"✓ Example data created: {n} samples\")\n",
    "\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cf69c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 4: PREPARE DATA\n",
    "# =============================================================================\n",
    "\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "feature_cols = [c for c in numeric_cols if c not in EXCLUDE_COLUMNS + [RESPONSE_COLUMN]]\n",
    "\n",
    "X = df[feature_cols].copy()\n",
    "y = df[RESPONSE_COLUMN].copy()\n",
    "\n",
    "n_samples = len(X)\n",
    "n_features = len(feature_cols)\n",
    "\n",
    "print(f\"Features ({n_features}): {feature_cols}\")\n",
    "print(f\"Response: {RESPONSE_COLUMN}\")\n",
    "print(f\"Samples: {n_samples}\")\n",
    "print(f\"Samples/Features ratio: {n_samples/n_features:.1f}\")\n",
    "\n",
    "if n_samples / n_features < 5:\n",
    "    print(\"\\n⚠️  Low sample-to-feature ratio\")\n",
    "    print(\"   Using regularized methods only (correlation, Lasso, Ridge, PLS)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332b1551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 5: HANDLE MISSING VALUES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DATA CLEANING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Missing values\n",
    "missing_X = X.isnull().sum()\n",
    "missing_y = y.isnull().sum()\n",
    "\n",
    "if missing_X.sum() > 0:\n",
    "    print(f\"Missing in features: {missing_X[missing_X > 0].to_dict()}\")\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns, index=X.index)\n",
    "    print(\"✓ Imputed with median\")\n",
    "else:\n",
    "    print(\"✓ No missing values in features\")\n",
    "\n",
    "if missing_y > 0:\n",
    "    valid_mask = ~y.isnull()\n",
    "    X = X[valid_mask]\n",
    "    y = y[valid_mask]\n",
    "    print(f\"✓ Removed {missing_y} rows with missing response\")\n",
    "\n",
    "print(f\"\\nClean dataset: {len(X)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488303fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 6: STORE ORIGINAL RANGES (FOR BO BOUNDS)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FEATURE RANGES (Original Scale)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"These will become your BO search bounds\\n\")\n",
    "\n",
    "feature_ranges = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'min': X.min().values,\n",
    "    'max': X.max().values,\n",
    "    'mean': X.mean().values,\n",
    "    'std': X.std().values,\n",
    "    'range': (X.max() - X.min()).values\n",
    "})\n",
    "\n",
    "print(feature_ranges.to_string(index=False))\n",
    "\n",
    "# Store for later\n",
    "original_X = X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32ec61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 7: STANDARDIZE FOR MODELING\n",
    "# =============================================================================\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X),\n",
    "    columns=X.columns,\n",
    "    index=X.index\n",
    ")\n",
    "\n",
    "print(\"✓ Features standardized for modeling\")\n",
    "print(\"  (Original ranges preserved for BO bounds)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b533c884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 8: VISUAL INSPECTION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"VISUAL INSPECTION: Feature vs Response\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "n_cols = min(4, n_features)\n",
    "n_rows = int(np.ceil(n_features / n_cols))\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(4*n_cols, 3.5*n_rows))\n",
    "axes = axes.flatten() if n_features > 1 else [axes]\n",
    "\n",
    "for i, col in enumerate(feature_cols):\n",
    "    ax = axes[i]\n",
    "    ax.scatter(X[col], y, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    # Trend line\n",
    "    z = np.polyfit(X[col], y, 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_line = np.linspace(X[col].min(), X[col].max(), 100)\n",
    "    ax.plot(x_line, p(x_line), 'r--', linewidth=2)\n",
    "    \n",
    "    corr = X[col].corr(y)\n",
    "    ax.set_xlabel(col)\n",
    "    ax.set_ylabel(RESPONSE_COLUMN)\n",
    "    ax.set_title(f'r = {corr:.3f}')\n",
    "\n",
    "for j in range(i+1, len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "\n",
    "plt.suptitle('Feature vs Response (Look for linear and non-linear patterns)', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ee9e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 9: METHOD 1 - CORRELATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"METHOD 1: PEARSON CORRELATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "correlations = X.corrwith(y)\n",
    "\n",
    "corr_results = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'correlation': correlations.values,\n",
    "    'abs_correlation': np.abs(correlations.values)\n",
    "}).sort_values('abs_correlation', ascending=False).reset_index(drop=True)\n",
    "\n",
    "corr_results['rank_corr'] = range(1, len(corr_results) + 1)\n",
    "corr_results['direction'] = ['Positive' if c > 0 else 'Negative' for c in corr_results['correlation']]\n",
    "\n",
    "def strength(r):\n",
    "    r = abs(r)\n",
    "    if r >= CORRELATION_STRONG: return 'Strong'\n",
    "    elif r >= CORRELATION_MODERATE: return 'Moderate'\n",
    "    else: return 'Weak'\n",
    "\n",
    "corr_results['strength'] = corr_results['correlation'].apply(strength)\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "print(corr_results[['rank_corr', 'feature', 'correlation', 'direction', 'strength']].to_string(index=False))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['forestgreen' if c > 0 else 'crimson' for c in corr_results['correlation']]\n",
    "plt.barh(corr_results['feature'][::-1], corr_results['abs_correlation'][::-1], color=colors[::-1])\n",
    "plt.axvline(x=CORRELATION_STRONG, color='green', linestyle='--', label=f'Strong ({CORRELATION_STRONG})')\n",
    "plt.axvline(x=CORRELATION_MODERATE, color='orange', linestyle='--', label=f'Moderate ({CORRELATION_MODERATE})')\n",
    "plt.xlabel('|Correlation|')\n",
    "plt.title('Correlation with Response\\n(Green=Positive, Red=Negative)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca577c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 10: METHOD 2 - MUTUAL INFORMATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"METHOD 2: MUTUAL INFORMATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "mi_scores = mutual_info_regression(X_scaled, y, random_state=RANDOM_STATE)\n",
    "\n",
    "mi_results = pd.DataFrame({\n",
    "    'feature': X_scaled.columns,\n",
    "    'MI_score': mi_scores\n",
    "}).sort_values('MI_score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "mi_results['rank_mi'] = range(1, len(mi_results) + 1)\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "print(mi_results[['rank_mi', 'feature', 'MI_score']].to_string(index=False))\n",
    "\n",
    "# Check for non-linear relationships\n",
    "comparison = corr_results[['feature', 'rank_corr']].merge(mi_results[['feature', 'rank_mi']], on='feature')\n",
    "comparison['rank_diff'] = comparison['rank_corr'] - comparison['rank_mi']\n",
    "potential_nonlinear = comparison[comparison['rank_diff'] >= 3]['feature'].tolist()\n",
    "\n",
    "if potential_nonlinear:\n",
    "    print(f\"\\n⚠️  Possible non-linear relationships: {potential_nonlinear}\")\n",
    "    print(\"   (MI rank much higher than correlation rank)\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(mi_results['feature'][::-1], mi_results['MI_score'][::-1], color='steelblue')\n",
    "plt.xlabel('Mutual Information')\n",
    "plt.title('Mutual Information (Captures Non-linear Relationships)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82f5b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 11: METHOD 3 - LASSO\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"METHOD 3: LASSO (Automatic Selection)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "lasso = LassoCV(cv=5, max_iter=10000, random_state=RANDOM_STATE)\n",
    "lasso.fit(X_scaled, y)\n",
    "\n",
    "lasso_results = pd.DataFrame({\n",
    "    'feature': X_scaled.columns,\n",
    "    'coefficient': lasso.coef_,\n",
    "    'abs_coefficient': np.abs(lasso.coef_)\n",
    "}).sort_values('abs_coefficient', ascending=False).reset_index(drop=True)\n",
    "\n",
    "lasso_results['rank_lasso'] = range(1, len(lasso_results) + 1)\n",
    "lasso_results['selected'] = lasso_results['coefficient'] != 0\n",
    "\n",
    "print(f\"\\nLasso alpha: {lasso.alpha_:.4f}\")\n",
    "print(f\"Features selected: {lasso_results['selected'].sum()}/{len(lasso_results)}\")\n",
    "print(\"\\nResults:\")\n",
    "print(lasso_results[['rank_lasso', 'feature', 'coefficient', 'selected']].to_string(index=False))\n",
    "\n",
    "selected_by_lasso = lasso_results[lasso_results['selected']]['feature'].tolist()\n",
    "print(f\"\\n✓ Lasso selected: {selected_by_lasso}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['forestgreen' if s else 'lightgray' for s in lasso_results['selected']]\n",
    "plt.barh(lasso_results['feature'][::-1], lasso_results['abs_coefficient'][::-1], color=colors[::-1])\n",
    "plt.xlabel('|Coefficient|')\n",
    "plt.title('Lasso (Green=Selected, Gray=Eliminated)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdbe612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 12: METHOD 4 - RIDGE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"METHOD 4: RIDGE (Stable Coefficients)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "ridge = RidgeCV(alphas=[0.01, 0.1, 1, 10, 100, 1000], cv=5)\n",
    "ridge.fit(X_scaled, y)\n",
    "\n",
    "ridge_results = pd.DataFrame({\n",
    "    'feature': X_scaled.columns,\n",
    "    'coefficient': ridge.coef_,\n",
    "    'abs_coefficient': np.abs(ridge.coef_)\n",
    "}).sort_values('abs_coefficient', ascending=False).reset_index(drop=True)\n",
    "\n",
    "ridge_results['rank_ridge'] = range(1, len(ridge_results) + 1)\n",
    "\n",
    "print(f\"\\nRidge alpha: {ridge.alpha_:.4f}\")\n",
    "print(\"\\nResults:\")\n",
    "print(ridge_results[['rank_ridge', 'feature', 'coefficient', 'abs_coefficient']].to_string(index=False))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['forestgreen' if c > 0 else 'crimson' for c in ridge_results['coefficient']]\n",
    "plt.barh(ridge_results['feature'][::-1], ridge_results['abs_coefficient'][::-1], color=colors[::-1])\n",
    "plt.xlabel('|Coefficient|')\n",
    "plt.title('Ridge Coefficients (Green=Positive, Red=Negative)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b871d85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 13: METHOD 5 - PLS WITH VIP\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"METHOD 5: PLS WITH VIP SCORES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Optimal components\n",
    "max_comp = min(5, X_scaled.shape[1], len(X_scaled) - 1)\n",
    "cv_scores = []\n",
    "for n_comp in range(1, max_comp + 1):\n",
    "    pls_temp = PLSRegression(n_components=n_comp)\n",
    "    scores = cross_val_score(pls_temp, X_scaled, y, cv=5, scoring='r2')\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "optimal_comp = np.argmax(cv_scores) + 1\n",
    "print(f\"Optimal components: {optimal_comp}\")\n",
    "\n",
    "# Fit PLS\n",
    "pls = PLSRegression(n_components=optimal_comp)\n",
    "pls.fit(X_scaled, y)\n",
    "\n",
    "# Calculate VIP\n",
    "def calculate_vip(model):\n",
    "    t = model.x_scores_\n",
    "    w = model.x_weights_\n",
    "    q = model.y_loadings_\n",
    "    n_features, n_components = w.shape\n",
    "    ss_y = np.sum(t ** 2, axis=0) * q.flatten() ** 2\n",
    "    ss_y_total = np.sum(ss_y)\n",
    "    vip = np.zeros(n_features)\n",
    "    for i in range(n_features):\n",
    "        weight_sum = 0\n",
    "        for j in range(n_components):\n",
    "            weight_sum += (w[i, j] ** 2) * ss_y[j] / (np.sum(w[:, j] ** 2))\n",
    "        vip[i] = np.sqrt(n_features * weight_sum / ss_y_total)\n",
    "    return vip\n",
    "\n",
    "vip_scores = calculate_vip(pls)\n",
    "\n",
    "pls_results = pd.DataFrame({\n",
    "    'feature': X_scaled.columns,\n",
    "    'VIP': vip_scores,\n",
    "    'coefficient': pls.coef_.flatten()\n",
    "}).sort_values('VIP', ascending=False).reset_index(drop=True)\n",
    "\n",
    "pls_results['rank_pls'] = range(1, len(pls_results) + 1)\n",
    "\n",
    "def vip_category(v):\n",
    "    if v >= VIP_IMPORTANT: return 'Important'\n",
    "    elif v >= VIP_MODERATE: return 'Moderate'\n",
    "    else: return 'Less Important'\n",
    "\n",
    "pls_results['VIP_category'] = pls_results['VIP'].apply(vip_category)\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "print(pls_results[['rank_pls', 'feature', 'VIP', 'VIP_category']].to_string(index=False))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['darkgreen' if v >= VIP_IMPORTANT else 'orange' if v >= VIP_MODERATE else 'lightcoral' \n",
    "          for v in pls_results['VIP']]\n",
    "plt.barh(pls_results['feature'][::-1], pls_results['VIP'][::-1], color=colors[::-1])\n",
    "plt.axvline(x=VIP_IMPORTANT, color='green', linestyle='--', label=f'Important ({VIP_IMPORTANT})')\n",
    "plt.axvline(x=VIP_MODERATE, color='orange', linestyle='--', label=f'Moderate ({VIP_MODERATE})')\n",
    "plt.xlabel('VIP Score')\n",
    "plt.title('PLS Variable Importance in Projection')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b07baff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 14: CHECK FOR INTERACTIONS (NEW - IMPORTANT FOR BO)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"INTERACTION SCREENING (Important for BO)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "Why this matters:\n",
    "  If Feature A × Feature B interaction is strong, you need BOTH in BO\n",
    "  even if individual effects are weak.\n",
    "  \n",
    "Method: Check if effect of A depends on level of B\n",
    "\"\"\")\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "# Get top features to check interactions\n",
    "top_n_for_interactions = 6\n",
    "top_features_for_int = corr_results.head(top_n_for_interactions)['feature'].tolist()\n",
    "\n",
    "interaction_results = []\n",
    "\n",
    "for f1, f2 in combinations(top_features_for_int, 2):\n",
    "    # Split by median of f2\n",
    "    median_f2 = X[f2].median()\n",
    "    low_f2 = X[f2] <= median_f2\n",
    "    high_f2 = X[f2] > median_f2\n",
    "    \n",
    "    # Correlation of f1 with y in each group\n",
    "    corr_low = X.loc[low_f2, f1].corr(y[low_f2]) if low_f2.sum() > 3 else np.nan\n",
    "    corr_high = X.loc[high_f2, f1].corr(y[high_f2]) if high_f2.sum() > 3 else np.nan\n",
    "    \n",
    "    if not np.isnan(corr_low) and not np.isnan(corr_high):\n",
    "        interaction_strength = abs(corr_high - corr_low)\n",
    "        interaction_results.append({\n",
    "            'interaction': f'{f1} × {f2}',\n",
    "            'feature_1': f1,\n",
    "            'feature_2': f2,\n",
    "            'corr_at_low_f2': corr_low,\n",
    "            'corr_at_high_f2': corr_high,\n",
    "            'interaction_strength': interaction_strength\n",
    "        })\n",
    "\n",
    "interaction_df = pd.DataFrame(interaction_results).sort_values('interaction_strength', ascending=False)\n",
    "\n",
    "print(\"\\nInteraction Screening (Top features):\")\n",
    "print(interaction_df[['interaction', 'corr_at_low_f2', 'corr_at_high_f2', 'interaction_strength']].head(10).to_string(index=False))\n",
    "\n",
    "# Flag strong interactions\n",
    "INTERACTION_THRESHOLD = 0.3\n",
    "strong_interactions = interaction_df[interaction_df['interaction_strength'] > INTERACTION_THRESHOLD]\n",
    "\n",
    "if len(strong_interactions) > 0:\n",
    "    print(f\"\\n⚠️  Potential interactions detected (strength > {INTERACTION_THRESHOLD}):\")\n",
    "    for _, row in strong_interactions.iterrows():\n",
    "        print(f\"   {row['interaction']}: strength = {row['interaction_strength']:.3f}\")\n",
    "        print(f\"      → Include BOTH {row['feature_1']} and {row['feature_2']} in BO\")\n",
    "else:\n",
    "    print(f\"\\n✓ No strong interactions detected (threshold: {INTERACTION_THRESHOLD})\")\n",
    "\n",
    "# Store for later\n",
    "features_with_interactions = set()\n",
    "for _, row in strong_interactions.iterrows():\n",
    "    features_with_interactions.add(row['feature_1'])\n",
    "    features_with_interactions.add(row['feature_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95264be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 15: MULTICOLLINEARITY CHECK\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MULTICOLLINEARITY CHECK\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "feature_corr = X.corr()\n",
    "\n",
    "high_corr_pairs = []\n",
    "for i in range(len(feature_corr.columns)):\n",
    "    for j in range(i+1, len(feature_corr.columns)):\n",
    "        r = feature_corr.iloc[i, j]\n",
    "        if abs(r) > MULTICOLLINEARITY_THRESHOLD:\n",
    "            high_corr_pairs.append({\n",
    "                'feature_1': feature_corr.columns[i],\n",
    "                'feature_2': feature_corr.columns[j],\n",
    "                'correlation': r\n",
    "            })\n",
    "\n",
    "if high_corr_pairs:\n",
    "    print(f\"\\n⚠️  Highly correlated pairs (|r| > {MULTICOLLINEARITY_THRESHOLD}):\")\n",
    "    for pair in high_corr_pairs:\n",
    "        print(f\"   {pair['feature_1']} ↔ {pair['feature_2']}: r = {pair['correlation']:.3f}\")\n",
    "        print(f\"   → Keep only ONE in BO (saves dimensions)\")\n",
    "else:\n",
    "    print(\"\\n✓ No highly correlated feature pairs\")\n",
    "\n",
    "# Heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "mask = np.triu(np.ones_like(feature_corr, dtype=bool), k=1)\n",
    "sns.heatmap(feature_corr, annot=True, cmap='RdBu_r', center=0, fmt='.2f', mask=mask)\n",
    "plt.title('Feature-Feature Correlations')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16db7eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 16: CONSENSUS RANKING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CONSENSUS RANKING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Merge all rankings\n",
    "consensus = corr_results[['feature', 'rank_corr', 'correlation', 'direction']].copy()\n",
    "consensus = consensus.merge(mi_results[['feature', 'rank_mi']], on='feature')\n",
    "consensus = consensus.merge(lasso_results[['feature', 'rank_lasso', 'selected']], on='feature')\n",
    "consensus = consensus.merge(ridge_results[['feature', 'rank_ridge']], on='feature')\n",
    "consensus = consensus.merge(pls_results[['feature', 'rank_pls', 'VIP']], on='feature')\n",
    "\n",
    "# Average rank\n",
    "rank_cols = ['rank_corr', 'rank_mi', 'rank_lasso', 'rank_ridge', 'rank_pls']\n",
    "consensus['avg_rank'] = consensus[rank_cols].mean(axis=1)\n",
    "consensus = consensus.sort_values('avg_rank').reset_index(drop=True)\n",
    "consensus['final_rank'] = range(1, len(consensus) + 1)\n",
    "\n",
    "# Method agreement\n",
    "def count_top_k(row, k=3):\n",
    "    return sum(1 for col in rank_cols if row[col] <= k)\n",
    "\n",
    "consensus['methods_in_top3'] = consensus.apply(lambda r: count_top_k(r, 3), axis=1)\n",
    "\n",
    "# Add interaction flag\n",
    "consensus['has_interaction'] = consensus['feature'].isin(features_with_interactions)\n",
    "\n",
    "print(\"\\nConsensus Ranking:\")\n",
    "display_cols = ['final_rank', 'feature', 'correlation', 'VIP', 'selected', \n",
    "                'avg_rank', 'methods_in_top3', 'has_interaction']\n",
    "print(consensus[display_cols].to_string(index=False))\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Heatmap\n",
    "ax1 = axes[0]\n",
    "heatmap_data = consensus.set_index('feature')[rank_cols]\n",
    "heatmap_data.columns = ['Corr', 'MI', 'Lasso', 'Ridge', 'PLS']\n",
    "sns.heatmap(heatmap_data, annot=True, fmt='.0f', cmap='RdYlGn_r', ax=ax1)\n",
    "ax1.set_title('Rankings Across Methods\\n(Lower = More Important)')\n",
    "\n",
    "# Agreement\n",
    "ax2 = axes[1]\n",
    "colors = ['darkgreen' if a >= 4 else 'orange' if a >= 3 else 'lightcoral' \n",
    "          for a in consensus['methods_in_top3']]\n",
    "ax2.barh(consensus['feature'][::-1], consensus['methods_in_top3'][::-1], color=colors[::-1])\n",
    "ax2.axvline(x=3, color='orange', linestyle='--', linewidth=2)\n",
    "ax2.set_xlabel('Methods Ranking in Top 3')\n",
    "ax2.set_title('Method Agreement')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383ad139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 17: AUTOMATIC FEATURE RECOMMENDATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"AUTOMATIC FEATURE RECOMMENDATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Score each feature\n",
    "def score_feature(row):\n",
    "    score = 0\n",
    "    \n",
    "    # Correlation strength\n",
    "    if abs(row['correlation']) >= CORRELATION_STRONG:\n",
    "        score += 3\n",
    "    elif abs(row['correlation']) >= CORRELATION_MODERATE:\n",
    "        score += 2\n",
    "    \n",
    "    # VIP\n",
    "    if row['VIP'] >= VIP_IMPORTANT:\n",
    "        score += 3\n",
    "    elif row['VIP'] >= VIP_MODERATE:\n",
    "        score += 2\n",
    "    \n",
    "    # Lasso selected\n",
    "    if row['selected']:\n",
    "        score += 2\n",
    "    \n",
    "    # Method agreement\n",
    "    score += row['methods_in_top3']\n",
    "    \n",
    "    # Interaction bonus (important for BO)\n",
    "    if row['has_interaction']:\n",
    "        score += 2\n",
    "    \n",
    "    return score\n",
    "\n",
    "consensus['selection_score'] = consensus.apply(score_feature, axis=1)\n",
    "consensus = consensus.sort_values('selection_score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nFeature Scoring:\")\n",
    "print(consensus[['feature', 'correlation', 'VIP', 'selected', 'has_interaction', 'selection_score']].to_string(index=False))\n",
    "\n",
    "# Recommend features\n",
    "recommended_features = []\n",
    "reasons = {}\n",
    "\n",
    "for _, row in consensus.iterrows():\n",
    "    include = False\n",
    "    reason = []\n",
    "    \n",
    "    if row['selection_score'] >= 6:\n",
    "        include = True\n",
    "        reason.append(f\"High score ({row['selection_score']})\")\n",
    "    \n",
    "    if abs(row['correlation']) >= CORRELATION_STRONG:\n",
    "        include = True\n",
    "        reason.append(f\"Strong correlation ({row['correlation']:.2f})\")\n",
    "    \n",
    "    if row['VIP'] >= VIP_IMPORTANT:\n",
    "        include = True\n",
    "        reason.append(f\"VIP ≥ {VIP_IMPORTANT}\")\n",
    "    \n",
    "    if row['has_interaction'] and row['selection_score'] >= 4:\n",
    "        include = True\n",
    "        reason.append(\"Part of interaction\")\n",
    "    \n",
    "    if include and len(recommended_features) < MAX_FEATURES:\n",
    "        recommended_features.append(row['feature'])\n",
    "        reasons[row['feature']] = '; '.join(reason)\n",
    "\n",
    "# Ensure minimum features\n",
    "if len(recommended_features) < MIN_FEATURES:\n",
    "    for _, row in consensus.iterrows():\n",
    "        if row['feature'] not in recommended_features:\n",
    "            recommended_features.append(row['feature'])\n",
    "            reasons[row['feature']] = f\"Added to meet minimum ({MIN_FEATURES})\"\n",
    "        if len(recommended_features) >= MIN_FEATURES:\n",
    "            break\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"RECOMMENDED FEATURES FOR BO ({len(recommended_features)})\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "for feat in recommended_features:\n",
    "    row = consensus[consensus['feature'] == feat].iloc[0]\n",
    "    print(f\"\\n  ✓ {feat}\")\n",
    "    print(f\"      Correlation: {row['correlation']:.3f} ({row['direction']})\")\n",
    "    print(f\"      VIP: {row['VIP']:.2f}\")\n",
    "    print(f\"      Reason: {reasons[feat]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1056997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 18: MANUAL ADJUSTMENT (YOUR INPUT)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MANUAL FEATURE SELECTION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "Review the recommendations above and adjust if needed.\n",
    "\n",
    "Consider:\n",
    "  1. Domain knowledge - does this feature make scientific sense?\n",
    "  2. Controllability - can you actually vary this in experiments?\n",
    "  3. Cost - is this feature expensive to change?\n",
    "  4. Interactions - if two features interact, keep both\n",
    "\"\"\")\n",
    "\n",
    "# Option 1: Accept recommendations\n",
    "selected_features = recommended_features.copy()\n",
    "\n",
    "# Option 2: Manual override - uncomment and modify\n",
    "# selected_features = [\n",
    "#     'temperature_C',\n",
    "#     'catalyst_loading_g',\n",
    "#     'pH',\n",
    "#     'reaction_time_min',\n",
    "# ]\n",
    "\n",
    "print(f\"\\nSelected features ({len(selected_features)}): {selected_features}\")\n",
    "\n",
    "# Validate selection\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"SELECTION VALIDATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check multicollinearity in selected set\n",
    "selected_corr = X[selected_features].corr()\n",
    "for i in range(len(selected_features)):\n",
    "    for j in range(i+1, len(selected_features)):\n",
    "        r = selected_corr.iloc[i, j]\n",
    "        if abs(r) > MULTICOLLINEARITY_THRESHOLD:\n",
    "            print(f\"⚠️  {selected_features[i]} ↔ {selected_features[j]}: r={r:.2f}\")\n",
    "            print(f\"   Consider removing one\")\n",
    "\n",
    "# Check for broken interactions\n",
    "for _, row in strong_interactions.iterrows():\n",
    "    f1, f2 = row['feature_1'], row['feature_2']\n",
    "    if (f1 in selected_features) != (f2 in selected_features):\n",
    "        print(f\"⚠️  Interaction {f1} × {f2} is broken\")\n",
    "        print(f\"   {f1} is {'selected' if f1 in selected_features else 'NOT selected'}\")\n",
    "        print(f\"   {f2} is {'selected' if f2 in selected_features else 'NOT selected'}\")\n",
    "\n",
    "if len(selected_features) > MAX_FEATURES:\n",
    "    print(f\"⚠️  {len(selected_features)} features selected, but target is {TARGET_FEATURES_FOR_BO}\")\n",
    "    print(f\"   BO may need more experiments to converge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1c5ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 19: VALIDATE WITH LOO-CV\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"VALIDATION: Leave-One-Out CV\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "X_selected = X_scaled[selected_features]\n",
    "\n",
    "# LOO-CV\n",
    "loo = LeaveOneOut()\n",
    "loo_predictions = []\n",
    "loo_actuals = []\n",
    "\n",
    "for train_idx, test_idx in loo.split(X_selected):\n",
    "    ridge_temp = RidgeCV(alphas=[0.1, 1, 10, 100], cv=3)\n",
    "    ridge_temp.fit(X_selected.iloc[train_idx], y.iloc[train_idx])\n",
    "    pred = ridge_temp.predict(X_selected.iloc[test_idx])[0]\n",
    "    loo_predictions.append(pred)\n",
    "    loo_actuals.append(y.iloc[test_idx].values[0])\n",
    "\n",
    "loo_predictions = np.array(loo_predictions)\n",
    "loo_actuals = np.array(loo_actuals)\n",
    "\n",
    "loo_r2 = r2_score(loo_actuals, loo_predictions)\n",
    "loo_rmse = np.sqrt(np.mean((loo_actuals - loo_predictions)**2))\n",
    "\n",
    "print(f\"\\nLOO-CV Results ({len(selected_features)} features):\")\n",
    "print(f\"  R²:   {loo_r2:.4f}\")\n",
    "print(f\"  RMSE: {loo_rmse:.4f}\")\n",
    "\n",
    "if loo_r2 > 0.5:\n",
    "    print(\"\\n✓ Good signal - features are predictive\")\n",
    "elif loo_r2 > 0.2:\n",
    "    print(\"\\n⚠️ Moderate signal - GP in BO can likely capture patterns\")\n",
    "else:\n",
    "    print(\"\\n⚠️ Weak linear signal - relationships may be non-linear\")\n",
    "    print(\"   GP surrogate in BO can handle this\")\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "ax1 = axes[0]\n",
    "ax1.scatter(loo_actuals, loo_predictions, alpha=0.6, edgecolors='black')\n",
    "min_val, max_val = min(loo_actuals.min(), loo_predictions.min()), max(loo_actuals.max(), loo_predictions.max())\n",
    "ax1.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)\n",
    "ax1.set_xlabel('Actual')\n",
    "ax1.set_ylabel('Predicted')\n",
    "ax1.set_title(f'LOO-CV: R² = {loo_r2:.4f}')\n",
    "\n",
    "ax2 = axes[1]\n",
    "residuals = loo_actuals - loo_predictions\n",
    "ax2.hist(residuals, bins=12, edgecolor='black', alpha=0.7)\n",
    "ax2.axvline(x=0, color='red', linestyle='--')\n",
    "ax2.set_xlabel('Residual')\n",
    "ax2.set_title('Residual Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a679fb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 20: DEFINE BO SEARCH SPACE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"BAYESIAN OPTIMIZATION SEARCH SPACE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get bounds from original data\n",
    "bo_bounds = []\n",
    "for feat in selected_features:\n",
    "    feat_min = original_X[feat].min()\n",
    "    feat_max = original_X[feat].max()\n",
    "    feat_range = feat_max - feat_min\n",
    "    \n",
    "    # Option: Extend bounds slightly beyond observed range\n",
    "    extend_pct = 0.1  # 10% extension\n",
    "    extended_min = feat_min - extend_pct * feat_range\n",
    "    extended_max = feat_max + extend_pct * feat_range\n",
    "    \n",
    "    bo_bounds.append({\n",
    "        'feature': feat,\n",
    "        'observed_min': feat_min,\n",
    "        'observed_max': feat_max,\n",
    "        'bo_min': extended_min,\n",
    "        'bo_max': extended_max,\n",
    "        'type': 'continuous'\n",
    "    })\n",
    "\n",
    "bo_bounds_df = pd.DataFrame(bo_bounds)\n",
    "\n",
    "print(\"\\nSearch Space Bounds:\")\n",
    "print(bo_bounds_df.to_string(index=False))\n",
    "\n",
    "print(f\"\"\"\n",
    "\\nNotes:\n",
    "  - BO bounds extended 10% beyond observed range\n",
    "  - Adjust if physical constraints exist (e.g., temperature > 0)\n",
    "  - Mark categorical features if any\n",
    "\"\"\")\n",
    "\n",
    "# Effect directions (helps with initial search)\n",
    "print(\"\\nEffect Directions (for initial BO region):\")\n",
    "for feat in selected_features:\n",
    "    row = consensus[consensus['feature'] == feat].iloc[0]\n",
    "    direction = row['direction']\n",
    "    if MAXIMIZE_RESPONSE:\n",
    "        suggest = \"HIGH\" if direction == 'Positive' else \"LOW\"\n",
    "    else:\n",
    "        suggest = \"LOW\" if direction == 'Positive' else \"HIGH\"\n",
    "    print(f\"  {feat}: {direction} effect → Start search at {suggest} values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a437bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 21: PREPARE INITIAL DATA FOR BO\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"INITIAL DATA FOR BAYESIAN OPTIMIZATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create initial dataset with selected features + response\n",
    "bo_initial_data = original_X[selected_features].copy()\n",
    "bo_initial_data[RESPONSE_COLUMN] = y.values\n",
    "\n",
    "print(f\"\\nInitial dataset shape: {bo_initial_data.shape}\")\n",
    "print(f\"  - {len(bo_initial_data)} experiments\")\n",
    "print(f\"  - {len(selected_features)} features\")\n",
    "print(f\"  - 1 response ({RESPONSE_COLUMN})\")\n",
    "\n",
    "print(f\"\\nResponse statistics:\")\n",
    "print(f\"  Min:  {y.min():.2f}\")\n",
    "print(f\"  Max:  {y.max():.2f}\")\n",
    "print(f\"  Mean: {y.mean():.2f}\")\n",
    "print(f\"  Std:  {y.std():.2f}\")\n",
    "\n",
    "# Best point so far\n",
    "if MAXIMIZE_RESPONSE:\n",
    "    best_idx = y.idxmax()\n",
    "    best_response = y.max()\n",
    "else:\n",
    "    best_idx = y.idxmin()\n",
    "    best_response = y.min()\n",
    "\n",
    "print(f\"\\nBest observed point:\")\n",
    "print(f\"  Response: {best_response:.2f}\")\n",
    "print(f\"  Conditions:\")\n",
    "for feat in selected_features:\n",
    "    print(f\"    {feat}: {original_X.loc[best_idx, feat]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ec7d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 22: EXPORT FOR BO\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EXPORT FILES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. Selected features\n",
    "with open('bo_selected_features.txt', 'w') as f:\n",
    "    f.write('\\n'.join(selected_features))\n",
    "print(\"✓ bo_selected_features.txt\")\n",
    "\n",
    "# 2. Search space bounds\n",
    "bo_bounds_df.to_csv('bo_search_bounds.csv', index=False)\n",
    "print(\"✓ bo_search_bounds.csv\")\n",
    "\n",
    "# 3. Initial data\n",
    "bo_initial_data.to_csv('bo_initial_data.csv', index=False)\n",
    "print(\"✓ bo_initial_data.csv\")\n",
    "\n",
    "# 4. Feature screening evidence\n",
    "consensus.to_csv('feature_screening_results.csv', index=False)\n",
    "print(\"✓ feature_screening_results.csv\")\n",
    "\n",
    "# 5. Interaction information\n",
    "if len(strong_interactions) > 0:\n",
    "    strong_interactions.to_csv('detected_interactions.csv', index=False)\n",
    "    print(\"✓ detected_interactions.csv\")\n",
    "\n",
    "# 6. BO configuration summary\n",
    "config_summary = {\n",
    "    'n_features': len(selected_features),\n",
    "    'n_initial_points': len(bo_initial_data),\n",
    "    'response_column': RESPONSE_COLUMN,\n",
    "    'maximize': MAXIMIZE_RESPONSE,\n",
    "    'best_observed': best_response,\n",
    "    'loo_cv_r2': loo_r2,\n",
    "}\n",
    "\n",
    "with open('bo_config.txt', 'w') as f:\n",
    "    for key, value in config_summary.items():\n",
    "        f.write(f\"{key}: {value}\\n\")\n",
    "print(\"✓ bo_config.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ad1a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 23: FINAL SUMMARY & BO RECOMMENDATIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FEATURE SCREENING SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\"\"\n",
    "DATA:\n",
    "  Initial experiments: {len(original_X)}\n",
    "  Features screened: {len(feature_cols)}\n",
    "  \n",
    "SCREENING METHODS USED:\n",
    "  1. Pearson Correlation\n",
    "  2. Mutual Information\n",
    "  3. Lasso (automatic selection)\n",
    "  4. Ridge (stable coefficients)\n",
    "  5. PLS with VIP scores\n",
    "  \n",
    "SELECTED FEATURES FOR BO ({len(selected_features)}):\n",
    "\"\"\")\n",
    "\n",
    "for i, feat in enumerate(selected_features, 1):\n",
    "    row = consensus[consensus['feature'] == feat].iloc[0]\n",
    "    bounds = bo_bounds_df[bo_bounds_df['feature'] == feat].iloc[0]\n",
    "    print(f\"  {i}. {feat}\")\n",
    "    print(f\"       Correlation: {row['correlation']:.3f}, VIP: {row['VIP']:.2f}\")\n",
    "    print(f\"       BO bounds: [{bounds['bo_min']:.2f}, {bounds['bo_max']:.2f}]\")\n",
    "\n",
    "if len(strong_interactions) > 0:\n",
    "    print(f\"\\nDETECTED INTERACTIONS:\")\n",
    "    for _, row in strong_interactions.iterrows():\n",
    "        print(f\"  - {row['interaction']}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "VALIDATION:\n",
    "  LOO-CV R²: {loo_r2:.4f}\n",
    "  LOO-CV RMSE: {loo_rmse:.4f}\n",
    "  \n",
    "BAYESIAN OPTIMIZATION RECOMMENDATIONS:\n",
    "  1. Use GP surrogate with Matérn kernel\n",
    "  2. Acquisition function: Expected Improvement (EI) or UCB\n",
    "  3. Initial points: {len(bo_initial_data)} (your screening data)\n",
    "  4. Expected iterations: {20 * len(selected_features)}-{40 * len(selected_features)} for convergence\n",
    "  \n",
    "  Estimated total experiments:\n",
    "    Screening: {len(original_X)} (done)\n",
    "    BO phase:  {20 * len(selected_features)}-{40 * len(selected_features)} (planned)\n",
    "    Total:     {len(original_X) + 20 * len(selected_features)}-{len(original_X) + 40 * len(selected_features)}\n",
    "\n",
    "FILES EXPORTED:\n",
    "  - bo_selected_features.txt\n",
    "  - bo_search_bounds.csv\n",
    "  - bo_initial_data.csv\n",
    "  - feature_screening_results.csv\n",
    "  - bo_config.txt\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"READY FOR BAYESIAN OPTIMIZATION\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
