{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4071da22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 1: IMPORTS\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import RidgeCV, LassoCV\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.model_selection import LeaveOneOut, cross_val_score\n",
    "from sklearn.metrics import r2_score\n",
    "from itertools import combinations\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 1: FEATURE SCREENING FOR BAYESIAN OPTIMIZATION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"✓ Imports complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6da7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 2: LOAD AND CLEAN DATA\n",
    "# =============================================================================\n",
    "\n",
    "# Load Excel file\n",
    "xls = pd.ExcelFile('../data/Encapsys Historic DOE data sets.xlsx', engine='openpyxl')\n",
    "sheet_name = 'Gelatin_PU_DSD'\n",
    "df = pd.read_excel(xls, sheet_name=sheet_name, header=5)\n",
    "\n",
    "print(f\"✓ Loaded sheet: '{sheet_name}'\")\n",
    "print(f\"  Initial shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2567a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 3: SPLIT DATAFRAME AT KEYWORD\n",
    "# =============================================================================\n",
    "\n",
    "split_word = \"PREDICTED OPTIMUM RUNS\"\n",
    "split_index = df.index[df['Run'] == split_word].tolist()\n",
    "\n",
    "if split_index:\n",
    "    split_index = split_index[0]\n",
    "    df_initial = df.iloc[:split_index]\n",
    "    df_optimum = df.iloc[split_index+1:]\n",
    "    print(f\"✓ Split at '{split_word}'\")\n",
    "    print(f\"  df_initial: {len(df_initial)} rows\")\n",
    "    print(f\"  df_optimum: {len(df_optimum)} rows\")\n",
    "else:\n",
    "    print(f\"⚠️ '{split_word}' not found\")\n",
    "    df_initial = df.copy()\n",
    "    df_optimum = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4880924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 4: CLEAN DATAFRAMES\n",
    "# =============================================================================\n",
    "\n",
    "# Clean df_initial\n",
    "df_initial = df_initial.drop(index=0)  # Drop units row\n",
    "df_initial = df_initial.dropna(how='all')\n",
    "df_initial = df_initial.reset_index(drop=True)\n",
    "\n",
    "# Clean df_optimum if exists\n",
    "if len(df_optimum) > 0:\n",
    "    df_optimum = df_optimum.dropna(how='all')\n",
    "    df_optimum = df_optimum.reset_index(drop=True)\n",
    "\n",
    "# Combine\n",
    "if len(df_optimum) > 0:\n",
    "    df_total = pd.concat([df_initial, df_optimum], axis=0, ignore_index=True)\n",
    "    print(f\"✓ Combined: {len(df_total)} total rows\")\n",
    "else:\n",
    "    df_total = df_initial.copy()\n",
    "    print(f\"✓ Using df_initial: {len(df_total)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21642537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 5: DEFINE FEATURE COLUMNS\n",
    "# =============================================================================\n",
    "\n",
    "stop_feature = \"Batch ID\"\n",
    "columns = df_total.columns.tolist()\n",
    "\n",
    "if stop_feature in columns:\n",
    "    feature_list = columns[:columns.index(stop_feature)]\n",
    "    print(f\"✓ Features up to '{stop_feature}':\")\n",
    "else:\n",
    "    feature_list = columns\n",
    "    print(f\"⚠️ '{stop_feature}' not found, using all columns\")\n",
    "\n",
    "for i, feat in enumerate(feature_list, 1):\n",
    "    print(f\"    {i}. {feat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6534ec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 6: CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "RESPONSE_COLUMN = \"Downy Leak\"          # Your response variable\n",
    "TARGET_FEATURES = 4                      # Aim for 3-5 features\n",
    "MAXIMIZE_RESPONSE = False                # True if higher is better\n",
    "\n",
    "# Thresholds\n",
    "CORRELATION_STRONG = 0.4\n",
    "CORRELATION_MODERATE = 0.2\n",
    "VIP_IMPORTANT = 1.0\n",
    "VIP_MODERATE = 0.8\n",
    "MULTICOLLINEARITY_THRESHOLD = 0.7\n",
    "INTERACTION_THRESHOLD = 0.3\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Response: {RESPONSE_COLUMN}\")\n",
    "print(f\"  Target features: {TARGET_FEATURES}\")\n",
    "print(f\"  Optimization: {'Maximize' if MAXIMIZE_RESPONSE else 'Minimize'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a9724e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 7: IDENTIFY FEATURE TYPES (Binary vs Continuous)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FEATURE TYPE CLASSIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "Classification rule:\n",
    "  - Binary: Exactly 2 unique values → encoded as 0/1\n",
    "  - Continuous: 3+ unique values → standardized\n",
    "\"\"\")\n",
    "\n",
    "# Get all numeric columns from feature list\n",
    "numeric_features = df_total[feature_list].select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Remove response if present\n",
    "numeric_features = [c for c in numeric_features if c != RESPONSE_COLUMN]\n",
    "\n",
    "# Classify each feature\n",
    "binary_cols = []\n",
    "continuous_cols = []\n",
    "binary_mappings = {}\n",
    "\n",
    "print(\"Feature Classification:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for col in numeric_features:\n",
    "    n_unique = df_total[col].nunique()\n",
    "    \n",
    "    if n_unique == 2:\n",
    "        # Binary\n",
    "        binary_cols.append(col)\n",
    "        unique_vals = df_total[col].dropna().unique()\n",
    "        mapping = {unique_vals[0]: 0, unique_vals[1]: 1}\n",
    "        df_total[col] = df_total[col].map(mapping)\n",
    "        binary_mappings[col] = mapping\n",
    "        print(f\"  {col}: {n_unique} unique → BINARY\")\n",
    "        print(f\"      Encoding: {unique_vals[0]} → 0, {unique_vals[1]} → 1\")\n",
    "    else:\n",
    "        # Continuous (3+ unique values)\n",
    "        continuous_cols.append(col)\n",
    "        print(f\"  {col}: {n_unique} unique → CONTINUOUS\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"SUMMARY:\")\n",
    "print(f\"  Binary: {len(binary_cols)} features\")\n",
    "print(f\"  Continuous: {len(continuous_cols)} features\")\n",
    "\n",
    "# Store for later use\n",
    "BINARY_FEATURES = binary_cols\n",
    "CONTINUOUS_FEATURES = continuous_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65532441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 8: PREPARE X AND y\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA PREPARATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Combine binary and continuous\n",
    "feature_cols = BINARY_FEATURES + CONTINUOUS_FEATURES\n",
    "\n",
    "# Create X and y\n",
    "X = df_total[feature_cols].copy()\n",
    "y = df_total[RESPONSE_COLUMN].copy()\n",
    "\n",
    "# Drop rows with missing response\n",
    "valid = ~y.isnull()\n",
    "n_dropped = (~valid).sum()\n",
    "X = X[valid].reset_index(drop=True)\n",
    "y = y[valid].reset_index(drop=True)\n",
    "\n",
    "# Store original\n",
    "original_X = X.copy()\n",
    "original_y = y.copy()\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  Samples: {len(X)}\")\n",
    "if n_dropped > 0:\n",
    "    print(f\"  Dropped (missing response): {n_dropped}\")\n",
    "\n",
    "print(f\"\\nFeatures ({len(feature_cols)} total):\")\n",
    "\n",
    "print(f\"\\n  Binary ({len(BINARY_FEATURES)}):\")\n",
    "if BINARY_FEATURES:\n",
    "    for f in BINARY_FEATURES:\n",
    "        print(f\"    • {f}\")\n",
    "else:\n",
    "    print(f\"    (none)\")\n",
    "\n",
    "print(f\"\\n  Continuous ({len(CONTINUOUS_FEATURES)}):\")\n",
    "if CONTINUOUS_FEATURES:\n",
    "    for f in CONTINUOUS_FEATURES:\n",
    "        print(f\"    • {f}\")\n",
    "else:\n",
    "    print(f\"    (none)\")\n",
    "\n",
    "print(f\"\\nResponse: {RESPONSE_COLUMN}\")\n",
    "print(f\"Samples/Features ratio: {len(X)/len(feature_cols):.1f}\")\n",
    "\n",
    "if len(X) / len(feature_cols) < 5:\n",
    "    print(f\"\\n⚠️ Low ratio - using regularized methods only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e89e6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 9: STANDARDIZE FEATURES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STANDARDIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = X.copy()\n",
    "\n",
    "# Standardize continuous only\n",
    "if CONTINUOUS_FEATURES:\n",
    "    X_scaled[CONTINUOUS_FEATURES] = scaler.fit_transform(X[CONTINUOUS_FEATURES])\n",
    "    print(f\"✓ Standardized {len(CONTINUOUS_FEATURES)} continuous features\")\n",
    "\n",
    "# For modeling: scale binary to [-1, +1]\n",
    "X_model = X_scaled.copy()\n",
    "for col in BINARY_FEATURES:\n",
    "    X_model[col] = X_model[col] * 2 - 1\n",
    "\n",
    "print(f\"✓ Binary features preserved as 0/1 (scaled to [-1,+1] for modeling)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dbec36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 10: VISUAL INSPECTION - SCATTER PLOTS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"VISUAL INSPECTION: Feature vs Response\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "n_feat = len(feature_cols)\n",
    "n_cols = min(4, n_feat)\n",
    "n_rows = int(np.ceil(n_feat / n_cols))\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(4*n_cols, 3.5*n_rows))\n",
    "axes = axes.flatten() if n_feat > 1 else [axes]\n",
    "\n",
    "for i, col in enumerate(feature_cols):\n",
    "    ax = axes[i]\n",
    "    corr = X[col].corr(y)\n",
    "    \n",
    "    if col in BINARY_FEATURES:\n",
    "        # Box plot for binary\n",
    "        for val in [0, 1]:\n",
    "            data = y[X[col] == val]\n",
    "            ax.boxplot([data], positions=[val], widths=0.6)\n",
    "        ax.set_xticks([0, 1])\n",
    "        ax.set_xticklabels(['0', '1'])\n",
    "        ax.set_xlabel(f'{col} (binary)')\n",
    "    else:\n",
    "        # Scatter for continuous\n",
    "        ax.scatter(X[col], y, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "        # Trend line\n",
    "        z = np.polyfit(X[col], y, 1)\n",
    "        p = np.poly1d(z)\n",
    "        x_line = np.linspace(X[col].min(), X[col].max(), 100)\n",
    "        ax.plot(x_line, p(x_line), 'r--', linewidth=2)\n",
    "        ax.set_xlabel(col)\n",
    "    \n",
    "    ax.set_ylabel(RESPONSE_COLUMN)\n",
    "    ax.set_title(f'r = {corr:.3f}', fontsize=10)\n",
    "\n",
    "# Hide empty subplots\n",
    "for j in range(i+1, len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "\n",
    "plt.suptitle(f'Features vs {RESPONSE_COLUMN}', fontsize=12, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Look for: linear trends, non-linear patterns, outliers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec13b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 11: RESPONSE DISTRIBUTION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RESPONSE DISTRIBUTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Histogram\n",
    "ax1 = axes[0]\n",
    "ax1.hist(y, bins=15, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "ax1.axvline(y.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {y.mean():.2f}')\n",
    "ax1.axvline(y.median(), color='orange', linestyle='--', linewidth=2, label=f'Median: {y.median():.2f}')\n",
    "ax1.set_xlabel(RESPONSE_COLUMN)\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Response Distribution')\n",
    "ax1.legend()\n",
    "\n",
    "# Box plot\n",
    "ax2 = axes[1]\n",
    "ax2.boxplot(y, vert=True)\n",
    "ax2.set_ylabel(RESPONSE_COLUMN)\n",
    "ax2.set_title('Response Box Plot')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Response statistics:\")\n",
    "print(f\"  Min:    {y.min():.4f}\")\n",
    "print(f\"  Max:    {y.max():.4f}\")\n",
    "print(f\"  Mean:   {y.mean():.4f}\")\n",
    "print(f\"  Median: {y.median():.4f}\")\n",
    "print(f\"  Std:    {y.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3fedbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 12: METHOD 1 - CORRELATION ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"METHOD 1: PEARSON CORRELATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "correlations = X.corrwith(y)\n",
    "\n",
    "corr_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'correlation': correlations.values,\n",
    "    'abs_corr': np.abs(correlations.values),\n",
    "    'type': ['binary' if f in BINARY_FEATURES else 'continuous' for f in feature_cols],\n",
    "    'direction': ['Positive' if c > 0 else 'Negative' for c in correlations.values]\n",
    "}).sort_values('abs_corr', ascending=False).reset_index(drop=True)\n",
    "\n",
    "corr_df['rank_corr'] = range(1, len(corr_df) + 1)\n",
    "\n",
    "# Categorize strength\n",
    "def corr_strength(r):\n",
    "    if abs(r) >= CORRELATION_STRONG: return 'Strong'\n",
    "    elif abs(r) >= CORRELATION_MODERATE: return 'Moderate'\n",
    "    else: return 'Weak'\n",
    "\n",
    "corr_df['strength'] = corr_df['correlation'].apply(corr_strength)\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "print(corr_df[['rank_corr', 'feature', 'correlation', 'direction', 'strength', 'type']].to_string(index=False))\n",
    "\n",
    "# Binary interpretation\n",
    "if BINARY_FEATURES:\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Binary Feature Effects:\")\n",
    "    for _, row in corr_df[corr_df['type'] == 'binary'].iterrows():\n",
    "        effect = \"INCREASES\" if row['correlation'] > 0 else \"DECREASES\"\n",
    "        print(f\"  {row['feature']}: When = 1, response {effect} (r = {row['correlation']:.3f})\")\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar plot\n",
    "ax1 = axes[0]\n",
    "colors = []\n",
    "for _, row in corr_df.iterrows():\n",
    "    if row['correlation'] > 0:\n",
    "        colors.append('forestgreen' if row['type'] == 'continuous' else 'steelblue')\n",
    "    else:\n",
    "        colors.append('crimson' if row['type'] == 'continuous' else 'darkorange')\n",
    "\n",
    "ax1.barh(corr_df['feature'][::-1], corr_df['abs_corr'][::-1], color=colors[::-1])\n",
    "ax1.axvline(x=CORRELATION_STRONG, color='green', linestyle='--', linewidth=2, label=f'Strong ({CORRELATION_STRONG})')\n",
    "ax1.axvline(x=CORRELATION_MODERATE, color='orange', linestyle='--', linewidth=1.5, label=f'Moderate ({CORRELATION_MODERATE})')\n",
    "ax1.set_xlabel('|Correlation|')\n",
    "ax1.set_title('Feature-Response Correlation\\n(Green/Blue=Positive, Red/Orange=Negative)')\n",
    "ax1.legend(loc='lower right')\n",
    "\n",
    "# Signed correlation\n",
    "ax2 = axes[1]\n",
    "colors_signed = ['forestgreen' if c > 0 else 'crimson' for c in corr_df['correlation']]\n",
    "ax2.barh(corr_df['feature'][::-1], corr_df['correlation'][::-1], color=colors_signed[::-1])\n",
    "ax2.axvline(x=0, color='black', linewidth=1)\n",
    "ax2.set_xlabel('Correlation (with sign)')\n",
    "ax2.set_title('Direction of Effect\\n(Green=Positive, Red=Negative)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507d3cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 13: METHOD 2 - LASSO (Automatic Selection)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"METHOD 2: LASSO REGRESSION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "lasso = LassoCV(cv=5, max_iter=10000, random_state=RANDOM_STATE)\n",
    "lasso.fit(X_model, y)\n",
    "\n",
    "lasso_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'coefficient': lasso.coef_,\n",
    "    'abs_coef': np.abs(lasso.coef_),\n",
    "    'selected': lasso.coef_ != 0,\n",
    "    'type': ['binary' if f in BINARY_FEATURES else 'continuous' for f in feature_cols]\n",
    "}).sort_values('abs_coef', ascending=False).reset_index(drop=True)\n",
    "\n",
    "lasso_df['rank_lasso'] = range(1, len(lasso_df) + 1)\n",
    "\n",
    "print(f\"\\nLasso alpha: {lasso.alpha_:.4f}\")\n",
    "print(f\"Features selected: {lasso_df['selected'].sum()}/{len(lasso_df)}\")\n",
    "print(\"\\nResults:\")\n",
    "print(lasso_df[['rank_lasso', 'feature', 'coefficient', 'selected', 'type']].to_string(index=False))\n",
    "\n",
    "selected_by_lasso = lasso_df[lasso_df['selected']]['feature'].tolist()\n",
    "print(f\"\\n✓ Lasso selected: {selected_by_lasso}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['forestgreen' if s else 'lightgray' for s in lasso_df['selected']]\n",
    "plt.barh(lasso_df['feature'][::-1], lasso_df['abs_coef'][::-1], color=colors[::-1])\n",
    "plt.xlabel('|Coefficient|')\n",
    "plt.title('Lasso Coefficients\\n(Green = Selected, Gray = Eliminated)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01090746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 14: METHOD 3 - PLS WITH VIP SCORES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"METHOD 3: PLS (VIP Scores)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Find optimal components\n",
    "max_comp = min(5, len(feature_cols), len(X) - 1)\n",
    "cv_scores = []\n",
    "\n",
    "print(\"Finding optimal components...\")\n",
    "for n in range(1, max_comp + 1):\n",
    "    scores = cross_val_score(PLSRegression(n_components=n), X_model, y, cv=5, scoring='r2')\n",
    "    cv_scores.append(scores.mean())\n",
    "    print(f\"  {n} components: CV R² = {scores.mean():.4f}\")\n",
    "\n",
    "optimal_comp = np.argmax(cv_scores) + 1\n",
    "print(f\"\\n✓ Optimal: {optimal_comp} components (CV R² = {max(cv_scores):.4f})\")\n",
    "\n",
    "# Fit PLS\n",
    "pls = PLSRegression(n_components=optimal_comp)\n",
    "pls.fit(X_model, y)\n",
    "\n",
    "# Calculate VIP\n",
    "def calc_vip(model):\n",
    "    t, w, q = model.x_scores_, model.x_weights_, model.y_loadings_\n",
    "    m, p = w.shape\n",
    "    ss = np.sum(t**2, axis=0) * q.flatten()**2\n",
    "    total_ss = np.sum(ss)\n",
    "    vip = np.zeros(m)\n",
    "    for i in range(m):\n",
    "        weight = sum((w[i,j]**2) * ss[j] / np.sum(w[:,j]**2) for j in range(p))\n",
    "        vip[i] = np.sqrt(m * weight / total_ss)\n",
    "    return vip\n",
    "\n",
    "vip_scores = calc_vip(pls)\n",
    "\n",
    "pls_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'VIP': vip_scores,\n",
    "    'type': ['binary' if f in BINARY_FEATURES else 'continuous' for f in feature_cols]\n",
    "}).sort_values('VIP', ascending=False).reset_index(drop=True)\n",
    "\n",
    "pls_df['rank_pls'] = range(1, len(pls_df) + 1)\n",
    "\n",
    "# Categorize VIP\n",
    "def vip_category(v):\n",
    "    if v >= VIP_IMPORTANT: return 'Important'\n",
    "    elif v >= VIP_MODERATE: return 'Moderate'\n",
    "    else: return 'Less Important'\n",
    "\n",
    "pls_df['category'] = pls_df['VIP'].apply(vip_category)\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "print(pls_df[['rank_pls', 'feature', 'VIP', 'category', 'type']].to_string(index=False))\n",
    "\n",
    "# Summary\n",
    "important = pls_df[pls_df['category'] == 'Important']['feature'].tolist()\n",
    "moderate = pls_df[pls_df['category'] == 'Moderate']['feature'].tolist()\n",
    "print(f\"\\n✓ Important (VIP ≥ {VIP_IMPORTANT}): {important if important else 'None'}\")\n",
    "print(f\"⚠ Moderate (VIP ≥ {VIP_MODERATE}): {moderate if moderate else 'None'}\")\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# VIP scores\n",
    "ax1 = axes[0]\n",
    "colors = ['darkgreen' if v >= VIP_IMPORTANT else 'orange' if v >= VIP_MODERATE else 'lightcoral' \n",
    "          for v in pls_df['VIP']]\n",
    "ax1.barh(pls_df['feature'][::-1], pls_df['VIP'][::-1], color=colors[::-1])\n",
    "ax1.axvline(x=VIP_IMPORTANT, color='green', linestyle='--', linewidth=2, label=f'Important ({VIP_IMPORTANT})')\n",
    "ax1.axvline(x=VIP_MODERATE, color='orange', linestyle='--', linewidth=1.5, label=f'Moderate ({VIP_MODERATE})')\n",
    "ax1.set_xlabel('VIP Score')\n",
    "ax1.set_title('PLS Variable Importance in Projection')\n",
    "ax1.legend()\n",
    "\n",
    "# Component selection\n",
    "ax2 = axes[1]\n",
    "ax2.plot(range(1, max_comp + 1), cv_scores, 'bo-', linewidth=2, markersize=8)\n",
    "ax2.axvline(x=optimal_comp, color='red', linestyle='--', label=f'Optimal = {optimal_comp}')\n",
    "ax2.set_xlabel('Number of Components')\n",
    "ax2.set_ylabel('CV R²')\n",
    "ax2.set_title('PLS Component Selection')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f05c508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 15: INTERACTION SCREENING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"INTERACTION SCREENING\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\"\"\n",
    "Checking if effect of Feature A depends on level of Feature B.\n",
    "\n",
    "Why it matters:\n",
    "  - If A × B interaction is strong, include BOTH in BO\n",
    "  - Even if individual effects are weak, interaction may be important\n",
    "\n",
    "Threshold: {INTERACTION_THRESHOLD}\n",
    "\"\"\")\n",
    "\n",
    "# Use top features for interaction screening\n",
    "top_n = min(6, len(feature_cols))\n",
    "top_features = corr_df.head(top_n)['feature'].tolist()\n",
    "\n",
    "interaction_results = []\n",
    "\n",
    "for f1, f2 in combinations(top_features, 2):\n",
    "    # Split by median of f2\n",
    "    median_f2 = X[f2].median()\n",
    "    low_f2 = X[f2] <= median_f2\n",
    "    high_f2 = X[f2] > median_f2\n",
    "    \n",
    "    # Check we have enough samples in each group\n",
    "    if low_f2.sum() >= 3 and high_f2.sum() >= 3:\n",
    "        # Correlation of f1 with y in each group\n",
    "        corr_low = X.loc[low_f2, f1].corr(y[low_f2])\n",
    "        corr_high = X.loc[high_f2, f1].corr(y[high_f2])\n",
    "        \n",
    "        if not np.isnan(corr_low) and not np.isnan(corr_high):\n",
    "            interaction_strength = abs(corr_high - corr_low)\n",
    "            \n",
    "            interaction_results.append({\n",
    "                'interaction': f'{f1} × {f2}',\n",
    "                'feature_1': f1,\n",
    "                'feature_2': f2,\n",
    "                'corr_low_f2': corr_low,\n",
    "                'corr_high_f2': corr_high,\n",
    "                'strength': interaction_strength,\n",
    "                'interpretation': 'Effect changes' if interaction_strength > INTERACTION_THRESHOLD else 'No interaction'\n",
    "            })\n",
    "\n",
    "if interaction_results:\n",
    "    interaction_df = pd.DataFrame(interaction_results).sort_values('strength', ascending=False)\n",
    "    \n",
    "    print(\"Interaction Analysis Results:\")\n",
    "    print(interaction_df[['interaction', 'corr_low_f2', 'corr_high_f2', 'strength', 'interpretation']].to_string(index=False))\n",
    "    \n",
    "    # Identify strong interactions\n",
    "    strong_interactions = interaction_df[interaction_df['strength'] > INTERACTION_THRESHOLD]\n",
    "    \n",
    "    if len(strong_interactions) > 0:\n",
    "        print(f\"\\n⚠️ POTENTIAL INTERACTIONS DETECTED:\")\n",
    "        features_with_interactions = set()\n",
    "        for _, row in strong_interactions.iterrows():\n",
    "            print(f\"\\n  {row['interaction']}: strength = {row['strength']:.3f}\")\n",
    "            print(f\"    Correlation of {row['feature_1']} with response:\")\n",
    "            print(f\"      When {row['feature_2']} is LOW:  r = {row['corr_low_f2']:.3f}\")\n",
    "            print(f\"      When {row['feature_2']} is HIGH: r = {row['corr_high_f2']:.3f}\")\n",
    "            print(f\"    → Include BOTH features in BO!\")\n",
    "            features_with_interactions.add(row['feature_1'])\n",
    "            features_with_interactions.add(row['feature_2'])\n",
    "        \n",
    "        features_with_interactions = list(features_with_interactions)\n",
    "    else:\n",
    "        print(f\"\\n✓ No strong interactions detected (threshold: {INTERACTION_THRESHOLD})\")\n",
    "        features_with_interactions = []\n",
    "    \n",
    "    # Visualize top interactions\n",
    "    if len(interaction_df) > 0:\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "        colors = ['crimson' if s > INTERACTION_THRESHOLD else 'steelblue' for s in interaction_df['strength']]\n",
    "        ax.barh(interaction_df['interaction'][::-1], interaction_df['strength'][::-1], color=colors[::-1])\n",
    "        ax.axvline(x=INTERACTION_THRESHOLD, color='red', linestyle='--', linewidth=2, \n",
    "                   label=f'Threshold ({INTERACTION_THRESHOLD})')\n",
    "        ax.set_xlabel('Interaction Strength')\n",
    "        ax.set_title('Interaction Screening\\n(Red = Potential Interaction)')\n",
    "        ax.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"✓ No interactions to analyze (insufficient data or features)\")\n",
    "    interaction_df = pd.DataFrame()\n",
    "    strong_interactions = pd.DataFrame()\n",
    "    features_with_interactions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0514df8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 16: MULTICOLLINEARITY CHECK\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MULTICOLLINEARITY CHECK\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\"\"\n",
    "Checking correlations BETWEEN features.\n",
    "\n",
    "Why it matters:\n",
    "  - Highly correlated features contain redundant information\n",
    "  - Including both wastes BO dimensions\n",
    "  - Keep only ONE from each correlated pair\n",
    "\n",
    "Threshold: |r| > {MULTICOLLINEARITY_THRESHOLD}\n",
    "\"\"\")\n",
    "\n",
    "# Feature correlation matrix\n",
    "feature_corr = X.corr()\n",
    "\n",
    "# Find high correlation pairs\n",
    "high_corr_pairs = []\n",
    "for i in range(len(feature_cols)):\n",
    "    for j in range(i+1, len(feature_cols)):\n",
    "        r = feature_corr.iloc[i, j]\n",
    "        if abs(r) > MULTICOLLINEARITY_THRESHOLD:\n",
    "            high_corr_pairs.append({\n",
    "                'feature_1': feature_cols[i],\n",
    "                'feature_2': feature_cols[j],\n",
    "                'correlation': r\n",
    "            })\n",
    "\n",
    "if high_corr_pairs:\n",
    "    print(\"⚠️ HIGHLY CORRELATED PAIRS:\")\n",
    "    for pair in high_corr_pairs:\n",
    "        print(f\"\\n  {pair['feature_1']} ↔ {pair['feature_2']}: r = {pair['correlation']:.3f}\")\n",
    "        print(f\"    → Consider keeping only ONE in BO\")\n",
    "else:\n",
    "    print(\"✓ No highly correlated feature pairs found\")\n",
    "\n",
    "# Heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "mask = np.triu(np.ones_like(feature_corr, dtype=bool), k=0)\n",
    "sns.heatmap(feature_corr, annot=True, cmap='RdBu_r', center=0, fmt='.2f',\n",
    "            mask=mask, square=True, linewidths=0.5)\n",
    "plt.title('Feature-Feature Correlations\\n(Check for multicollinearity)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de3d491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 17: CONSENSUS RANKING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CONSENSUS RANKING (All Methods)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Merge all rankings\n",
    "consensus = corr_df[['feature', 'rank_corr', 'correlation', 'direction', 'type', 'strength']].merge(\n",
    "    lasso_df[['feature', 'rank_lasso', 'selected']], on='feature'\n",
    ").merge(\n",
    "    pls_df[['feature', 'rank_pls', 'VIP', 'category']], on='feature'\n",
    ")\n",
    "\n",
    "# Calculate average rank\n",
    "consensus['avg_rank'] = consensus[['rank_corr', 'rank_lasso', 'rank_pls']].mean(axis=1)\n",
    "consensus = consensus.sort_values('avg_rank').reset_index(drop=True)\n",
    "consensus['final_rank'] = range(1, len(consensus) + 1)\n",
    "\n",
    "# Method agreement (how many methods rank in top 3)\n",
    "def count_top_k(row, k=3):\n",
    "    count = 0\n",
    "    if row['rank_corr'] <= k: count += 1\n",
    "    if row['rank_lasso'] <= k: count += 1\n",
    "    if row['rank_pls'] <= k: count += 1\n",
    "    return count\n",
    "\n",
    "consensus['methods_top3'] = consensus.apply(lambda r: count_top_k(r, 3), axis=1)\n",
    "\n",
    "# Add interaction flag\n",
    "consensus['has_interaction'] = consensus['feature'].isin(features_with_interactions)\n",
    "\n",
    "print(\"\\nConsensus Ranking:\")\n",
    "display_cols = ['final_rank', 'feature', 'type', 'correlation', 'VIP', 'selected', \n",
    "                'avg_rank', 'methods_top3', 'has_interaction']\n",
    "print(consensus[display_cols].to_string(index=False))\n",
    "\n",
    "# Summary by agreement\n",
    "high_agreement = consensus[consensus['methods_top3'] >= 3]['feature'].tolist()\n",
    "moderate_agreement = consensus[consensus['methods_top3'] == 2]['feature'].tolist()\n",
    "\n",
    "print(f\"\\n✓ High agreement (3/3 methods in top 3): {high_agreement if high_agreement else 'None'}\")\n",
    "print(f\"⚠ Moderate agreement (2/3 methods): {moderate_agreement if moderate_agreement else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c26911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 18: CONSENSUS VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CONSENSUS VISUALIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Heatmap of rankings\n",
    "ax1 = axes[0, 0]\n",
    "heatmap_data = consensus.set_index('feature')[['rank_corr', 'rank_lasso', 'rank_pls']]\n",
    "heatmap_data.columns = ['Correlation', 'Lasso', 'PLS']\n",
    "sns.heatmap(heatmap_data, annot=True, fmt='.0f', cmap='RdYlGn_r', ax=ax1,\n",
    "            cbar_kws={'label': 'Rank (lower=better)'})\n",
    "ax1.set_title('Rankings Across Methods')\n",
    "\n",
    "# 2. Method agreement\n",
    "ax2 = axes[0, 1]\n",
    "colors = ['darkgreen' if a >= 3 else 'orange' if a >= 2 else 'lightcoral' \n",
    "          for a in consensus['methods_top3']]\n",
    "ax2.barh(consensus['feature'][::-1], consensus['methods_top3'][::-1], color=colors[::-1])\n",
    "ax2.axvline(x=2, color='orange', linestyle='--', linewidth=2)\n",
    "ax2.set_xlabel('Methods Ranking Feature in Top 3')\n",
    "ax2.set_title('Method Agreement\\n(Green=3/3, Orange=2/3, Red=1/3 or less)')\n",
    "\n",
    "# 3. Average rank\n",
    "ax3 = axes[1, 0]\n",
    "colors = ['steelblue' if t == 'binary' else 'forestgreen' for t in consensus['type']]\n",
    "ax3.barh(consensus['feature'][::-1], consensus['avg_rank'][::-1], color=colors[::-1])\n",
    "ax3.set_xlabel('Average Rank (lower = better)')\n",
    "ax3.set_title('Consensus Ranking\\n(Green=Continuous, Blue=Binary)')\n",
    "ax3.invert_xaxis()\n",
    "\n",
    "# 4. Correlation vs VIP\n",
    "ax4 = axes[1, 1]\n",
    "for _, row in consensus.iterrows():\n",
    "    color = 'steelblue' if row['type'] == 'binary' else 'forestgreen'\n",
    "    marker = 's' if row['has_interaction'] else 'o'\n",
    "    ax4.scatter(abs(row['correlation']), row['VIP'], c=color, s=100, marker=marker, \n",
    "                edgecolors='black', linewidth=0.5)\n",
    "    ax4.annotate(row['feature'], (abs(row['correlation']), row['VIP']), \n",
    "                 fontsize=8, ha='left', va='bottom')\n",
    "\n",
    "ax4.axhline(y=VIP_IMPORTANT, color='green', linestyle='--', alpha=0.7, label=f'VIP={VIP_IMPORTANT}')\n",
    "ax4.axvline(x=CORRELATION_STRONG, color='blue', linestyle='--', alpha=0.7, label=f'|r|={CORRELATION_STRONG}')\n",
    "ax4.set_xlabel('|Correlation|')\n",
    "ax4.set_ylabel('VIP Score')\n",
    "ax4.set_title('Correlation vs VIP\\n(Square=Has Interaction)')\n",
    "ax4.legend(loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21baedde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 19: AUTOMATIC FEATURE RECOMMENDATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FEATURE RECOMMENDATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Score each feature\n",
    "def score_feature(row):\n",
    "    score = 0\n",
    "    \n",
    "    # Correlation\n",
    "    if abs(row['correlation']) >= CORRELATION_STRONG: score += 3\n",
    "    elif abs(row['correlation']) >= CORRELATION_MODERATE: score += 2\n",
    "    \n",
    "    # VIP\n",
    "    if row['VIP'] >= VIP_IMPORTANT: score += 3\n",
    "    elif row['VIP'] >= VIP_MODERATE: score += 2\n",
    "    \n",
    "    # Lasso selected\n",
    "    if row['selected']: score += 2\n",
    "    \n",
    "    # Method agreement\n",
    "    score += row['methods_top3']\n",
    "    \n",
    "    # Interaction bonus\n",
    "    if row['has_interaction']: score += 2\n",
    "    \n",
    "    return score\n",
    "\n",
    "consensus['score'] = consensus.apply(score_feature, axis=1)\n",
    "consensus = consensus.sort_values('score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nFeature Scores:\")\n",
    "print(consensus[['feature', 'type', 'correlation', 'VIP', 'selected', 'has_interaction', 'score']].to_string(index=False))\n",
    "\n",
    "# Generate recommendations\n",
    "recommended = []\n",
    "reasons = {}\n",
    "\n",
    "for _, row in consensus.iterrows():\n",
    "    include = False\n",
    "    reason = []\n",
    "    \n",
    "    if row['score'] >= 6:\n",
    "        include = True\n",
    "        reason.append(f\"High score ({row['score']})\")\n",
    "    \n",
    "    if abs(row['correlation']) >= CORRELATION_STRONG:\n",
    "        include = True\n",
    "        reason.append(f\"Strong correlation\")\n",
    "    \n",
    "    if row['VIP'] >= VIP_IMPORTANT:\n",
    "        include = True\n",
    "        reason.append(f\"VIP ≥ {VIP_IMPORTANT}\")\n",
    "    \n",
    "    if row['has_interaction'] and row['score'] >= 4:\n",
    "        include = True\n",
    "        reason.append(\"Part of interaction\")\n",
    "    \n",
    "    if include and len(recommended) < 6:  # Max 6\n",
    "        recommended.append(row['feature'])\n",
    "        reasons[row['feature']] = ', '.join(reason)\n",
    "\n",
    "# Ensure minimum\n",
    "MIN_FEATURES = 3\n",
    "if len(recommended) < MIN_FEATURES:\n",
    "    for _, row in consensus.iterrows():\n",
    "        if row['feature'] not in recommended:\n",
    "            recommended.append(row['feature'])\n",
    "            reasons[row['feature']] = \"Added to meet minimum\"\n",
    "        if len(recommended) >= MIN_FEATURES:\n",
    "            break\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"RECOMMENDED ({len(recommended)} features):\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "for feat in recommended:\n",
    "    row = consensus[consensus['feature'] == feat].iloc[0]\n",
    "    feat_type = \"[binary]\" if row['type'] == 'binary' else \"[continuous]\"\n",
    "    print(f\"\\n  ✓ {feat} {feat_type}\")\n",
    "    print(f\"      Correlation: {row['correlation']:.3f}\")\n",
    "    print(f\"      VIP: {row['VIP']:.2f}\")\n",
    "    print(f\"      Reason: {reasons[feat]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e330347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 20: MANUAL FEATURE SELECTION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FEATURE SELECTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Option 1: Use recommendations\n",
    "selected_features = recommended[:TARGET_FEATURES]\n",
    "\n",
    "# Option 2: Manual override - uncomment and modify\n",
    "# selected_features = [\n",
    "#     'feature_1',\n",
    "#     'feature_2',\n",
    "#     'feature_3',\n",
    "# ]\n",
    "\n",
    "print(f\"\\nSelected features ({len(selected_features)}):\")\n",
    "for i, feat in enumerate(selected_features, 1):\n",
    "    row = consensus[consensus['feature'] == feat].iloc[0]\n",
    "    print(f\"  {i}. {feat} ({row['type']}) - corr: {row['correlation']:.3f}, VIP: {row['VIP']:.2f}\")\n",
    "\n",
    "# Validation\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"VALIDATION CHECKS:\")\n",
    "\n",
    "# Check multicollinearity in selection\n",
    "if len(selected_features) > 1:\n",
    "    sel_corr = X[selected_features].corr()\n",
    "    issues = []\n",
    "    for i in range(len(selected_features)):\n",
    "        for j in range(i+1, len(selected_features)):\n",
    "            r = sel_corr.iloc[i, j]\n",
    "            if abs(r) > MULTICOLLINEARITY_THRESHOLD:\n",
    "                issues.append(f\"{selected_features[i]} ↔ {selected_features[j]}: r={r:.2f}\")\n",
    "    \n",
    "    if issues:\n",
    "        print(f\"  ⚠️ Multicollinearity:\")\n",
    "        for issue in issues:\n",
    "            print(f\"      {issue}\")\n",
    "    else:\n",
    "        print(f\"  ✓ No multicollinearity issues\")\n",
    "\n",
    "# Check broken interactions\n",
    "if len(strong_interactions) > 0:\n",
    "    broken = []\n",
    "    for _, row in strong_interactions.iterrows():\n",
    "        f1, f2 = row['feature_1'], row['feature_2']\n",
    "        if (f1 in selected_features) != (f2 in selected_features):\n",
    "            broken.append((f1, f2))\n",
    "    \n",
    "    if broken:\n",
    "        print(f\"  ⚠️ Broken interactions:\")\n",
    "        for f1, f2 in broken:\n",
    "            in1 = \"IN\" if f1 in selected_features else \"OUT\"\n",
    "            in2 = \"IN\" if f2 in selected_features else \"OUT\"\n",
    "            print(f\"      {f1} ({in1}) × {f2} ({in2})\")\n",
    "    else:\n",
    "        print(f\"  ✓ No broken interactions\")\n",
    "else:\n",
    "    print(f\"  ✓ No interactions to check\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e209d951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 21: VALIDATE WITH LOO-CV\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"VALIDATION: Leave-One-Out Cross-Validation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Prepare data\n",
    "X_sel = X_model[selected_features]\n",
    "\n",
    "# LOO-CV\n",
    "loo_preds, loo_actual = [], []\n",
    "for train_idx, test_idx in LeaveOneOut().split(X_sel):\n",
    "    model = RidgeCV(alphas=[0.1, 1, 10, 100], cv=3)\n",
    "    model.fit(X_sel.iloc[train_idx], y.iloc[train_idx])\n",
    "    loo_preds.append(model.predict(X_sel.iloc[test_idx])[0])\n",
    "    loo_actual.append(y.iloc[test_idx].values[0])\n",
    "\n",
    "loo_preds = np.array(loo_preds)\n",
    "loo_actual = np.array(loo_actual)\n",
    "\n",
    "# Metrics\n",
    "loo_r2 = r2_score(loo_actual, loo_preds)\n",
    "loo_rmse = np.sqrt(np.mean((loo_actual - loo_preds)**2))\n",
    "loo_mae = np.mean(np.abs(loo_actual - loo_preds))\n",
    "\n",
    "print(f\"\\nLOO-CV Results ({len(selected_features)} features):\")\n",
    "print(f\"  R²:   {loo_r2:.4f}\")\n",
    "print(f\"  RMSE: {loo_rmse:.4f}\")\n",
    "print(f\"  MAE:  {loo_mae:.4f}\")\n",
    "\n",
    "# Interpretation\n",
    "print(f\"\\nInterpretation:\")\n",
    "if loo_r2 > 0.5:\n",
    "    print(\"  ✓ Good signal - features are predictive\")\n",
    "elif loo_r2 > 0.2:\n",
    "    print(\"  ⚠️ Moderate signal - GP in BO can likely improve\")\n",
    "elif loo_r2 > 0:\n",
    "    print(\"  ⚠️ Weak linear signal - may be non-linear\")\n",
    "else:\n",
    "    print(\"  ⚠️ No linear signal - check data or feature selection\")\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Predicted vs Actual\n",
    "ax1 = axes[0]\n",
    "ax1.scatter(loo_actual, loo_preds, alpha=0.7, edgecolors='black', linewidth=0.5)\n",
    "min_val, max_val = min(loo_actual.min(), loo_preds.min()), max(loo_actual.max(), loo_preds.max())\n",
    "ax1.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect')\n",
    "ax1.set_xlabel('Actual')\n",
    "ax1.set_ylabel('Predicted')\n",
    "ax1.set_title(f'LOO-CV: R² = {loo_r2:.4f}')\n",
    "ax1.legend()\n",
    "\n",
    "# Residuals histogram\n",
    "ax2 = axes[1]\n",
    "residuals = loo_actual - loo_preds\n",
    "ax2.hist(residuals, bins=12, edgecolor='black', alpha=0.7)\n",
    "ax2.axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "ax2.set_xlabel('Residual (Actual - Predicted)')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title(f'Residuals: Mean={residuals.mean():.3f}')\n",
    "\n",
    "# Residuals vs Predicted\n",
    "ax3 = axes[2]\n",
    "ax3.scatter(loo_preds, residuals, alpha=0.7, edgecolors='black', linewidth=0.5)\n",
    "ax3.axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "ax3.set_xlabel('Predicted')\n",
    "ax3.set_ylabel('Residual')\n",
    "ax3.set_title('Residuals vs Predicted')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e381860e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 22: DEFINE BO SEARCH SPACE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BAYESIAN OPTIMIZATION SEARCH SPACE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create bounds\n",
    "bo_bounds = []\n",
    "for feat in selected_features:\n",
    "    if feat in BINARY_FEATURES:\n",
    "        bo_bounds.append({\n",
    "            'feature': feat,\n",
    "            'type': 'binary',\n",
    "            'min': 0,\n",
    "            'max': 1,\n",
    "            'observed_min': 0,\n",
    "            'observed_max': 1\n",
    "        })\n",
    "    else:\n",
    "        feat_min = original_X[feat].min()\n",
    "        feat_max = original_X[feat].max()\n",
    "        feat_range = feat_max - feat_min\n",
    "        margin = 0.1 * feat_range\n",
    "        bo_bounds.append({\n",
    "            'feature': feat,\n",
    "            'type': 'continuous',\n",
    "            'min': feat_min - margin,\n",
    "            'max': feat_max + margin,\n",
    "            'observed_min': feat_min,\n",
    "            'observed_max': feat_max\n",
    "        })\n",
    "\n",
    "bo_bounds_df = pd.DataFrame(bo_bounds)\n",
    "\n",
    "print(\"\\nSearch Space Bounds:\")\n",
    "print(bo_bounds_df.to_string(index=False))\n",
    "\n",
    "# Effect directions\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"EFFECT DIRECTIONS:\")\n",
    "for feat in selected_features:\n",
    "    row = consensus[consensus['feature'] == feat].iloc[0]\n",
    "    direction = row['correlation']\n",
    "    \n",
    "    if MAXIMIZE_RESPONSE:\n",
    "        suggest = \"HIGH\" if direction > 0 else \"LOW\"\n",
    "    else:\n",
    "        suggest = \"LOW\" if direction > 0 else \"HIGH\"\n",
    "    \n",
    "    print(f\"  {feat}: {'Positive' if direction > 0 else 'Negative'} effect → Suggest {suggest}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797d1fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 23: EXPORT FOR BO\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EXPORT FOR BAYESIAN OPTIMIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initial data\n",
    "bo_data = original_X[selected_features].copy()\n",
    "bo_data[RESPONSE_COLUMN] = original_y.values\n",
    "\n",
    "# Best point so far\n",
    "if MAXIMIZE_RESPONSE:\n",
    "    best_idx = original_y.idxmax()\n",
    "    best_val = original_y.max()\n",
    "else:\n",
    "    best_idx = original_y.idxmin()\n",
    "    best_val = original_y.min()\n",
    "\n",
    "print(f\"\\nInitial Data:\")\n",
    "print(f\"  Samples: {len(bo_data)}\")\n",
    "print(f\"  Features: {len(selected_features)}\")\n",
    "\n",
    "print(f\"\\nBest Observed {'Maximum' if MAXIMIZE_RESPONSE else 'Minimum'}:\")\n",
    "print(f\"  {RESPONSE_COLUMN} = {best_val:.4f}\")\n",
    "print(f\"  Conditions:\")\n",
    "for feat in selected_features:\n",
    "    print(f\"    {feat}: {original_X.loc[best_idx, feat]:.4f}\")\n",
    "\n",
    "# Save files\n",
    "bo_bounds_df.to_csv('bo_bounds.csv', index=False)\n",
    "bo_data.to_csv('bo_initial_data.csv', index=False)\n",
    "\n",
    "# Save feature info\n",
    "feature_info = consensus[consensus['feature'].isin(selected_features)][\n",
    "    ['feature', 'type', 'correlation', 'VIP', 'has_interaction']\n",
    "].reset_index(drop=True)\n",
    "feature_info.to_csv('bo_feature_info.csv', index=False)\n",
    "\n",
    "# Save binary mappings for selected binary features\n",
    "selected_binary = [f for f in selected_features if f in BINARY_FEATURES]\n",
    "if selected_binary:\n",
    "    mapping_records = [{'feature': f, 'value_0': list(binary_mappings[f].keys())[0], \n",
    "                        'value_1': list(binary_mappings[f].keys())[1]} \n",
    "                       for f in selected_binary]\n",
    "    pd.DataFrame(mapping_records).to_csv('bo_binary_mappings.csv', index=False)\n",
    "    print(f\"\\n✓ Saved: bo_binary_mappings.csv\")\n",
    "\n",
    "print(f\"\\n✓ Saved: bo_bounds.csv\")\n",
    "print(f\"✓ Saved: bo_initial_data.csv\")\n",
    "print(f\"✓ Saved: bo_feature_info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be89f133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 24: FINAL SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 1 COMPLETE: FEATURE SCREENING SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\"\"\n",
    "DATA:\n",
    "  Total samples: {len(X)}\n",
    "  Total features: {len(feature_cols)}\n",
    "    - Binary: {len(BINARY_FEATURES)}\n",
    "    - Continuous: {len(CONTINUOUS_FEATURES)}\n",
    "\n",
    "METHODS USED:\n",
    "  1. Pearson Correlation\n",
    "  2. Lasso Regression (automatic selection)\n",
    "  3. PLS with VIP Scores\n",
    "  4. Interaction Screening\n",
    "  5. Multicollinearity Check\n",
    "\n",
    "SELECTED FOR BAYESIAN OPTIMIZATION ({len(selected_features)}):\n",
    "\"\"\")\n",
    "\n",
    "for feat in selected_features:\n",
    "    row = consensus[consensus['feature'] == feat].iloc[0]\n",
    "    bounds = bo_bounds_df[bo_bounds_df['feature'] == feat].iloc[0]\n",
    "    int_flag = \" ⚡\" if row['has_interaction'] else \"\"\n",
    "    print(f\"  • {feat} ({row['type']}){int_flag}\")\n",
    "    print(f\"      Correlation: {row['correlation']:+.3f}\")\n",
    "    print(f\"      VIP Score: {row['VIP']:.2f}\")\n",
    "    print(f\"      Bounds: [{bounds['min']:.2f}, {bounds['max']:.2f}]\")\n",
    "\n",
    "if features_with_interactions:\n",
    "    print(f\"\\n  ⚡ = Part of detected interaction\")\n",
    "\n",
    "print(f\"\"\"\n",
    "VALIDATION:\n",
    "  LOO-CV R²: {loo_r2:.4f}\n",
    "  LOO-CV RMSE: {loo_rmse:.4f}\n",
    "\n",
    "NEXT STEPS (Phase 2: Bayesian Optimization):\n",
    "  1. Load bo_initial_data.csv as initial training data\n",
    "  2. Use bo_bounds.csv for search space\n",
    "  3. Fit GP surrogate model\n",
    "  4. Run acquisition function loop (EI or UCB)\n",
    "  5. Expected iterations: {20*len(selected_features)}-{40*len(selected_features)}\n",
    "\n",
    "OUTPUT FILES:\n",
    "  • bo_bounds.csv - Feature bounds for BO\n",
    "  • bo_initial_data.csv - Initial GP training data\n",
    "  • bo_feature_info.csv - Feature details\n",
    "\"\"\")\n",
    "\n",
    "if selected_binary:\n",
    "    print(f\"  • bo_binary_mappings.csv - Binary encoding reference\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"READY FOR PHASE 2: BAYESIAN OPTIMIZATION\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
