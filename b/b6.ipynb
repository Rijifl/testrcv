{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b40089",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Bayesian Optimization Pipeline - Phase 2\n",
    "=========================================\n",
    "Iterative Bayesian Optimization for Chemical Experiments\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Optional, Tuple, Any, Union\n",
    "from itertools import product\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel, Matern, WhiteKernel\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00df8728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Configuration\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "@dataclass\n",
    "class Phase2Config:\n",
    "    \"\"\"Phase 2 configuration parameters.\"\"\"\n",
    "    \n",
    "    # Data\n",
    "    response_column: str\n",
    "    sheet_name: str = 'data'\n",
    "    header_row: int = 5\n",
    "    split_keyword: Optional[str] = \"PREDICTED OPTIMUM RUNS\"\n",
    "    stop_feature: Optional[str] = \"Batch ID\"\n",
    "    \n",
    "    # Optimization\n",
    "    maximize_response: bool = True\n",
    "    n_suggestions: int = 5\n",
    "    selection_strategy: str = 'diverse'  # 'diverse' hedges bets; 'greedy' exploits best region\n",
    "    min_distance: float = 0.1  # Normalized distance; prevents clustered suggestions\n",
    "    \n",
    "    # Acquisition function\n",
    "    # EI chosen over UCB: naturally balances exploration/exploitation without tuning\n",
    "    exploration_weight: float = 0.01  # xi parameter; small value favors exploitation\n",
    "    \n",
    "    # Search space\n",
    "    # 10% margin allows mild extrapolation beyond observed data\n",
    "    bounds_margin: float = 0.1\n",
    "    \n",
    "    # Optimizer\n",
    "    # Multi-start needed because acquisition function is multimodal\n",
    "    n_optimizer_restarts: int = 25\n",
    "    \n",
    "    # Stopping\n",
    "    # Patience-based: simple, interpretable, works well empirically\n",
    "    patience: int = 3\n",
    "    min_improvement: float = 0.01  # 1% relative improvement threshold\n",
    "    \n",
    "    # Duplicate prevention\n",
    "    # Prevents suggesting experiments too similar to existing ones\n",
    "    duplicate_threshold: float = 0.05\n",
    "    \n",
    "    # Output\n",
    "    output_dir: str = 'bo_phase2_output'\n",
    "    fresh_start: bool = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1949e7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# DataLoader (Same as Phase 1)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "class DataLoader:\n",
    "    \"\"\"Handles data loading and feature classification.\"\"\"\n",
    "    \n",
    "    def load_excel(self, file_path: str, sheet_name: str, header_row: int) -> pd.DataFrame:\n",
    "        xls = pd.ExcelFile(file_path, engine='openpyxl')\n",
    "        return pd.read_excel(xls, sheet_name=sheet_name, header=header_row)\n",
    "    \n",
    "    def split_at_keyword(self, df: pd.DataFrame, keyword: str, \n",
    "                         column: str = 'Run') -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        split_index = df.index[df[column] == keyword].tolist()\n",
    "        if split_index:\n",
    "            idx = split_index[0]\n",
    "            return df.iloc[:idx], df.iloc[idx+1:]\n",
    "        return df.copy(), pd.DataFrame()\n",
    "    \n",
    "    def clean_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df.drop(index=0, errors='ignore')\n",
    "        df = df.dropna(how='all')\n",
    "        return df.reset_index(drop=True)\n",
    "    \n",
    "    def classify_features(self, df: pd.DataFrame, feature_cols: List[str]\n",
    "                         ) -> Tuple[List[str], List[str], Dict]:\n",
    "        \"\"\"Classify as binary (2 unique values) or continuous.\"\"\"\n",
    "        numeric_features = df[feature_cols].select_dtypes(include=[np.number]).columns.tolist()\n",
    "        \n",
    "        binary_cols, continuous_cols, binary_mappings = [], [], {}\n",
    "        \n",
    "        for col in numeric_features:\n",
    "            if df[col].nunique() == 2:\n",
    "                binary_cols.append(col)\n",
    "                unique_vals = sorted(df[col].dropna().unique())\n",
    "                mapping = {unique_vals[0]: 0, unique_vals[1]: 1}\n",
    "                df[col] = df[col].map(mapping)\n",
    "                binary_mappings[col] = mapping\n",
    "            else:\n",
    "                continuous_cols.append(col)\n",
    "        \n",
    "        return binary_cols, continuous_cols, binary_mappings\n",
    "    \n",
    "    def get_feature_columns(self, df: pd.DataFrame, stop_feature: Optional[str], \n",
    "                           response_column: str) -> List[str]:\n",
    "        columns = df.columns.tolist()\n",
    "        \n",
    "        if stop_feature and stop_feature in columns:\n",
    "            feature_list = columns[:columns.index(stop_feature)]\n",
    "        else:\n",
    "            feature_list = [c for c in columns if c != response_column]\n",
    "        \n",
    "        # Remove index-like columns\n",
    "        return [f for f in feature_list if f.lower() not in ['run', 'index', 'unnamed: 0']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febab38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# SearchSpace\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "class SearchSpace:\n",
    "    \"\"\"\n",
    "    Manages bounds and binary enumeration.\n",
    "    \n",
    "    Binary features are enumerated (not treated as continuous) because:\n",
    "    - GPs struggle with discontinuous functions\n",
    "    - Categorical boundaries are sharp in chemistry (catalyst A vs B)\n",
    "    - Enumeration is tractable for typical 1-2 binary features\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, X: pd.DataFrame, binary_features: List[str], \n",
    "                 continuous_features: List[str], bounds_margin: float = 0.1):\n",
    "        self.binary_features = binary_features\n",
    "        self.continuous_features = continuous_features\n",
    "        \n",
    "        self._bounds = {}\n",
    "        self._binary_combinations = []\n",
    "        \n",
    "        # Compute continuous bounds with margin for extrapolation\n",
    "        for col in continuous_features:\n",
    "            col_min, col_max = X[col].min(), X[col].max()\n",
    "            margin = (col_max - col_min) * bounds_margin\n",
    "            self._bounds[col] = {\n",
    "                'min': col_min - margin, 'max': col_max + margin,\n",
    "                'observed_min': col_min, 'observed_max': col_max, 'type': 'continuous'\n",
    "            }\n",
    "        \n",
    "        for col in binary_features:\n",
    "            self._bounds[col] = {'min': 0, 'max': 1, 'observed_min': 0, \n",
    "                                 'observed_max': 1, 'type': 'binary'}\n",
    "        \n",
    "        # Enumerate all binary combinations (2^n, typically n <= 2)\n",
    "        if binary_features:\n",
    "            values = list(product([0, 1], repeat=len(binary_features)))\n",
    "            self._binary_combinations = [dict(zip(binary_features, combo)) for combo in values]\n",
    "        else:\n",
    "            self._binary_combinations = [{}]\n",
    "    \n",
    "    def get_continuous_bounds(self) -> List[Tuple[float, float]]:\n",
    "        return [(self._bounds[col]['min'], self._bounds[col]['max']) \n",
    "                for col in self.continuous_features]\n",
    "    \n",
    "    def get_bounds_df(self) -> pd.DataFrame:\n",
    "        rows = [{'feature': col, **bounds} for col, bounds in self._bounds.items()]\n",
    "        return pd.DataFrame(rows)\n",
    "    \n",
    "    @property\n",
    "    def bounds(self) -> Dict:\n",
    "        return self._bounds\n",
    "    \n",
    "    @property\n",
    "    def binary_combinations(self) -> List[Dict]:\n",
    "        return self._binary_combinations\n",
    "    \n",
    "    @property\n",
    "    def n_binary_combinations(self) -> int:\n",
    "        return len(self._binary_combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73544262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# SurrogateModel\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "class SurrogateModel:\n",
    "    \"\"\"\n",
    "    Gaussian Process surrogate model.\n",
    "    \n",
    "    Design choices:\n",
    "    - Matern 5/2 kernel: twice differentiable, handles non-smooth responses\n",
    "      better than RBF while remaining smooth enough for optimization\n",
    "    - StandardScaler on X and y: improves numerical stability and \n",
    "      makes length scale interpretation consistent across features\n",
    "    - WhiteKernel: explicitly models observation noise\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.scaler_X = StandardScaler()\n",
    "        self.scaler_y = StandardScaler()\n",
    "        self._is_fitted = False\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        X_scaled = self.scaler_X.fit_transform(X)\n",
    "        y_scaled = self.scaler_y.fit_transform(y.reshape(-1, 1)).ravel()\n",
    "        \n",
    "        # Matern 5/2: good default for physical processes\n",
    "        kernel = (\n",
    "            ConstantKernel(1.0, (1e-3, 1e3)) * \n",
    "            Matern(length_scale=np.ones(X.shape[1]), \n",
    "                   length_scale_bounds=(1e-3, 1e3), nu=2.5) +\n",
    "            WhiteKernel(noise_level=0.1, noise_level_bounds=(1e-5, 1e1))\n",
    "        )\n",
    "        \n",
    "        self.model = GaussianProcessRegressor(\n",
    "            kernel=kernel, n_restarts_optimizer=10,\n",
    "            normalize_y=False, random_state=RANDOM_STATE\n",
    "        )\n",
    "        self.model.fit(X_scaled, y_scaled)\n",
    "        self._is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X: np.ndarray, return_std: bool = True):\n",
    "        X_scaled = self.scaler_X.transform(X)\n",
    "        \n",
    "        if return_std:\n",
    "            mu_scaled, sigma_scaled = self.model.predict(X_scaled, return_std=True)\n",
    "            mu = self.scaler_y.inverse_transform(mu_scaled.reshape(-1, 1)).ravel()\n",
    "            sigma = sigma_scaled * self.scaler_y.scale_[0]\n",
    "            return mu, sigma\n",
    "        else:\n",
    "            mu_scaled = self.model.predict(X_scaled)\n",
    "            return self.scaler_y.inverse_transform(mu_scaled.reshape(-1, 1)).ravel()\n",
    "    \n",
    "    def score(self, X: np.ndarray, y: np.ndarray) -> Dict[str, float]:\n",
    "        y_pred = self.predict(X, return_std=False)\n",
    "        return {\n",
    "            'r2': r2_score(y, y_pred),\n",
    "            'rmse': np.sqrt(mean_squared_error(y, y_pred))\n",
    "        }\n",
    "    \n",
    "    @property\n",
    "    def is_fitted(self) -> bool:\n",
    "        return self._is_fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cacca37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# AcquisitionOptimizer\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "class AcquisitionOptimizer:\n",
    "    \"\"\"\n",
    "    Optimizes Expected Improvement acquisition function.\n",
    "    \n",
    "    EI chosen because:\n",
    "    - Balances exploration/exploitation without explicit weighting\n",
    "    - Well-understood theoretical properties\n",
    "    - Works well empirically for chemical optimization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, surrogate: SurrogateModel, search_space: SearchSpace,\n",
    "                 config: Phase2Config):\n",
    "        self.surrogate = surrogate\n",
    "        self.search_space = search_space\n",
    "        self.config = config\n",
    "        self._best_y = None\n",
    "    \n",
    "    def set_best_y(self, best_y: float):\n",
    "        self._best_y = best_y\n",
    "    \n",
    "    def expected_improvement(self, X: np.ndarray, xi: float = 0.01) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        EI(x) = (mu - f_best - xi) * Phi(Z) + sigma * phi(Z)\n",
    "        \n",
    "        xi > 0 encourages exploration; xi ≈ 0 favors exploitation\n",
    "        \"\"\"\n",
    "        mu, sigma = self.surrogate.predict(X, return_std=True)\n",
    "        sigma = np.maximum(sigma, 1e-9)\n",
    "        \n",
    "        if self.config.maximize_response:\n",
    "            improvement = mu - self._best_y - xi\n",
    "        else:\n",
    "            improvement = self._best_y - mu - xi\n",
    "        \n",
    "        Z = improvement / sigma\n",
    "        ei = improvement * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "        ei[sigma < 1e-9] = 0.0\n",
    "        return ei\n",
    "    \n",
    "    def _construct_full_x(self, continuous_x: np.ndarray, binary_values: Dict) -> np.ndarray:\n",
    "        \"\"\"Combine continuous values with fixed binary values.\"\"\"\n",
    "        full_x = []\n",
    "        cont_idx = 0\n",
    "        for col in self.search_space.continuous_features + self.search_space.binary_features:\n",
    "            if col in self.search_space.binary_features:\n",
    "                full_x.append(binary_values[col])\n",
    "            else:\n",
    "                full_x.append(continuous_x[cont_idx])\n",
    "                cont_idx += 1\n",
    "        return np.array(full_x)\n",
    "    \n",
    "    def _negative_ei(self, x: np.ndarray, binary_values: Dict) -> float:\n",
    "        full_x = self._construct_full_x(x, binary_values)\n",
    "        ei = self.expected_improvement(full_x.reshape(1, -1), xi=self.config.exploration_weight)\n",
    "        return -ei[0]\n",
    "    \n",
    "    def optimize_single(self, binary_values: Dict, \n",
    "                        existing_points: Optional[np.ndarray] = None) -> Tuple[np.ndarray, float]:\n",
    "        \"\"\"Find best point for given binary combination using multi-start L-BFGS-B.\"\"\"\n",
    "        bounds = self.search_space.get_continuous_bounds()\n",
    "        \n",
    "        if not bounds:\n",
    "            full_x = self._construct_full_x(np.array([]), binary_values)\n",
    "            ei = self.expected_improvement(full_x.reshape(1, -1), xi=self.config.exploration_weight)[0]\n",
    "            return full_x, ei\n",
    "        \n",
    "        best_x, best_ei = None, -np.inf\n",
    "        \n",
    "        # Multi-start: acquisition function is often multimodal\n",
    "        for _ in range(self.config.n_optimizer_restarts):\n",
    "            x0 = np.array([np.random.uniform(b[0], b[1]) for b in bounds])\n",
    "            \n",
    "            try:\n",
    "                result = minimize(\n",
    "                    lambda x: self._negative_ei(x, binary_values),\n",
    "                    x0, method='L-BFGS-B', bounds=bounds\n",
    "                )\n",
    "                \n",
    "                if -result.fun > best_ei:\n",
    "                    candidate_x = self._construct_full_x(result.x, binary_values)\n",
    "                    \n",
    "                    # Enforce diversity constraint\n",
    "                    if existing_points is not None and len(existing_points) > 0:\n",
    "                        min_dist = np.min(np.linalg.norm(existing_points - candidate_x, axis=1))\n",
    "                        if min_dist < self.config.min_distance:\n",
    "                            continue\n",
    "                    \n",
    "                    best_x, best_ei = candidate_x, -result.fun\n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        return best_x, best_ei\n",
    "    \n",
    "    def find_next_points(self, n_points: int, existing_X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Find n_points suggestions using configured strategy.\"\"\"\n",
    "        all_points = existing_X.copy() if len(existing_X) > 0 else None\n",
    "        \n",
    "        if self.config.selection_strategy == 'greedy':\n",
    "            return self._greedy_selection(n_points, all_points)\n",
    "        else:\n",
    "            return self._diverse_selection(n_points, all_points)\n",
    "    \n",
    "    def _greedy_selection(self, n_points: int, existing_points: np.ndarray) -> Tuple[List, List]:\n",
    "        \"\"\"Select top n points by EI, regardless of spread.\"\"\"\n",
    "        candidates = []\n",
    "        \n",
    "        for binary_combo in self.search_space.binary_combinations:\n",
    "            best_x, best_ei = self.optimize_single(binary_combo, existing_points)\n",
    "            if best_x is not None:\n",
    "                candidates.append((best_x, best_ei))\n",
    "        \n",
    "        candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        suggestions, ei_values = [], []\n",
    "        for x, ei in candidates[:n_points]:\n",
    "            if existing_points is not None:\n",
    "                min_dist = np.min(np.linalg.norm(existing_points - x, axis=1))\n",
    "                if min_dist < self.config.duplicate_threshold:\n",
    "                    continue\n",
    "            suggestions.append(x)\n",
    "            ei_values.append(ei)\n",
    "        \n",
    "        return np.array(suggestions), np.array(ei_values)\n",
    "    \n",
    "    def _diverse_selection(self, n_points: int, existing_points: np.ndarray) -> Tuple[List, List]:\n",
    "        \"\"\"Distribute suggestions across binary combinations and enforce spacing.\"\"\"\n",
    "        suggestions, ei_values = [], []\n",
    "        points_to_avoid = existing_points.copy() if existing_points is not None and len(existing_points) > 0 else None\n",
    "        \n",
    "        n_combos = self.search_space.n_binary_combinations\n",
    "        combo_counts = {i: 0 for i in range(n_combos)}\n",
    "        \n",
    "        for _ in range(n_points):\n",
    "            # Round-robin across binary combinations\n",
    "            combo_idx = min(combo_counts, key=combo_counts.get)\n",
    "            binary_combo = self.search_space.binary_combinations[combo_idx]\n",
    "            \n",
    "            best_x, best_ei = self.optimize_single(binary_combo, points_to_avoid)\n",
    "            \n",
    "            if best_x is not None and best_ei > 0:\n",
    "                suggestions.append(best_x)\n",
    "                ei_values.append(best_ei)\n",
    "                combo_counts[combo_idx] += 1\n",
    "                \n",
    "                # Add to avoidance set for diversity\n",
    "                if points_to_avoid is not None:\n",
    "                    points_to_avoid = np.vstack([points_to_avoid, best_x])\n",
    "                else:\n",
    "                    points_to_avoid = best_x.reshape(1, -1)\n",
    "            else:\n",
    "                combo_counts[combo_idx] = float('inf')\n",
    "        \n",
    "        return np.array(suggestions), np.array(ei_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0033d840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# ConvergenceTracker\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "class ConvergenceTracker:\n",
    "    \"\"\"\n",
    "    Tracks optimization progress with patience-based stopping.\n",
    "    \n",
    "    Patience approach chosen over fixed iterations because:\n",
    "    - Adapts to problem difficulty\n",
    "    - Clear interpretation for experimenters\n",
    "    - Avoids premature stopping or wasted experiments\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: Phase2Config):\n",
    "        self.config = config\n",
    "        self.history = []\n",
    "        self.no_improvement_count = 0\n",
    "    \n",
    "    def load_history(self, history: List[Dict]):\n",
    "        self.history = history\n",
    "        self._update_no_improvement_count()\n",
    "    \n",
    "    def _update_no_improvement_count(self):\n",
    "        if len(self.history) < 2:\n",
    "            self.no_improvement_count = 0\n",
    "            return\n",
    "        \n",
    "        count = 0\n",
    "        best_so_far = self.history[0]['best_response']\n",
    "        \n",
    "        for record in self.history[1:]:\n",
    "            if self.config.maximize_response:\n",
    "                improved = record['best_response'] > best_so_far * (1 + self.config.min_improvement)\n",
    "            else:\n",
    "                improved = record['best_response'] < best_so_far * (1 - self.config.min_improvement)\n",
    "            \n",
    "            if improved:\n",
    "                best_so_far = record['best_response']\n",
    "                count = 0\n",
    "            else:\n",
    "                count += 1\n",
    "        \n",
    "        self.no_improvement_count = count\n",
    "    \n",
    "    def record_iteration(self, iteration: int, best_response: float, \n",
    "                         n_experiments: int, suggestions: pd.DataFrame):\n",
    "        record = {\n",
    "            'iteration': iteration,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'best_response': best_response,\n",
    "            'n_experiments': n_experiments,\n",
    "            'n_suggestions': len(suggestions)\n",
    "        }\n",
    "        \n",
    "        if len(self.history) > 0:\n",
    "            prev_best = self.history[-1]['best_response']\n",
    "            if self.config.maximize_response:\n",
    "                improvement = (best_response - prev_best) / abs(prev_best) if prev_best != 0 else 0\n",
    "                improved = improvement > self.config.min_improvement\n",
    "            else:\n",
    "                improvement = (prev_best - best_response) / abs(prev_best) if prev_best != 0 else 0\n",
    "                improved = improvement > self.config.min_improvement\n",
    "            \n",
    "            record['improvement'] = improvement\n",
    "            record['improved'] = improved\n",
    "            \n",
    "            self.no_improvement_count = 0 if improved else self.no_improvement_count + 1\n",
    "        \n",
    "        self.history.append(record)\n",
    "    \n",
    "    @property\n",
    "    def should_stop(self) -> bool:\n",
    "        return self.no_improvement_count >= self.config.patience\n",
    "    \n",
    "    @property\n",
    "    def current_iteration(self) -> int:\n",
    "        return len(self.history)\n",
    "    \n",
    "    @property\n",
    "    def best_response(self) -> Optional[float]:\n",
    "        if not self.history:\n",
    "            return None\n",
    "        if self.config.maximize_response:\n",
    "            return max(h['best_response'] for h in self.history)\n",
    "        return min(h['best_response'] for h in self.history)\n",
    "    \n",
    "    def get_history_df(self) -> pd.DataFrame:\n",
    "        return pd.DataFrame(self.history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6cf57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Checkpoint Manager\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "class Phase2Checkpoint:\n",
    "    \"\"\"Persists state between iterations (days/weeks apart in practice).\"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir: str):\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        self.checkpoint_file = self.output_dir / 'phase2_checkpoint.pkl'\n",
    "        self.json_file = self.output_dir / 'phase2_checkpoint.json'\n",
    "    \n",
    "    def save(self, state: Dict[str, Any]):\n",
    "        with open(self.checkpoint_file, 'wb') as f:\n",
    "            pickle.dump(state, f)\n",
    "        \n",
    "        # Human-readable summary\n",
    "        json_state = {k: v for k, v in state.items() \n",
    "                      if k not in ['history']}  # Exclude large objects\n",
    "        with open(self.json_file, 'w') as f:\n",
    "            json.dump(json_state, f, indent=2, default=str)\n",
    "    \n",
    "    def load(self) -> Optional[Dict[str, Any]]:\n",
    "        if not self.checkpoint_file.exists():\n",
    "            return None\n",
    "        with open(self.checkpoint_file, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    \n",
    "    def exists(self) -> bool:\n",
    "        return self.checkpoint_file.exists()\n",
    "    \n",
    "    def clear(self):\n",
    "        if self.checkpoint_file.exists():\n",
    "            self.checkpoint_file.unlink()\n",
    "        if self.json_file.exists():\n",
    "            self.json_file.unlink()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f3e48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Plotter\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "class Phase2Plotter:\n",
    "    \"\"\"Generates diagnostic visualizations.\"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir: str):\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    def plot_convergence(self, tracker: ConvergenceTracker, config: Phase2Config):\n",
    "        history_df = tracker.get_history_df()\n",
    "        if len(history_df) < 1:\n",
    "            return\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(8, 5))\n",
    "        ax.plot(history_df['iteration'], history_df['best_response'], 'bo-', linewidth=2, markersize=8)\n",
    "        ax.set_xlabel('Iteration')\n",
    "        ax.set_ylabel('Best Response')\n",
    "        ax.set_title(f'Optimization Progress ({\"Max\" if config.maximize_response else \"Min\"})')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.output_dir / 'convergence_plot.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_gp_1d(self, surrogate: SurrogateModel, X: np.ndarray, y: np.ndarray,\n",
    "                   feature_names: List[str], feature_idx: int, search_space: SearchSpace):\n",
    "        \"\"\"1D slice through GP (other features at mean).\"\"\"\n",
    "        feature_name = feature_names[feature_idx]\n",
    "        if feature_name not in search_space.continuous_features:\n",
    "            return\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(8, 5))\n",
    "        \n",
    "        n_test = 100\n",
    "        X_mean = X.mean(axis=0)\n",
    "        bounds = search_space.bounds[feature_name]\n",
    "        test_values = np.linspace(bounds['min'], bounds['max'], n_test)\n",
    "        \n",
    "        X_test = np.tile(X_mean, (n_test, 1))\n",
    "        X_test[:, feature_idx] = test_values\n",
    "        \n",
    "        mu, sigma = surrogate.predict(X_test, return_std=True)\n",
    "        \n",
    "        ax.fill_between(test_values, mu - 2*sigma, mu + 2*sigma, alpha=0.3, label='95% CI')\n",
    "        ax.plot(test_values, mu, 'b-', linewidth=2, label='GP Mean')\n",
    "        ax.scatter(X[:, feature_idx], y, c='red', s=50, zorder=5, edgecolors='black', label='Observed')\n",
    "        ax.set_xlabel(feature_name)\n",
    "        ax.set_ylabel('Response')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.output_dir / f'gp_1d_{feature_name}.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_gp_2d(self, surrogate: SurrogateModel, X: np.ndarray, y: np.ndarray,\n",
    "                   feature_names: List[str], idx1: int, idx2: int, search_space: SearchSpace):\n",
    "        \"\"\"2D contour of GP surface.\"\"\"\n",
    "        feat_1, feat_2 = feature_names[idx1], feature_names[idx2]\n",
    "        \n",
    "        if feat_1 not in search_space.continuous_features or \\\n",
    "           feat_2 not in search_space.continuous_features:\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        n_grid = 50\n",
    "        x1_range = np.linspace(search_space.bounds[feat_1]['min'], \n",
    "                               search_space.bounds[feat_1]['max'], n_grid)\n",
    "        x2_range = np.linspace(search_space.bounds[feat_2]['min'], \n",
    "                               search_space.bounds[feat_2]['max'], n_grid)\n",
    "        X1, X2 = np.meshgrid(x1_range, x2_range)\n",
    "        \n",
    "        X_mean = X.mean(axis=0)\n",
    "        X_test = np.tile(X_mean, (n_grid * n_grid, 1))\n",
    "        X_test[:, idx1] = X1.ravel()\n",
    "        X_test[:, idx2] = X2.ravel()\n",
    "        \n",
    "        mu, sigma = surrogate.predict(X_test, return_std=True)\n",
    "        Mu = mu.reshape(n_grid, n_grid)\n",
    "        Sigma = sigma.reshape(n_grid, n_grid)\n",
    "        \n",
    "        # Mean surface\n",
    "        cs1 = axes[0].contourf(X1, X2, Mu, levels=20, cmap='viridis')\n",
    "        axes[0].scatter(X[:, idx1], X[:, idx2], c='red', s=50, edgecolors='white')\n",
    "        axes[0].set_xlabel(feat_1)\n",
    "        axes[0].set_ylabel(feat_2)\n",
    "        axes[0].set_title('GP Mean')\n",
    "        plt.colorbar(cs1, ax=axes[0])\n",
    "        \n",
    "        # Uncertainty surface\n",
    "        cs2 = axes[1].contourf(X1, X2, Sigma, levels=20, cmap='YlOrRd')\n",
    "        axes[1].scatter(X[:, idx1], X[:, idx2], c='blue', s=50, edgecolors='white')\n",
    "        axes[1].set_xlabel(feat_1)\n",
    "        axes[1].set_ylabel(feat_2)\n",
    "        axes[1].set_title('GP Uncertainty')\n",
    "        plt.colorbar(cs2, ax=axes[1])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.output_dir / f'gp_2d_{feat_1}_{feat_2}.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671258e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# ExperimentSuggester\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "class ExperimentSuggester:\n",
    "    \"\"\"Formats and exports experiment suggestions.\"\"\"\n",
    "    \n",
    "    def __init__(self, search_space: SearchSpace, config: Phase2Config):\n",
    "        self.search_space = search_space\n",
    "        self.config = config\n",
    "    \n",
    "    def format_suggestions(self, suggested_X: np.ndarray, ei_values: np.ndarray,\n",
    "                           feature_names: List[str], surrogate: SurrogateModel,\n",
    "                           binary_mappings: Dict) -> pd.DataFrame:\n",
    "        if len(suggested_X) == 0:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        mu, sigma = surrogate.predict(suggested_X, return_std=True)\n",
    "        \n",
    "        df = pd.DataFrame(suggested_X, columns=feature_names)\n",
    "        \n",
    "        # Reverse binary mappings to original values\n",
    "        for col in self.search_space.binary_features:\n",
    "            if col in binary_mappings:\n",
    "                reverse_map = {v: k for k, v in binary_mappings[col].items()}\n",
    "                df[col] = df[col].round().astype(int).map(reverse_map)\n",
    "            else:\n",
    "                df[col] = df[col].round().astype(int)\n",
    "        \n",
    "        # Round continuous features\n",
    "        for col in self.search_space.continuous_features:\n",
    "            df[col] = df[col].round(4)\n",
    "        \n",
    "        df['predicted_response'] = mu.round(4)\n",
    "        df['uncertainty'] = sigma.round(4)\n",
    "        df['acquisition_value'] = ei_values.round(6)\n",
    "        \n",
    "        df = df.sort_values('acquisition_value', ascending=False).reset_index(drop=True)\n",
    "        df.insert(0, 'rank', range(1, len(df) + 1))\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def export_csv(self, suggestions_df: pd.DataFrame, iteration: int) -> str:\n",
    "        output_path = Path(self.config.output_dir)\n",
    "        output_path.mkdir(exist_ok=True)\n",
    "        filename = output_path / f'suggestions_iter{iteration}.csv'\n",
    "        suggestions_df.to_csv(filename, index=False)\n",
    "        return str(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d77841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Results Container\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "@dataclass\n",
    "class Phase2Results:\n",
    "    \"\"\"Container for pipeline outputs.\"\"\"\n",
    "    iteration: int\n",
    "    suggestions: pd.DataFrame\n",
    "    current_best: float\n",
    "    previous_best: Optional[float]\n",
    "    improved: bool\n",
    "    should_continue: bool\n",
    "    no_improvement_count: int\n",
    "    model_metrics: Dict[str, float]\n",
    "    suggestions_file: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc993ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Main Pipeline\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "class Phase2Pipeline:\n",
    "    \"\"\"\n",
    "    Bayesian Optimization pipeline for chemical experiments.\n",
    "    \n",
    "    Workflow:\n",
    "    1. Load experimental data from Excel\n",
    "    2. Fit GP surrogate model\n",
    "    3. Optimize EI acquisition to suggest next experiments\n",
    "    4. Save checkpoint for next iteration\n",
    "    5. Repeat after lab runs suggested experiments\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: Phase2Config):\n",
    "        self.config = config\n",
    "        self._output_dir = Path(config.output_dir)\n",
    "        self._output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        self._loader = DataLoader()\n",
    "        self._checkpoint_mgr = Phase2Checkpoint(config.output_dir)\n",
    "        self._plotter = Phase2Plotter(config.output_dir)\n",
    "        self._tracker = ConvergenceTracker(config)\n",
    "        \n",
    "        self._X = None\n",
    "        self._y = None\n",
    "        self._feature_names = []\n",
    "        self._binary_features = []\n",
    "        self._continuous_features = []\n",
    "        self._binary_mappings = {}\n",
    "        self._search_space = None\n",
    "        self._surrogate = None\n",
    "        self._iteration = 0\n",
    "    \n",
    "    def run(self, data_file: str) -> Phase2Results:\n",
    "        \"\"\"Run one BO iteration.\"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"Phase 2: Bayesian Optimization\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Load checkpoint if continuing\n",
    "        previous_best = None\n",
    "        if not self.config.fresh_start and self._checkpoint_mgr.exists():\n",
    "            self._load_checkpoint()\n",
    "            previous_best = self._tracker.best_response\n",
    "            print(f\"  Loaded checkpoint (iteration {self._iteration})\")\n",
    "        \n",
    "        self._load_data(data_file)\n",
    "        self._iteration += 1\n",
    "        \n",
    "        print(f\"\\n  Iteration: {self._iteration}\")\n",
    "        print(f\"  Experiments: {len(self._X)}\")\n",
    "        \n",
    "        # Build search space\n",
    "        self._search_space = SearchSpace(\n",
    "            self._X, self._binary_features, self._continuous_features,\n",
    "            self.config.bounds_margin\n",
    "        )\n",
    "        \n",
    "        # Fit surrogate\n",
    "        self._fit_surrogate()\n",
    "        \n",
    "        # Find current best\n",
    "        if self.config.maximize_response:\n",
    "            current_best = self._y.max()\n",
    "        else:\n",
    "            current_best = self._y.min()\n",
    "        print(f\"  Current best: {current_best:.4f}\")\n",
    "        \n",
    "        # Generate suggestions\n",
    "        suggestions_df = self._generate_suggestions(current_best)\n",
    "        \n",
    "        # Export\n",
    "        suggester = ExperimentSuggester(self._search_space, self.config)\n",
    "        suggestions_file = suggester.export_csv(suggestions_df, self._iteration)\n",
    "        print(f\"  Suggestions saved: {suggestions_file}\")\n",
    "        \n",
    "        # Update tracker\n",
    "        self._tracker.record_iteration(self._iteration, current_best, len(self._X), suggestions_df)\n",
    "        \n",
    "        # Check improvement\n",
    "        improved = False\n",
    "        if previous_best is not None:\n",
    "            if self.config.maximize_response:\n",
    "                improved = current_best > previous_best * (1 + self.config.min_improvement)\n",
    "            else:\n",
    "                improved = current_best < previous_best * (1 - self.config.min_improvement)\n",
    "        \n",
    "        # Save checkpoint\n",
    "        self._save_checkpoint(current_best)\n",
    "        \n",
    "        # Generate plots\n",
    "        self._generate_plots()\n",
    "        \n",
    "        # Print status\n",
    "        self._print_status(current_best, previous_best, improved)\n",
    "        \n",
    "        return Phase2Results(\n",
    "            iteration=self._iteration,\n",
    "            suggestions=suggestions_df,\n",
    "            current_best=current_best,\n",
    "            previous_best=previous_best,\n",
    "            improved=improved,\n",
    "            should_continue=not self._tracker.should_stop,\n",
    "            no_improvement_count=self._tracker.no_improvement_count,\n",
    "            model_metrics=self._surrogate.score(self._X.values, self._y.values),\n",
    "            suggestions_file=suggestions_file\n",
    "        )\n",
    "    \n",
    "    def _load_data(self, data_file: str):\n",
    "        print(f\"\\n  Loading: {data_file}\")\n",
    "        \n",
    "        df = self._loader.load_excel(data_file, self.config.sheet_name, self.config.header_row)\n",
    "        \n",
    "        if self.config.split_keyword:\n",
    "            df, _ = self._loader.split_at_keyword(df, self.config.split_keyword)\n",
    "        \n",
    "        df = self._loader.clean_data(df)\n",
    "        \n",
    "        feature_cols = self._loader.get_feature_columns(\n",
    "            df, self.config.stop_feature, self.config.response_column\n",
    "        )\n",
    "        \n",
    "        self._binary_features, self._continuous_features, self._binary_mappings = \\\n",
    "            self._loader.classify_features(df, feature_cols)\n",
    "        \n",
    "        self._feature_names = self._continuous_features + self._binary_features\n",
    "        self._X = df[self._feature_names].copy()\n",
    "        self._y = df[self.config.response_column].copy()\n",
    "        \n",
    "        # Drop missing response\n",
    "        valid = ~self._y.isnull()\n",
    "        self._X = self._X[valid].reset_index(drop=True)\n",
    "        self._y = self._y[valid].reset_index(drop=True)\n",
    "        \n",
    "        print(f\"  Features: {len(self._feature_names)} \"\n",
    "              f\"({len(self._continuous_features)} cont, {len(self._binary_features)} bin)\")\n",
    "    \n",
    "    def _fit_surrogate(self):\n",
    "        print(\"\\n  Fitting GP model...\")\n",
    "        self._surrogate = SurrogateModel()\n",
    "        self._surrogate.fit(self._X.values, self._y.values)\n",
    "        metrics = self._surrogate.score(self._X.values, self._y.values)\n",
    "        print(f\"  R²: {metrics['r2']:.4f}, RMSE: {metrics['rmse']:.4f}\")\n",
    "    \n",
    "    def _generate_suggestions(self, current_best: float) -> pd.DataFrame:\n",
    "        print(f\"\\n  Generating {self.config.n_suggestions} suggestions ({self.config.selection_strategy})...\")\n",
    "        \n",
    "        optimizer = AcquisitionOptimizer(self._surrogate, self._search_space, self.config)\n",
    "        optimizer.set_best_y(current_best)\n",
    "        \n",
    "        suggested_X, ei_values = optimizer.find_next_points(\n",
    "            self.config.n_suggestions, self._X.values\n",
    "        )\n",
    "        \n",
    "        suggester = ExperimentSuggester(self._search_space, self.config)\n",
    "        return suggester.format_suggestions(\n",
    "            suggested_X, ei_values, self._feature_names,\n",
    "            self._surrogate, self._binary_mappings\n",
    "        )\n",
    "    \n",
    "    def _save_checkpoint(self, current_best: float):\n",
    "        state = {\n",
    "            'iteration': self._iteration,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'n_experiments': len(self._X),\n",
    "            'best_response': current_best,\n",
    "            'no_improvement_count': self._tracker.no_improvement_count,\n",
    "            'should_stop': self._tracker.should_stop,\n",
    "            'features': self._feature_names,\n",
    "            'binary_features': self._binary_features,\n",
    "            'continuous_features': self._continuous_features,\n",
    "            'binary_mappings': self._binary_mappings,\n",
    "            'history': self._tracker.history\n",
    "        }\n",
    "        self._checkpoint_mgr.save(state)\n",
    "    \n",
    "    def _load_checkpoint(self):\n",
    "        state = self._checkpoint_mgr.load()\n",
    "        if state:\n",
    "            self._iteration = state['iteration']\n",
    "            self._tracker.load_history(state.get('history', []))\n",
    "    \n",
    "    def _generate_plots(self):\n",
    "        print(\"\\n  Generating plots...\")\n",
    "        \n",
    "        if len(self._tracker.history) > 0:\n",
    "            self._plotter.plot_convergence(self._tracker, self.config)\n",
    "        \n",
    "        # 1D plots for first 3 continuous features\n",
    "        for i, feat in enumerate(self._continuous_features[:3]):\n",
    "            feat_idx = self._feature_names.index(feat)\n",
    "            self._plotter.plot_gp_1d(\n",
    "                self._surrogate, self._X.values, self._y.values,\n",
    "                self._feature_names, feat_idx, self._search_space\n",
    "            )\n",
    "        \n",
    "        # 2D plot if 2+ continuous features\n",
    "        if len(self._continuous_features) >= 2:\n",
    "            idx1 = self._feature_names.index(self._continuous_features[0])\n",
    "            idx2 = self._feature_names.index(self._continuous_features[1])\n",
    "            self._plotter.plot_gp_2d(\n",
    "                self._surrogate, self._X.values, self._y.values,\n",
    "                self._feature_names, idx1, idx2, self._search_space\n",
    "            )\n",
    "    \n",
    "    def _print_status(self, current_best: float, previous_best: Optional[float], improved: bool):\n",
    "        print(\"\\n\" + \"-\" * 60)\n",
    "        print(\"Status\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        if previous_best is not None:\n",
    "            arrow = \"↑\" if improved else \"→\"\n",
    "            print(f\"  Best: {previous_best:.4f} {arrow} {current_best:.4f}\")\n",
    "            print(f\"  Improved: {'Yes' if improved else 'No'}\")\n",
    "        else:\n",
    "            print(f\"  Best: {current_best:.4f} (first iteration)\")\n",
    "        \n",
    "        print(f\"  No improvement: {self._tracker.no_improvement_count}/{self.config.patience}\")\n",
    "        \n",
    "        if self._tracker.should_stop:\n",
    "            print(\"\\n  ⚠ STOPPING CRITERION MET\")\n",
    "        else:\n",
    "            print(\"\\n  ✓ Continue with suggested experiments\")\n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Clear checkpoint and start fresh.\"\"\"\n",
    "        self._checkpoint_mgr.clear()\n",
    "        self._tracker = ConvergenceTracker(self.config)\n",
    "        self._iteration = 0\n",
    "        print(\"  Pipeline reset.\")\n",
    "    \n",
    "    @property\n",
    "    def iteration(self) -> int:\n",
    "        return self._iteration\n",
    "    \n",
    "    @property\n",
    "    def history(self) -> pd.DataFrame:\n",
    "        return self._tracker.get_history_df()\n",
    "    \n",
    "    @property\n",
    "    def bounds(self) -> pd.DataFrame:\n",
    "        if self._search_space:\n",
    "            return self._search_space.get_bounds_df()\n",
    "        return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a076f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Convenience Function\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def run_bo_iteration(data_file: str, response_column: str, \n",
    "                     n_suggestions: int = 5, maximize: bool = True,\n",
    "                     strategy: str = 'diverse', output_dir: str = 'bo_results',\n",
    "                     **kwargs) -> Phase2Results:\n",
    "    \"\"\"Quick single-iteration BO run.\"\"\"\n",
    "    config = Phase2Config(\n",
    "        response_column=response_column,\n",
    "        n_suggestions=n_suggestions,\n",
    "        maximize_response=maximize,\n",
    "        selection_strategy=strategy,\n",
    "        output_dir=output_dir,\n",
    "        **kwargs\n",
    "    )\n",
    "    return Phase2Pipeline(config).run(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdd4fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Example Usage\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "config = Phase2Config(\n",
    "    response_column='Yield',\n",
    "    sheet_name='data',\n",
    "    header_row=5,\n",
    "    stop_feature='Batch ID',\n",
    "    maximize_response=True,\n",
    "    n_suggestions=5,\n",
    "    selection_strategy='diverse',\n",
    "    patience=3,\n",
    "    output_dir='bo_phase2_output'\n",
    ")\n",
    "\n",
    "pipeline = Phase2Pipeline(config)\n",
    "results = pipeline.run('experiments.xlsx')\n",
    "\n",
    "print(\"\\nSuggested Experiments:\")\n",
    "print(results.suggestions.to_string(index=False))\n",
    "print(f\"\\nContinue: {results.should_continue}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
