{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b40089",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Bayesian Optimization Pipeline - Phase 2 (Enhanced)\n",
    "====================================================\n",
    "Iterative Bayesian Optimization for Chemical Experiments\n",
    "With Range Optimization and Discrete Feature Support\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Optional, Tuple, Any, Union\n",
    "from itertools import product\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel, Matern, WhiteKernel\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00df8728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Configuration\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "@dataclass\n",
    "class Phase2Config:\n",
    "    \"\"\"Phase 2 configuration parameters.\"\"\"\n",
    "    \n",
    "    # Data\n",
    "    response_column: str\n",
    "    sheet_name: str = 'data'\n",
    "    header_row: int = 5\n",
    "    split_keyword: Optional[str] = \"PREDICTED OPTIMUM RUNS\"\n",
    "    stop_feature: Optional[str] = \"Batch ID\"\n",
    "    \n",
    "    # Optimization Mode\n",
    "    # Set maximize_response=True for maximization, False for minimization\n",
    "    # Set target_range=(lower, upper) for range optimization (overrides maximize_response)\n",
    "    maximize_response: bool = True\n",
    "    target_range: Optional[Tuple[float, float]] = None  # NEW: (lower_bound, upper_bound)\n",
    "    \n",
    "    # Discrete Features Configuration\n",
    "    # Dictionary mapping feature names to list of allowed values\n",
    "    # Example: {'Temperature': [100, 150, 200], 'Catalyst': ['A', 'B', 'C']}\n",
    "    discrete_features: Dict[str, List[Any]] = field(default_factory=dict)  # NEW\n",
    "    \n",
    "    # Suggestions\n",
    "    n_suggestions: int = 5\n",
    "    selection_strategy: str = 'diverse'  # 'diverse' hedges bets; 'greedy' exploits best region\n",
    "    min_distance: float = 0.1  # Normalized distance; prevents clustered suggestions\n",
    "    \n",
    "    # Acquisition function\n",
    "    exploration_weight: float = 0.01  # xi parameter; small value favors exploitation\n",
    "    \n",
    "    # Search space\n",
    "    # Margin for continuous features (allows mild extrapolation beyond observed data)\n",
    "    bounds_margin: float = 0.1  # Now customizable as requested\n",
    "    \n",
    "    # Optimizer\n",
    "    n_optimizer_restarts: int = 25\n",
    "    \n",
    "    # Stopping\n",
    "    patience: int = 3\n",
    "    min_improvement: float = 0.01  # 1% relative improvement threshold\n",
    "    \n",
    "    # Duplicate prevention\n",
    "    duplicate_threshold: float = 0.05\n",
    "    \n",
    "    # Output\n",
    "    output_dir: str = 'bo_phase2_output'\n",
    "    fresh_start: bool = False\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Validate configuration.\"\"\"\n",
    "        if self.target_range is not None:\n",
    "            if len(self.target_range) != 2:\n",
    "                raise ValueError(\"target_range must be a tuple of (lower, upper)\")\n",
    "            if self.target_range[0] >= self.target_range[1]:\n",
    "                raise ValueError(\"target_range lower bound must be less than upper bound\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1949e7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# DataLoader\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "class DataLoader:\n",
    "    \"\"\"Handles data loading and feature classification.\"\"\"\n",
    "    \n",
    "    def load_excel(self, file_path: str, sheet_name: str, header_row: int) -> pd.DataFrame:\n",
    "        xls = pd.ExcelFile(file_path, engine='openpyxl')\n",
    "        return pd.read_excel(xls, sheet_name=sheet_name, header=header_row)\n",
    "    \n",
    "    def split_at_keyword(self, df: pd.DataFrame, keyword: str, \n",
    "                         column: str = 'Run') -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        split_index = df.index[df[column] == keyword].tolist()\n",
    "        if split_index:\n",
    "            idx = split_index[0]\n",
    "            return df.iloc[:idx], df.iloc[idx+1:]\n",
    "        return df.copy(), pd.DataFrame()\n",
    "    \n",
    "    def clean_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df.drop(index=0, errors='ignore')\n",
    "        df = df.dropna(how='all')\n",
    "        return df.reset_index(drop=True)\n",
    "    \n",
    "    def classify_features(self, df: pd.DataFrame, feature_cols: List[str],\n",
    "                         discrete_config: Dict[str, List[Any]]\n",
    "                         ) -> Tuple[List[str], List[str], Dict[str, List[Any]]]:\n",
    "        \"\"\"\n",
    "        Classify features as discrete or continuous.\n",
    "        \n",
    "        NEW: Instead of binary detection, uses user-provided discrete_config\n",
    "        to determine which features are discrete and their allowed values.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with features\n",
    "            feature_cols: List of feature column names\n",
    "            discrete_config: Dict mapping feature names to allowed values\n",
    "            \n",
    "        Returns:\n",
    "            discrete_cols: List of discrete feature names\n",
    "            continuous_cols: List of continuous feature names\n",
    "            discrete_values: Dict mapping discrete features to their allowed values\n",
    "        \"\"\"\n",
    "        numeric_features = df[feature_cols].select_dtypes(include=[np.number]).columns.tolist()\n",
    "        \n",
    "        discrete_cols = []\n",
    "        continuous_cols = []\n",
    "        discrete_values = {}\n",
    "        \n",
    "        for col in numeric_features:\n",
    "            if col in discrete_config:\n",
    "                # User specified this as discrete\n",
    "                discrete_cols.append(col)\n",
    "                discrete_values[col] = discrete_config[col]\n",
    "                \n",
    "                # Validate that observed data contains only allowed values (warning only)\n",
    "                observed_values = set(df[col].dropna().unique())\n",
    "                allowed_values = set(discrete_config[col])\n",
    "                unexpected = observed_values - allowed_values\n",
    "                if unexpected:\n",
    "                    print(f\"  Warning: Feature '{col}' has observed values {unexpected} \"\n",
    "                          f\"not in allowed list {discrete_config[col]}\")\n",
    "            else:\n",
    "                continuous_cols.append(col)\n",
    "        \n",
    "        # Handle non-numeric discrete features (e.g., categorical strings)\n",
    "        for col in feature_cols:\n",
    "            if col in discrete_config and col not in numeric_features:\n",
    "                discrete_cols.append(col)\n",
    "                discrete_values[col] = discrete_config[col]\n",
    "                # Create numeric encoding for non-numeric discrete features\n",
    "                df[col + '_encoded'] = df[col].map(\n",
    "                    {v: i for i, v in enumerate(discrete_config[col])}\n",
    "                )\n",
    "        \n",
    "        return discrete_cols, continuous_cols, discrete_values\n",
    "    \n",
    "    def get_feature_columns(self, df: pd.DataFrame, stop_feature: Optional[str], \n",
    "                           response_column: str) -> List[str]:\n",
    "        columns = df.columns.tolist()\n",
    "        \n",
    "        if stop_feature and stop_feature in columns:\n",
    "            feature_list = columns[:columns.index(stop_feature)]\n",
    "        else:\n",
    "            feature_list = [c for c in columns if c != response_column]\n",
    "        \n",
    "        # Remove index-like columns\n",
    "        return [f for f in feature_list if f.lower() not in ['run', 'index', 'unnamed: 0']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febab38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# SearchSpace\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "class SearchSpace:\n",
    "    \"\"\"\n",
    "    Manages bounds and discrete feature enumeration.\n",
    "    \n",
    "    CHANGED: Now handles discrete features with user-specified allowed values\n",
    "    instead of just binary features.\n",
    "    \n",
    "    Discrete features are enumerated because:\n",
    "    - GPs struggle with discontinuous functions\n",
    "    - Categorical boundaries are sharp in chemistry\n",
    "    - Enumeration is tractable for typical discrete feature combinations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, X: pd.DataFrame, discrete_features: List[str],\n",
    "                 discrete_values: Dict[str, List[Any]],\n",
    "                 continuous_features: List[str], bounds_margin: float = 0.1):\n",
    "        self.discrete_features = discrete_features\n",
    "        self.discrete_values = discrete_values\n",
    "        self.continuous_features = continuous_features\n",
    "        \n",
    "        self._bounds = {}\n",
    "        self._discrete_combinations = []\n",
    "        \n",
    "        # Compute continuous bounds with margin for extrapolation\n",
    "        for col in continuous_features:\n",
    "            col_min, col_max = X[col].min(), X[col].max()\n",
    "            margin = (col_max - col_min) * bounds_margin\n",
    "            self._bounds[col] = {\n",
    "                'min': col_min - margin, \n",
    "                'max': col_max + margin,\n",
    "                'observed_min': col_min, \n",
    "                'observed_max': col_max, \n",
    "                'type': 'continuous'\n",
    "            }\n",
    "        \n",
    "        # Store discrete feature bounds (for reference)\n",
    "        for col in discrete_features:\n",
    "            allowed = discrete_values[col]\n",
    "            self._bounds[col] = {\n",
    "                'allowed_values': allowed,\n",
    "                'n_levels': len(allowed),\n",
    "                'type': 'discrete'\n",
    "            }\n",
    "        \n",
    "        # Enumerate all discrete combinations\n",
    "        if discrete_features:\n",
    "            value_lists = [discrete_values[col] for col in discrete_features]\n",
    "            combinations = list(product(*value_lists))\n",
    "            self._discrete_combinations = [\n",
    "                dict(zip(discrete_features, combo)) for combo in combinations\n",
    "            ]\n",
    "        else:\n",
    "            self._discrete_combinations = [{}]\n",
    "    \n",
    "    def get_continuous_bounds(self) -> List[Tuple[float, float]]:\n",
    "        return [(self._bounds[col]['min'], self._bounds[col]['max']) \n",
    "                for col in self.continuous_features]\n",
    "    \n",
    "    def get_bounds_df(self) -> pd.DataFrame:\n",
    "        rows = []\n",
    "        for col, bounds in self._bounds.items():\n",
    "            if bounds['type'] == 'continuous':\n",
    "                rows.append({\n",
    "                    'feature': col,\n",
    "                    'type': 'continuous',\n",
    "                    'min': bounds['min'],\n",
    "                    'max': bounds['max'],\n",
    "                    'observed_min': bounds['observed_min'],\n",
    "                    'observed_max': bounds['observed_max']\n",
    "                })\n",
    "            else:\n",
    "                rows.append({\n",
    "                    'feature': col,\n",
    "                    'type': 'discrete',\n",
    "                    'allowed_values': str(bounds['allowed_values']),\n",
    "                    'n_levels': bounds['n_levels']\n",
    "                })\n",
    "        return pd.DataFrame(rows)\n",
    "    \n",
    "    @property\n",
    "    def bounds(self) -> Dict:\n",
    "        return self._bounds\n",
    "    \n",
    "    @property\n",
    "    def discrete_combinations(self) -> List[Dict]:\n",
    "        return self._discrete_combinations\n",
    "    \n",
    "    @property\n",
    "    def n_discrete_combinations(self) -> int:\n",
    "        return len(self._discrete_combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73544262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# SurrogateModel\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "class SurrogateModel:\n",
    "    \"\"\"\n",
    "    Gaussian Process surrogate model.\n",
    "    \n",
    "    Design choices:\n",
    "    - Matern 5/2 kernel: twice differentiable, handles non-smooth responses\n",
    "      better than RBF while remaining smooth enough for optimization\n",
    "    - StandardScaler on X and y: improves numerical stability and \n",
    "      makes length scale interpretation consistent across features\n",
    "    - WhiteKernel: explicitly models observation noise\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.scaler_X = StandardScaler()\n",
    "        self.scaler_y = StandardScaler()\n",
    "        self._is_fitted = False\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        X_scaled = self.scaler_X.fit_transform(X)\n",
    "        y_scaled = self.scaler_y.fit_transform(y.reshape(-1, 1)).ravel()\n",
    "        \n",
    "        # Matern 5/2: good default for physical processes\n",
    "        kernel = (\n",
    "            ConstantKernel(1.0, (1e-3, 1e3)) * \n",
    "            Matern(length_scale=np.ones(X.shape[1]), \n",
    "                   length_scale_bounds=(1e-3, 1e3), nu=2.5) +\n",
    "            WhiteKernel(noise_level=0.1, noise_level_bounds=(1e-5, 1e1))\n",
    "        )\n",
    "        \n",
    "        self.model = GaussianProcessRegressor(\n",
    "            kernel=kernel, n_restarts_optimizer=10,\n",
    "            normalize_y=False, random_state=RANDOM_STATE\n",
    "        )\n",
    "        self.model.fit(X_scaled, y_scaled)\n",
    "        self._is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X: np.ndarray, return_std: bool = True):\n",
    "        X_scaled = self.scaler_X.transform(X)\n",
    "        \n",
    "        if return_std:\n",
    "            mu_scaled, sigma_scaled = self.model.predict(X_scaled, return_std=True)\n",
    "            mu = self.scaler_y.inverse_transform(mu_scaled.reshape(-1, 1)).ravel()\n",
    "            sigma = sigma_scaled * self.scaler_y.scale_[0]\n",
    "            return mu, sigma\n",
    "        else:\n",
    "            mu_scaled = self.model.predict(X_scaled)\n",
    "            return self.scaler_y.inverse_transform(mu_scaled.reshape(-1, 1)).ravel()\n",
    "    \n",
    "    def score(self, X: np.ndarray, y: np.ndarray) -> Dict[str, float]:\n",
    "        y_pred = self.predict(X, return_std=False)\n",
    "        return {\n",
    "            'r2': r2_score(y, y_pred),\n",
    "            'rmse': np.sqrt(mean_squared_error(y, y_pred))\n",
    "        }\n",
    "    \n",
    "    @property\n",
    "    def is_fitted(self) -> bool:\n",
    "        return self._is_fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cacca37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# AcquisitionOptimizer\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "class AcquisitionOptimizer:\n",
    "    \"\"\"\n",
    "    Optimizes acquisition function for experiment suggestions.\n",
    "    \n",
    "    NEW: Supports three optimization modes:\n",
    "    1. Maximization (maximize_response=True, target_range=None)\n",
    "    2. Minimization (maximize_response=False, target_range=None)\n",
    "    3. Range optimization (target_range=(lower, upper))\n",
    "    \n",
    "    For range optimization, uses Expected Improvement for Target (EIT)\n",
    "    which rewards predictions that fall within the target range.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, surrogate: SurrogateModel, search_space: SearchSpace,\n",
    "                 config: Phase2Config):\n",
    "        self.surrogate = surrogate\n",
    "        self.search_space = search_space\n",
    "        self.config = config\n",
    "        self._best_y = None\n",
    "    \n",
    "    def set_best_y(self, best_y: float):\n",
    "        self._best_y = float(best_y)\n",
    "    \n",
    "    def expected_improvement(self, X: np.ndarray, xi: float = 0.01) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Standard EI for maximization/minimization.\n",
    "        EI(x) = (mu - f_best - xi) * Phi(Z) + sigma * phi(Z)\n",
    "        \"\"\"\n",
    "        X = np.array(X, dtype=np.float64)\n",
    "        mu, sigma = self.surrogate.predict(X, return_std=True)\n",
    "        mu = np.array(mu, dtype=np.float64)\n",
    "        sigma = np.array(sigma, dtype=np.float64)\n",
    "        sigma = np.maximum(sigma, 1e-9)\n",
    "        \n",
    "        if self.config.maximize_response:\n",
    "            improvement = mu - self._best_y - xi\n",
    "        else:\n",
    "            improvement = self._best_y - mu - xi\n",
    "        \n",
    "        Z = improvement / sigma\n",
    "        ei = improvement * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "        ei[sigma < 1e-9] = 0.0\n",
    "        return ei\n",
    "    \n",
    "    def expected_improvement_for_range(self, X: np.ndarray, xi: float = 0.01) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        NEW: Expected Improvement for Target Range (EIT).\n",
    "        \n",
    "        Computes the probability-weighted expected \"closeness\" to the target range.\n",
    "        \n",
    "        For a target range [L, U]:\n",
    "        - If prediction is below L: reward for moving toward L\n",
    "        - If prediction is above U: reward for moving toward U  \n",
    "        - If prediction is in [L, U]: reward based on probability of being in range\n",
    "        \n",
    "        This formulation:\n",
    "        EIT(x) = P(L <= y <= U | x) + alpha * (phi(Z_L) + phi(Z_U)) * sigma\n",
    "        \n",
    "        Where the second term encourages exploration at the boundaries.\n",
    "        \"\"\"\n",
    "        X = np.array(X, dtype=np.float64)\n",
    "        mu, sigma = self.surrogate.predict(X, return_std=True)\n",
    "        mu = np.array(mu, dtype=np.float64)\n",
    "        sigma = np.array(sigma, dtype=np.float64)\n",
    "        sigma = np.maximum(sigma, 1e-9)\n",
    "        \n",
    "        L, U = self.config.target_range\n",
    "        \n",
    "        # Standardized distances to bounds\n",
    "        Z_L = (L - mu) / sigma  # Distance to lower bound\n",
    "        Z_U = (U - mu) / sigma  # Distance to upper bound\n",
    "        \n",
    "        # Probability of being in range: P(L <= y <= U)\n",
    "        prob_in_range = norm.cdf(Z_U) - norm.cdf(Z_L)\n",
    "        \n",
    "        # Expected improvement toward range\n",
    "        # If below range: expected improvement toward L\n",
    "        # If above range: expected improvement toward U\n",
    "        \n",
    "        # Component 1: Probability of being in the target range\n",
    "        eit = prob_in_range.copy()\n",
    "        \n",
    "        # Component 2: For points outside range, add expected movement toward range\n",
    "        # Below range: encourage moving up toward L\n",
    "        below_range = mu < L\n",
    "        if np.any(below_range):\n",
    "            improvement_to_L = (L - mu[below_range] - xi)\n",
    "            Z_improve = improvement_to_L / sigma[below_range]\n",
    "            ei_below = improvement_to_L * norm.cdf(Z_improve) + sigma[below_range] * norm.pdf(Z_improve)\n",
    "            # Scale to be comparable with probability\n",
    "            ei_below = ei_below / (U - L + 1e-9)\n",
    "            eit[below_range] += ei_below * (1 - prob_in_range[below_range])\n",
    "        \n",
    "        # Above range: encourage moving down toward U\n",
    "        above_range = mu > U\n",
    "        if np.any(above_range):\n",
    "            improvement_to_U = (mu[above_range] - U - xi)\n",
    "            Z_improve = improvement_to_U / sigma[above_range]\n",
    "            ei_above = improvement_to_U * norm.cdf(Z_improve) + sigma[above_range] * norm.pdf(Z_improve)\n",
    "            # Scale to be comparable with probability\n",
    "            ei_above = ei_above / (U - L + 1e-9)\n",
    "            eit[above_range] += ei_above * (1 - prob_in_range[above_range])\n",
    "        \n",
    "        # Component 3: Exploration bonus at boundaries (small)\n",
    "        exploration_bonus = xi * (norm.pdf(Z_L) + norm.pdf(Z_U)) * sigma / (U - L + 1e-9)\n",
    "        eit += exploration_bonus\n",
    "        \n",
    "        eit[sigma < 1e-9] = 0.0\n",
    "        return eit\n",
    "    \n",
    "    def acquisition_function(self, X: np.ndarray, xi: float = 0.01) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        NEW: Unified acquisition function that selects appropriate method\n",
    "        based on configuration.\n",
    "        \"\"\"\n",
    "        if self.config.target_range is not None:\n",
    "            return self.expected_improvement_for_range(X, xi)\n",
    "        else:\n",
    "            return self.expected_improvement(X, xi)\n",
    "    \n",
    "    def _construct_full_x(self, continuous_x: np.ndarray, discrete_values: Dict) -> np.ndarray:\n",
    "        \"\"\"Combine continuous values with fixed discrete values.\"\"\"\n",
    "        full_x = []\n",
    "        cont_idx = 0\n",
    "        for col in self.search_space.continuous_features + self.search_space.discrete_features:\n",
    "            if col in self.search_space.discrete_features:\n",
    "                full_x.append(float(discrete_values[col]))\n",
    "            else:\n",
    "                full_x.append(float(continuous_x[cont_idx]))\n",
    "                cont_idx += 1\n",
    "        return np.array(full_x, dtype=np.float64)\n",
    "    \n",
    "    def _negative_acq(self, x: np.ndarray, discrete_values: Dict) -> float:\n",
    "        \"\"\"Negative acquisition function for minimization.\"\"\"\n",
    "        x = np.array(x, dtype=np.float64)\n",
    "        full_x = self._construct_full_x(x, discrete_values)\n",
    "        acq = self.acquisition_function(full_x.reshape(1, -1), xi=self.config.exploration_weight)\n",
    "        return -float(acq[0])\n",
    "    \n",
    "    def _compute_distance(self, point: np.ndarray, reference_points: np.ndarray) -> float:\n",
    "        \"\"\"Compute minimum distance from point to any reference point.\"\"\"\n",
    "        point = np.array(point, dtype=np.float64).flatten()\n",
    "        reference_points = np.array(reference_points, dtype=np.float64)\n",
    "        \n",
    "        if reference_points.size == 0:\n",
    "            return float('inf')\n",
    "        \n",
    "        if reference_points.ndim == 1:\n",
    "            reference_points = reference_points.reshape(1, -1)\n",
    "        \n",
    "        diff = reference_points - point\n",
    "        distances = np.sqrt(np.sum(diff ** 2, axis=1))\n",
    "        return float(np.min(distances))\n",
    "    \n",
    "    def optimize_single(self, discrete_values: Dict, \n",
    "                        existing_points: Optional[np.ndarray] = None) -> Tuple[Optional[np.ndarray], float]:\n",
    "        \"\"\"Find best point for given discrete combination using multi-start L-BFGS-B.\"\"\"\n",
    "        bounds = self.search_space.get_continuous_bounds()\n",
    "        \n",
    "        # Handle case with no continuous features\n",
    "        if not bounds:\n",
    "            full_x = self._construct_full_x(np.array([]), discrete_values)\n",
    "            acq = self.acquisition_function(full_x.reshape(1, -1), xi=self.config.exploration_weight)[0]\n",
    "            return full_x, float(acq)\n",
    "        \n",
    "        # Convert bounds to float\n",
    "        bounds = [(float(b[0]), float(b[1])) for b in bounds]\n",
    "        \n",
    "        best_x = None\n",
    "        best_acq = float('-inf')\n",
    "        \n",
    "        for _ in range(self.config.n_optimizer_restarts):\n",
    "            x0 = np.array([np.random.uniform(b[0], b[1]) for b in bounds], dtype=np.float64)\n",
    "            \n",
    "            try:\n",
    "                result = minimize(\n",
    "                    lambda x: self._negative_acq(x, discrete_values),\n",
    "                    x0, method='L-BFGS-B', bounds=bounds\n",
    "                )\n",
    "                \n",
    "                candidate_acq = -float(result.fun)\n",
    "                \n",
    "                if candidate_acq > best_acq:\n",
    "                    candidate_x = self._construct_full_x(result.x, discrete_values)\n",
    "                    \n",
    "                    # Enforce diversity constraint\n",
    "                    if existing_points is not None and existing_points.size > 0:\n",
    "                        min_dist = self._compute_distance(candidate_x, existing_points)\n",
    "                        if min_dist < self.config.min_distance:\n",
    "                            continue\n",
    "                    \n",
    "                    best_x = candidate_x\n",
    "                    best_acq = candidate_acq\n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        return best_x, best_acq\n",
    "    \n",
    "    def find_next_points(self, n_points: int, existing_X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Find n_points suggestions using configured strategy.\"\"\"\n",
    "        # Ensure proper array format\n",
    "        if existing_X is not None and existing_X.size > 0:\n",
    "            all_points = np.array(existing_X, dtype=np.float64).copy()\n",
    "            if all_points.ndim == 1:\n",
    "                all_points = all_points.reshape(1, -1)\n",
    "        else:\n",
    "            all_points = None\n",
    "        \n",
    "        if self.config.selection_strategy == 'greedy':\n",
    "            suggestions, acq_values = self._greedy_selection(n_points, all_points)\n",
    "        else:\n",
    "            suggestions, acq_values = self._diverse_selection(n_points, all_points)\n",
    "        \n",
    "        # Fallback if no suggestions generated\n",
    "        if len(suggestions) == 0:\n",
    "            print(\"  Warning: Acquisition optimization failed. Using uncertainty-based exploration.\")\n",
    "            suggestions, acq_values = self._fallback_selection(n_points, all_points)\n",
    "        \n",
    "        return suggestions, acq_values\n",
    "    \n",
    "    def _greedy_selection(self, n_points: int, existing_points: Optional[np.ndarray]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Select top n points by acquisition value.\"\"\"\n",
    "        candidates = []\n",
    "        \n",
    "        for discrete_combo in self.search_space.discrete_combinations:\n",
    "            best_x, best_acq = self.optimize_single(discrete_combo, existing_points)\n",
    "            if best_x is not None:\n",
    "                candidates.append((np.array(best_x, dtype=np.float64), float(best_acq)))\n",
    "        \n",
    "        # Sort by acquisition value (descending)\n",
    "        candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        suggestions, acq_values = [], []\n",
    "        \n",
    "        for x, acq in candidates:\n",
    "            # Check duplicate threshold\n",
    "            if existing_points is not None and existing_points.size > 0:\n",
    "                min_dist = self._compute_distance(x, existing_points)\n",
    "                if min_dist < self.config.duplicate_threshold:\n",
    "                    continue\n",
    "            \n",
    "            suggestions.append(x.tolist())\n",
    "            acq_values.append(acq)\n",
    "            \n",
    "            if len(suggestions) >= n_points:\n",
    "                break\n",
    "        \n",
    "        # Return empty arrays with correct shape if no suggestions\n",
    "        n_features = len(self.search_space.continuous_features) + len(self.search_space.discrete_features)\n",
    "        if len(suggestions) == 0:\n",
    "            return np.array([]).reshape(0, n_features), np.array([])\n",
    "        \n",
    "        return np.array(suggestions, dtype=np.float64), np.array(acq_values, dtype=np.float64)\n",
    "    \n",
    "    def _diverse_selection(self, n_points: int, existing_points: Optional[np.ndarray]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Distribute suggestions across discrete combinations with spacing.\"\"\"\n",
    "        suggestions, acq_values = [], []\n",
    "        \n",
    "        # Initialize points to avoid\n",
    "        if existing_points is not None and existing_points.size > 0:\n",
    "            points_to_avoid = np.array(existing_points, dtype=np.float64).copy()\n",
    "            if points_to_avoid.ndim == 1:\n",
    "                points_to_avoid = points_to_avoid.reshape(1, -1)\n",
    "        else:\n",
    "            points_to_avoid = None\n",
    "        \n",
    "        n_combos = self.search_space.n_discrete_combinations\n",
    "        combo_counts = {i: 0 for i in range(n_combos)}\n",
    "        \n",
    "        for _ in range(n_points):\n",
    "            # Round-robin across discrete combinations\n",
    "            combo_idx = min(combo_counts, key=combo_counts.get)\n",
    "            discrete_combo = self.search_space.discrete_combinations[combo_idx]\n",
    "            \n",
    "            best_x, best_acq = self.optimize_single(discrete_combo, points_to_avoid)\n",
    "            \n",
    "            if best_x is not None:\n",
    "                best_x = np.array(best_x, dtype=np.float64)\n",
    "                suggestions.append(best_x.tolist())\n",
    "                acq_values.append(float(best_acq))\n",
    "                combo_counts[combo_idx] += 1\n",
    "                \n",
    "                # Add to avoidance set for diversity\n",
    "                if points_to_avoid is not None:\n",
    "                    points_to_avoid = np.vstack([points_to_avoid, best_x.reshape(1, -1)])\n",
    "                else:\n",
    "                    points_to_avoid = best_x.reshape(1, -1)\n",
    "            else:\n",
    "                combo_counts[combo_idx] = float('inf')\n",
    "        \n",
    "        # Return empty arrays with correct shape if no suggestions\n",
    "        n_features = len(self.search_space.continuous_features) + len(self.search_space.discrete_features)\n",
    "        if len(suggestions) == 0:\n",
    "            return np.array([]).reshape(0, n_features), np.array([])\n",
    "        \n",
    "        return np.array(suggestions, dtype=np.float64), np.array(acq_values, dtype=np.float64)\n",
    "    \n",
    "    def _fallback_selection(self, n_points: int, existing_points: Optional[np.ndarray]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Fallback when acquisition optimization fails: sample points with highest uncertainty.\n",
    "        \"\"\"\n",
    "        n_candidates = 1000\n",
    "        candidates = []\n",
    "        \n",
    "        # Generate random candidates across search space\n",
    "        for discrete_combo in self.search_space.discrete_combinations:\n",
    "            n_per_combo = n_candidates // max(1, self.search_space.n_discrete_combinations)\n",
    "            \n",
    "            for _ in range(n_per_combo):\n",
    "                x = []\n",
    "                for col in self.search_space.continuous_features:\n",
    "                    bounds = self.search_space.bounds[col]\n",
    "                    low = float(bounds['min'])\n",
    "                    high = float(bounds['max'])\n",
    "                    x.append(np.random.uniform(low, high))\n",
    "                for col in self.search_space.discrete_features:\n",
    "                    x.append(float(discrete_combo[col]))\n",
    "                candidates.append(x)\n",
    "        \n",
    "        candidates = np.array(candidates, dtype=np.float64)\n",
    "        \n",
    "        # Get uncertainty for all candidates\n",
    "        _, sigma = self.surrogate.predict(candidates, return_std=True)\n",
    "        sigma = np.array(sigma, dtype=np.float64)\n",
    "        \n",
    "        # Sort by uncertainty (descending)\n",
    "        sorted_idx = np.argsort(sigma)[::-1]\n",
    "        \n",
    "        suggestions, uncertainty_values = [], []\n",
    "        \n",
    "        for idx in sorted_idx:\n",
    "            candidate = candidates[idx].copy()\n",
    "            \n",
    "            # Check not too close to existing data\n",
    "            if existing_points is not None and existing_points.size > 0:\n",
    "                min_dist = self._compute_distance(candidate, existing_points)\n",
    "                if min_dist < self.config.duplicate_threshold:\n",
    "                    continue\n",
    "            \n",
    "            # Check not too close to already suggested points\n",
    "            if len(suggestions) > 0:\n",
    "                suggestions_arr = np.array(suggestions, dtype=np.float64)\n",
    "                min_dist = self._compute_distance(candidate, suggestions_arr)\n",
    "                if min_dist < self.config.min_distance:\n",
    "                    continue\n",
    "            \n",
    "            suggestions.append(candidate.tolist())\n",
    "            uncertainty_values.append(float(sigma[idx]))\n",
    "            \n",
    "            if len(suggestions) >= n_points:\n",
    "                break\n",
    "        \n",
    "        n_features = len(self.search_space.continuous_features) + len(self.search_space.discrete_features)\n",
    "        if len(suggestions) == 0:\n",
    "            return np.array([]).reshape(0, n_features), np.array([])\n",
    "        \n",
    "        return np.array(suggestions, dtype=np.float64), np.array(uncertainty_values, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0033d840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# ConvergenceTracker\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "class ConvergenceTracker:\n",
    "    \"\"\"\n",
    "    Tracks optimization progress with patience-based stopping.\n",
    "    \n",
    "    NEW: For range optimization, tracks closeness to target range\n",
    "    instead of just best value.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: Phase2Config):\n",
    "        self.config = config\n",
    "        self.history = []\n",
    "        self.no_improvement_count = 0\n",
    "    \n",
    "    def load_history(self, history: List[Dict]):\n",
    "        self.history = history\n",
    "        self._update_no_improvement_count()\n",
    "    \n",
    "    def _compute_range_score(self, response: float) -> float:\n",
    "        \"\"\"\n",
    "        NEW: Compute how close a response is to the target range.\n",
    "        Returns 0 if in range, negative distance otherwise.\n",
    "        \"\"\"\n",
    "        if self.config.target_range is None:\n",
    "            return response if self.config.maximize_response else -response\n",
    "        \n",
    "        L, U = self.config.target_range\n",
    "        if L <= response <= U:\n",
    "            return 0.0  # Perfect - in range\n",
    "        elif response < L:\n",
    "            return -(L - response)  # Below range (negative)\n",
    "        else:\n",
    "            return -(response - U)  # Above range (negative)\n",
    "    \n",
    "    def _is_improvement(self, current: float, previous: float) -> bool:\n",
    "        \"\"\"Check if current is better than previous.\"\"\"\n",
    "        if self.config.target_range is not None:\n",
    "            # For range optimization: closer to 0 is better\n",
    "            current_score = self._compute_range_score(current)\n",
    "            previous_score = self._compute_range_score(previous)\n",
    "            return current_score > previous_score + self.config.min_improvement\n",
    "        else:\n",
    "            if self.config.maximize_response:\n",
    "                return current > previous * (1 + self.config.min_improvement)\n",
    "            else:\n",
    "                return current < previous * (1 - self.config.min_improvement)\n",
    "    \n",
    "    def _update_no_improvement_count(self):\n",
    "        if len(self.history) < 2:\n",
    "            self.no_improvement_count = 0\n",
    "            return\n",
    "        \n",
    "        count = 0\n",
    "        best_so_far = self.history[0]['best_response']\n",
    "        \n",
    "        for record in self.history[1:]:\n",
    "            if self._is_improvement(record['best_response'], best_so_far):\n",
    "                best_so_far = record['best_response']\n",
    "                count = 0\n",
    "            else:\n",
    "                count += 1\n",
    "        \n",
    "        self.no_improvement_count = count\n",
    "    \n",
    "    def record_iteration(self, iteration: int, best_response: float, \n",
    "                         n_experiments: int, suggestions: pd.DataFrame,\n",
    "                         in_range_count: int = 0, total_count: int = 0):\n",
    "        record = {\n",
    "            'iteration': iteration,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'best_response': best_response,\n",
    "            'n_experiments': n_experiments,\n",
    "            'n_suggestions': len(suggestions)\n",
    "        }\n",
    "        \n",
    "        # NEW: Add range-specific metrics\n",
    "        if self.config.target_range is not None:\n",
    "            record['in_range_count'] = in_range_count\n",
    "            record['in_range_pct'] = (in_range_count / total_count * 100) if total_count > 0 else 0\n",
    "            record['range_score'] = self._compute_range_score(best_response)\n",
    "        \n",
    "        if len(self.history) > 0:\n",
    "            prev_best = self.history[-1]['best_response']\n",
    "            improved = self._is_improvement(best_response, prev_best)\n",
    "            \n",
    "            if self.config.target_range is not None:\n",
    "                record['improvement'] = self._compute_range_score(best_response) - self._compute_range_score(prev_best)\n",
    "            else:\n",
    "                if self.config.maximize_response:\n",
    "                    record['improvement'] = (best_response - prev_best) / abs(prev_best) if prev_best != 0 else 0\n",
    "                else:\n",
    "                    record['improvement'] = (prev_best - best_response) / abs(prev_best) if prev_best != 0 else 0\n",
    "            \n",
    "            record['improved'] = improved\n",
    "            self.no_improvement_count = 0 if improved else self.no_improvement_count + 1\n",
    "        \n",
    "        self.history.append(record)\n",
    "    \n",
    "    @property\n",
    "    def should_stop(self) -> bool:\n",
    "        return self.no_improvement_count >= self.config.patience\n",
    "    \n",
    "    @property\n",
    "    def current_iteration(self) -> int:\n",
    "        return len(self.history)\n",
    "    \n",
    "    @property\n",
    "    def best_response(self) -> Optional[float]:\n",
    "        if not self.history:\n",
    "            return None\n",
    "        \n",
    "        if self.config.target_range is not None:\n",
    "            # For range optimization, return the response closest to range\n",
    "            scores = [(h['best_response'], self._compute_range_score(h['best_response'])) \n",
    "                      for h in self.history]\n",
    "            return max(scores, key=lambda x: x[1])[0]\n",
    "        else:\n",
    "            if self.config.maximize_response:\n",
    "                return max(h['best_response'] for h in self.history)\n",
    "            return min(h['best_response'] for h in self.history)\n",
    "    \n",
    "    def get_history_df(self) -> pd.DataFrame:\n",
    "        return pd.DataFrame(self.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6cf57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Checkpoint Manager\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "class Phase2Checkpoint:\n",
    "    \"\"\"Persists state between iterations.\"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir: str):\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        self.checkpoint_file = self.output_dir / 'phase2_checkpoint.pkl'\n",
    "        self.json_file = self.output_dir / 'phase2_checkpoint.json'\n",
    "    \n",
    "    def save(self, state: Dict[str, Any]):\n",
    "        with open(self.checkpoint_file, 'wb') as f:\n",
    "            pickle.dump(state, f)\n",
    "        \n",
    "        json_state = {k: v for k, v in state.items() \n",
    "                      if k not in ['history']}\n",
    "        with open(self.json_file, 'w') as f:\n",
    "            json.dump(json_state, f, indent=2, default=str)\n",
    "    \n",
    "    def load(self) -> Optional[Dict[str, Any]]:\n",
    "        if not self.checkpoint_file.exists():\n",
    "            return None\n",
    "        with open(self.checkpoint_file, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    \n",
    "    def exists(self) -> bool:\n",
    "        return self.checkpoint_file.exists()\n",
    "    \n",
    "    def clear(self):\n",
    "        if self.checkpoint_file.exists():\n",
    "            self.checkpoint_file.unlink()\n",
    "        if self.json_file.exists():\n",
    "            self.json_file.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f3e48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Plotter (Enhanced)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "class Phase2Plotter:\n",
    "    \"\"\"Generates diagnostic visualizations.\"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir: str):\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    def plot_convergence(self, tracker: ConvergenceTracker, config: Phase2Config):\n",
    "        history_df = tracker.get_history_df()\n",
    "        if len(history_df) < 1:\n",
    "            return\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(8, 5))\n",
    "        ax.plot(history_df['iteration'], history_df['best_response'], 'bo-', linewidth=2, markersize=8)\n",
    "        \n",
    "        # NEW: Add target range visualization\n",
    "        if config.target_range is not None:\n",
    "            L, U = config.target_range\n",
    "            ax.axhline(y=L, color='g', linestyle='--', alpha=0.7, label=f'Target Range [{L}, {U}]')\n",
    "            ax.axhline(y=U, color='g', linestyle='--', alpha=0.7)\n",
    "            ax.fill_between(history_df['iteration'], L, U, alpha=0.1, color='green')\n",
    "            ax.set_title('Optimization Progress (Target Range)')\n",
    "        else:\n",
    "            ax.set_title(f'Optimization Progress ({\"Max\" if config.maximize_response else \"Min\"})')\n",
    "        \n",
    "        ax.set_xlabel('Iteration')\n",
    "        ax.set_ylabel('Best Response')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.output_dir / 'convergence_plot.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_gp_1d(self, surrogate: SurrogateModel, X: np.ndarray, y: np.ndarray,\n",
    "                   feature_names: List[str], feature_idx: int, search_space: SearchSpace,\n",
    "                   config: Phase2Config):\n",
    "        \"\"\"1D slice through GP (other features at mean).\"\"\"\n",
    "        feature_name = feature_names[feature_idx]\n",
    "        if feature_name not in search_space.continuous_features:\n",
    "            return\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(8, 5))\n",
    "        \n",
    "        n_test = 100\n",
    "        X_mean = X.mean(axis=0)\n",
    "        bounds = search_space.bounds[feature_name]\n",
    "        test_values = np.linspace(bounds['min'], bounds['max'], n_test)\n",
    "        \n",
    "        X_test = np.tile(X_mean, (n_test, 1))\n",
    "        X_test[:, feature_idx] = test_values\n",
    "        \n",
    "        mu, sigma = surrogate.predict(X_test, return_std=True)\n",
    "        \n",
    "        ax.fill_between(test_values, mu - 2*sigma, mu + 2*sigma, alpha=0.3, label='95% CI')\n",
    "        ax.plot(test_values, mu, 'b-', linewidth=2, label='GP Mean')\n",
    "        ax.scatter(X[:, feature_idx], y, c='red', s=50, zorder=5, edgecolors='black', label='Observed')\n",
    "        \n",
    "        # NEW: Add target range visualization\n",
    "        if config.target_range is not None:\n",
    "            L, U = config.target_range\n",
    "            ax.axhline(y=L, color='g', linestyle='--', alpha=0.7, label=f'Target Range')\n",
    "            ax.axhline(y=U, color='g', linestyle='--', alpha=0.7)\n",
    "            ax.fill_between(test_values, L, U, alpha=0.1, color='green')\n",
    "        \n",
    "        ax.set_xlabel(feature_name)\n",
    "        ax.set_ylabel('Response')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.output_dir / f'gp_1d_{feature_name}.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_gp_2d(self, surrogate: SurrogateModel, X: np.ndarray, y: np.ndarray,\n",
    "                   feature_names: List[str], idx1: int, idx2: int, search_space: SearchSpace,\n",
    "                   config: Phase2Config):\n",
    "        \"\"\"2D contour of GP surface.\"\"\"\n",
    "        feat_1, feat_2 = feature_names[idx1], feature_names[idx2]\n",
    "        \n",
    "        if feat_1 not in search_space.continuous_features or \\\n",
    "           feat_2 not in search_space.continuous_features:\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        n_grid = 50\n",
    "        x1_range = np.linspace(search_space.bounds[feat_1]['min'], \n",
    "                               search_space.bounds[feat_1]['max'], n_grid)\n",
    "        x2_range = np.linspace(search_space.bounds[feat_2]['min'], \n",
    "                               search_space.bounds[feat_2]['max'], n_grid)\n",
    "        X1, X2 = np.meshgrid(x1_range, x2_range)\n",
    "        \n",
    "        X_mean = X.mean(axis=0)\n",
    "        X_test = np.tile(X_mean, (n_grid * n_grid, 1))\n",
    "        X_test[:, idx1] = X1.ravel()\n",
    "        X_test[:, idx2] = X2.ravel()\n",
    "        \n",
    "        mu, sigma = surrogate.predict(X_test, return_std=True)\n",
    "        Mu = mu.reshape(n_grid, n_grid)\n",
    "        Sigma = sigma.reshape(n_grid, n_grid)\n",
    "        \n",
    "        # Mean surface\n",
    "        cs1 = axes[0].contourf(X1, X2, Mu, levels=20, cmap='viridis')\n",
    "        axes[0].scatter(X[:, idx1], X[:, idx2], c='red', s=50, edgecolors='white')\n",
    "        axes[0].set_xlabel(feat_1)\n",
    "        axes[0].set_ylabel(feat_2)\n",
    "        axes[0].set_title('GP Mean')\n",
    "        plt.colorbar(cs1, ax=axes[0])\n",
    "        \n",
    "        # NEW: Add contour lines for target range if specified\n",
    "        if config.target_range is not None:\n",
    "            L, U = config.target_range\n",
    "            axes[0].contour(X1, X2, Mu, levels=[L, U], colors='green', linewidths=2, linestyles='--')\n",
    "        \n",
    "        # Uncertainty surface\n",
    "        cs2 = axes[1].contourf(X1, X2, Sigma, levels=20, cmap='YlOrRd')\n",
    "        axes[1].scatter(X[:, idx1], X[:, idx2], c='blue', s=50, edgecolors='white')\n",
    "        axes[1].set_xlabel(feat_1)\n",
    "        axes[1].set_ylabel(feat_2)\n",
    "        axes[1].set_title('GP Uncertainty')\n",
    "        plt.colorbar(cs2, ax=axes[1])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.output_dir / f'gp_2d_{feat_1}_{feat_2}.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671258e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# ExperimentSuggester\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "class ExperimentSuggester:\n",
    "    \"\"\"Formats and exports experiment suggestions.\"\"\"\n",
    "    \n",
    "    def __init__(self, search_space: SearchSpace, config: Phase2Config):\n",
    "        self.search_space = search_space\n",
    "        self.config = config\n",
    "    \n",
    "    def format_suggestions(self, suggested_X: np.ndarray, acq_values: np.ndarray,\n",
    "                           feature_names: List[str], surrogate: SurrogateModel,\n",
    "                           discrete_values: Dict[str, List[Any]]) -> pd.DataFrame:\n",
    "        if len(suggested_X) == 0:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        mu, sigma = surrogate.predict(suggested_X, return_std=True)\n",
    "        \n",
    "        df = pd.DataFrame(suggested_X, columns=feature_names)\n",
    "        \n",
    "        # Round continuous features\n",
    "        for col in self.search_space.continuous_features:\n",
    "            df[col] = df[col].round(4)\n",
    "        \n",
    "        # For discrete features, snap to nearest allowed value\n",
    "        for col in self.search_space.discrete_features:\n",
    "            if col in discrete_values:\n",
    "                allowed = discrete_values[col]\n",
    "                # Snap to nearest allowed value\n",
    "                df[col] = df[col].apply(lambda x: min(allowed, key=lambda v: abs(float(v) - x)))\n",
    "        \n",
    "        df['predicted_response'] = mu.round(4)\n",
    "        df['uncertainty'] = sigma.round(4)\n",
    "        df['acquisition_value'] = acq_values.round(6)\n",
    "        \n",
    "        # NEW: Add range probability if target_range is specified\n",
    "        if self.config.target_range is not None:\n",
    "            L, U = self.config.target_range\n",
    "            # Compute probability of being in range\n",
    "            Z_L = (L - mu) / np.maximum(sigma, 1e-9)\n",
    "            Z_U = (U - mu) / np.maximum(sigma, 1e-9)\n",
    "            prob_in_range = norm.cdf(Z_U) - norm.cdf(Z_L)\n",
    "            df['prob_in_range'] = prob_in_range.round(4)\n",
    "        \n",
    "        df = df.sort_values('acquisition_value', ascending=False).reset_index(drop=True)\n",
    "        df.insert(0, 'rank', range(1, len(df) + 1))\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def export_csv(self, suggestions_df: pd.DataFrame, iteration: int) -> str:\n",
    "        output_path = Path(self.config.output_dir)\n",
    "        output_path.mkdir(exist_ok=True)\n",
    "        filename = output_path / f'suggestions_iter{iteration}.csv'\n",
    "        suggestions_df.to_csv(filename, index=False)\n",
    "        return str(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d77841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Results Container\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "@dataclass\n",
    "class Phase2Results:\n",
    "    \"\"\"Container for pipeline outputs.\"\"\"\n",
    "    iteration: int\n",
    "    suggestions: pd.DataFrame\n",
    "    current_best: float\n",
    "    previous_best: Optional[float]\n",
    "    improved: bool\n",
    "    should_continue: bool\n",
    "    no_improvement_count: int\n",
    "    model_metrics: Dict[str, float]\n",
    "    suggestions_file: str\n",
    "    # NEW: Range optimization metrics\n",
    "    in_range_count: Optional[int] = None\n",
    "    in_range_percentage: Optional[float] = None\n",
    "    optimization_mode: str = \"maximize\"  # \"maximize\", \"minimize\", or \"range\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc993ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Main Pipeline\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "class Phase2Pipeline:\n",
    "    \"\"\"\n",
    "    Bayesian Optimization pipeline for chemical experiments.\n",
    "    \n",
    "    NEW FEATURES:\n",
    "    1. Range optimization: Optimize response toward a target range [L, U]\n",
    "    2. Discrete features: User-specified discrete features with allowed values\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: Phase2Config):\n",
    "        self.config = config\n",
    "        self._output_dir = Path(config.output_dir)\n",
    "        self._output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        self._loader = DataLoader()\n",
    "        self._checkpoint_mgr = Phase2Checkpoint(config.output_dir)\n",
    "        self._plotter = Phase2Plotter(config.output_dir)\n",
    "        self._tracker = ConvergenceTracker(config)\n",
    "        \n",
    "        self._X = None\n",
    "        self._y = None\n",
    "        self._feature_names = []\n",
    "        self._discrete_features = []\n",
    "        self._continuous_features = []\n",
    "        self._discrete_values = {}\n",
    "        self._search_space = None\n",
    "        self._surrogate = None\n",
    "        self._iteration = 0\n",
    "    \n",
    "    def run(self, data_file: str) -> Phase2Results:\n",
    "        \"\"\"Run one BO iteration.\"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"Phase 2: Bayesian Optimization (Enhanced)\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Print optimization mode\n",
    "        if self.config.target_range is not None:\n",
    "            print(f\"  Mode: Range Optimization [{self.config.target_range[0]}, {self.config.target_range[1]}]\")\n",
    "            opt_mode = \"range\"\n",
    "        elif self.config.maximize_response:\n",
    "            print(\"  Mode: Maximization\")\n",
    "            opt_mode = \"maximize\"\n",
    "        else:\n",
    "            print(\"  Mode: Minimization\")\n",
    "            opt_mode = \"minimize\"\n",
    "        \n",
    "        # Print discrete features config\n",
    "        if self.config.discrete_features:\n",
    "            print(f\"  Discrete Features: {list(self.config.discrete_features.keys())}\")\n",
    "        \n",
    "        # Load checkpoint if continuing\n",
    "        previous_best = None\n",
    "        if not self.config.fresh_start and self._checkpoint_mgr.exists():\n",
    "            self._load_checkpoint()\n",
    "            previous_best = self._tracker.best_response\n",
    "            print(f\"  Loaded checkpoint (iteration {self._iteration})\")\n",
    "        \n",
    "        self._load_data(data_file)\n",
    "        self._iteration += 1\n",
    "        \n",
    "        print(f\"\\n  Iteration: {self._iteration}\")\n",
    "        print(f\"  Experiments: {len(self._X)}\")\n",
    "        \n",
    "        # Build search space\n",
    "        self._search_space = SearchSpace(\n",
    "            self._X, self._discrete_features, self._discrete_values,\n",
    "            self._continuous_features, self.config.bounds_margin\n",
    "        )\n",
    "        \n",
    "        # Fit surrogate\n",
    "        self._fit_surrogate()\n",
    "        \n",
    "        # Find current best and range metrics\n",
    "        in_range_count = None\n",
    "        in_range_pct = None\n",
    "        \n",
    "        if self.config.target_range is not None:\n",
    "            L, U = self.config.target_range\n",
    "            in_range_mask = (self._y >= L) & (self._y <= U)\n",
    "            in_range_count = in_range_mask.sum()\n",
    "            in_range_pct = in_range_count / len(self._y) * 100\n",
    "            \n",
    "            # Best is the one closest to range (or in range with lowest variance)\n",
    "            if in_range_count > 0:\n",
    "                # Pick one in range (could be center of range)\n",
    "                in_range_responses = self._y[in_range_mask]\n",
    "                target_center = (L + U) / 2\n",
    "                current_best = in_range_responses.iloc[(in_range_responses - target_center).abs().argmin()]\n",
    "            else:\n",
    "                # Pick closest to range\n",
    "                distances = np.minimum(np.abs(self._y - L), np.abs(self._y - U))\n",
    "                distances[in_range_mask] = 0\n",
    "                current_best = self._y.iloc[distances.argmin()]\n",
    "            \n",
    "            print(f\"  Current best: {current_best:.4f}\")\n",
    "            print(f\"  In range: {in_range_count}/{len(self._y)} ({in_range_pct:.1f}%)\")\n",
    "        else:\n",
    "            if self.config.maximize_response:\n",
    "                current_best = self._y.max()\n",
    "            else:\n",
    "                current_best = self._y.min()\n",
    "            print(f\"  Current best: {current_best:.4f}\")\n",
    "        \n",
    "        # Generate suggestions\n",
    "        suggestions_df = self._generate_suggestions(current_best)\n",
    "        \n",
    "        # Export\n",
    "        suggester = ExperimentSuggester(self._search_space, self.config)\n",
    "        suggestions_file = suggester.export_csv(suggestions_df, self._iteration)\n",
    "        print(f\"  Suggestions saved: {suggestions_file}\")\n",
    "        \n",
    "        # Update tracker\n",
    "        self._tracker.record_iteration(\n",
    "            self._iteration, current_best, len(self._X), suggestions_df,\n",
    "            in_range_count=in_range_count or 0, total_count=len(self._y)\n",
    "        )\n",
    "        \n",
    "        # Check improvement\n",
    "        improved = False\n",
    "        if previous_best is not None:\n",
    "            improved = self._tracker._is_improvement(current_best, previous_best)\n",
    "        \n",
    "        # Save checkpoint\n",
    "        self._save_checkpoint(current_best)\n",
    "        \n",
    "        # Generate plots\n",
    "        self._generate_plots()\n",
    "        \n",
    "        # Print status\n",
    "        self._print_status(current_best, previous_best, improved, in_range_count, in_range_pct)\n",
    "        \n",
    "        return Phase2Results(\n",
    "            iteration=self._iteration,\n",
    "            suggestions=suggestions_df,\n",
    "            current_best=current_best,\n",
    "            previous_best=previous_best,\n",
    "            improved=improved,\n",
    "            should_continue=not self._tracker.should_stop,\n",
    "            no_improvement_count=self._tracker.no_improvement_count,\n",
    "            model_metrics=self._surrogate.score(self._X.values, self._y.values),\n",
    "            suggestions_file=suggestions_file,\n",
    "            in_range_count=in_range_count,\n",
    "            in_range_percentage=in_range_pct,\n",
    "            optimization_mode=opt_mode\n",
    "        )\n",
    "    \n",
    "    def _load_data(self, data_file: str):\n",
    "        print(f\"\\n  Loading: {data_file}\")\n",
    "        \n",
    "        df = self._loader.load_excel(data_file, self.config.sheet_name, self.config.header_row)\n",
    "        \n",
    "        if self.config.split_keyword:\n",
    "            df, _ = self._loader.split_at_keyword(df, self.config.split_keyword)\n",
    "        \n",
    "        df = self._loader.clean_data(df)\n",
    "        \n",
    "        feature_cols = self._loader.get_feature_columns(\n",
    "            df, self.config.stop_feature, self.config.response_column\n",
    "        )\n",
    "        \n",
    "        # NEW: Use discrete_features from config\n",
    "        self._discrete_features, self._continuous_features, self._discrete_values = \\\n",
    "            self._loader.classify_features(df, feature_cols, self.config.discrete_features)\n",
    "        \n",
    "        self._feature_names = self._continuous_features + self._discrete_features\n",
    "        self._X = df[self._feature_names].copy()\n",
    "        self._y = df[self.config.response_column].copy()\n",
    "        \n",
    "        # Drop missing response\n",
    "        valid = ~self._y.isnull()\n",
    "        self._X = self._X[valid].reset_index(drop=True)\n",
    "        self._y = self._y[valid].reset_index(drop=True)\n",
    "        \n",
    "        print(f\"  Features: {len(self._feature_names)} \"\n",
    "              f\"({len(self._continuous_features)} continuous, {len(self._discrete_features)} discrete)\")\n",
    "        \n",
    "        if self._discrete_features:\n",
    "            for feat in self._discrete_features:\n",
    "                print(f\"    - {feat}: {self._discrete_values[feat]}\")\n",
    "    \n",
    "    def _fit_surrogate(self):\n",
    "        print(\"\\n  Fitting GP model...\")\n",
    "        self._surrogate = SurrogateModel()\n",
    "        self._surrogate.fit(self._X.values, self._y.values)\n",
    "        metrics = self._surrogate.score(self._X.values, self._y.values)\n",
    "        print(f\"  R²: {metrics['r2']:.4f}, RMSE: {metrics['rmse']:.4f}\")\n",
    "    \n",
    "    def _generate_suggestions(self, current_best: float) -> pd.DataFrame:\n",
    "        print(f\"\\n  Generating {self.config.n_suggestions} suggestions ({self.config.selection_strategy})...\")\n",
    "        \n",
    "        optimizer = AcquisitionOptimizer(self._surrogate, self._search_space, self.config)\n",
    "        optimizer.set_best_y(current_best)\n",
    "        \n",
    "        suggested_X, acq_values = optimizer.find_next_points(\n",
    "            self.config.n_suggestions, self._X.values\n",
    "        )\n",
    "        \n",
    "        suggester = ExperimentSuggester(self._search_space, self.config)\n",
    "        return suggester.format_suggestions(\n",
    "            suggested_X, acq_values, self._feature_names,\n",
    "            self._surrogate, self._discrete_values\n",
    "        )\n",
    "    \n",
    "    def _save_checkpoint(self, current_best: float):\n",
    "        state = {\n",
    "            'iteration': self._iteration,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'n_experiments': len(self._X),\n",
    "            'best_response': current_best,\n",
    "            'no_improvement_count': self._tracker.no_improvement_count,\n",
    "            'should_stop': self._tracker.should_stop,\n",
    "            'features': self._feature_names,\n",
    "            'discrete_features': self._discrete_features,\n",
    "            'continuous_features': self._continuous_features,\n",
    "            'discrete_values': self._discrete_values,\n",
    "            'history': self._tracker.history,\n",
    "            # NEW: Save range config\n",
    "            'target_range': self.config.target_range\n",
    "        }\n",
    "        self._checkpoint_mgr.save(state)\n",
    "    \n",
    "    def _load_checkpoint(self):\n",
    "        state = self._checkpoint_mgr.load()\n",
    "        if state:\n",
    "            self._iteration = state['iteration']\n",
    "            self._tracker.load_history(state.get('history', []))\n",
    "    \n",
    "    def _generate_plots(self):\n",
    "        print(\"\\n  Generating plots...\")\n",
    "        \n",
    "        if len(self._tracker.history) > 0:\n",
    "            self._plotter.plot_convergence(self._tracker, self.config)\n",
    "        \n",
    "        # 1D plots for first 3 continuous features\n",
    "        for i, feat in enumerate(self._continuous_features[:3]):\n",
    "            feat_idx = self._feature_names.index(feat)\n",
    "            self._plotter.plot_gp_1d(\n",
    "                self._surrogate, self._X.values, self._y.values,\n",
    "                self._feature_names, feat_idx, self._search_space, self.config\n",
    "            )\n",
    "        \n",
    "        # 2D plot if 2+ continuous features\n",
    "        if len(self._continuous_features) >= 2:\n",
    "            idx1 = self._feature_names.index(self._continuous_features[0])\n",
    "            idx2 = self._feature_names.index(self._continuous_features[1])\n",
    "            self._plotter.plot_gp_2d(\n",
    "                self._surrogate, self._X.values, self._y.values,\n",
    "                self._feature_names, idx1, idx2, self._search_space, self.config\n",
    "            )\n",
    "    \n",
    "    def _print_status(self, current_best: float, previous_best: Optional[float], \n",
    "                      improved: bool, in_range_count: Optional[int] = None,\n",
    "                      in_range_pct: Optional[float] = None):\n",
    "        print(\"\\n\" + \"-\" * 60)\n",
    "        print(\"Status\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        if self.config.target_range is not None:\n",
    "            L, U = self.config.target_range\n",
    "            print(f\"  Target Range: [{L}, {U}]\")\n",
    "            if in_range_count is not None:\n",
    "                print(f\"  In Range: {in_range_count} experiments ({in_range_pct:.1f}%)\")\n",
    "        \n",
    "        if previous_best is not None:\n",
    "            arrow = \"↑\" if improved else \"→\"\n",
    "            print(f\"  Best: {previous_best:.4f} {arrow} {current_best:.4f}\")\n",
    "            print(f\"  Improved: {'Yes' if improved else 'No'}\")\n",
    "        else:\n",
    "            print(f\"  Best: {current_best:.4f} (first iteration)\")\n",
    "        \n",
    "        print(f\"  No improvement: {self._tracker.no_improvement_count}/{self.config.patience}\")\n",
    "        \n",
    "        if self._tracker.should_stop:\n",
    "            print(\"\\n  ⚠ STOPPING CRITERION MET\")\n",
    "        else:\n",
    "            print(\"\\n  ✓ Continue with suggested experiments\")\n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Clear checkpoint and start fresh.\"\"\"\n",
    "        self._checkpoint_mgr.clear()\n",
    "        self._tracker = ConvergenceTracker(self.config)\n",
    "        self._iteration = 0\n",
    "        print(\"  Pipeline reset.\")\n",
    "    \n",
    "    @property\n",
    "    def iteration(self) -> int:\n",
    "        return self._iteration\n",
    "    \n",
    "    @property\n",
    "    def history(self) -> pd.DataFrame:\n",
    "        return self._tracker.get_history_df()\n",
    "    \n",
    "    @property\n",
    "    def bounds(self) -> pd.DataFrame:\n",
    "        if self._search_space:\n",
    "            return self._search_space.get_bounds_df()\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a076f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Convenience Function (Enhanced)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def run_bo_iteration(data_file: str, response_column: str, \n",
    "                     n_suggestions: int = 5, maximize: bool = True,\n",
    "                     target_range: Optional[Tuple[float, float]] = None,\n",
    "                     discrete_features: Optional[Dict[str, List[Any]]] = None,\n",
    "                     strategy: str = 'diverse', output_dir: str = 'bo_results',\n",
    "                     bounds_margin: float = 0.1,\n",
    "                     sheet_name: str = 'data',\n",
    "                     header_row: int = 5,\n",
    "                     stop_feature: Optional[str] = None,\n",
    "                     split_keyword: Optional[str] = \"PREDICTED OPTIMUM RUNS\",\n",
    "                     exploration_weight: float = 0.01,\n",
    "                     min_distance: float = 0.1,\n",
    "                     duplicate_threshold: float = 0.05,\n",
    "                     n_optimizer_restarts: int = 25,\n",
    "                     patience: int = 3,\n",
    "                     min_improvement: float = 0.01,\n",
    "                     fresh_start: bool = False,\n",
    "                     **kwargs) -> Phase2Results:\n",
    "    \"\"\"\n",
    "    Quick single-iteration BO run.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_file : str\n",
    "        Path to Excel file containing experimental data.\n",
    "    response_column : str\n",
    "        Name of the column containing the response variable to optimize.\n",
    "    n_suggestions : int, default=5\n",
    "        Number of experiment suggestions to generate.\n",
    "    maximize : bool, default=True\n",
    "        If True, maximize the response. If False, minimize.\n",
    "        Ignored if target_range is specified.\n",
    "    target_range : tuple of (float, float), optional (NEW)\n",
    "        Target range for response optimization as (lower_bound, upper_bound).\n",
    "        If specified, optimizes to get responses within this range.\n",
    "        Overrides the maximize parameter.\n",
    "    discrete_features : dict, optional (NEW)\n",
    "        Dictionary mapping feature names to lists of allowed values.\n",
    "        Example: {'Temperature': [100, 150, 200], 'Catalyst': [1, 2, 3]}\n",
    "        Features not in this dict are treated as continuous.\n",
    "    strategy : str, default='diverse'\n",
    "        Selection strategy: 'diverse' (spread across search space) or \n",
    "        'greedy' (focus on best predicted region).\n",
    "    output_dir : str, default='bo_results'\n",
    "        Directory for output files (suggestions CSV, plots, checkpoints).\n",
    "    bounds_margin : float, default=0.1\n",
    "        Fraction to extend continuous feature bounds beyond observed range.\n",
    "        Allows mild extrapolation.\n",
    "    sheet_name : str, default='data'\n",
    "        Name of Excel sheet containing data.\n",
    "    header_row : int, default=5\n",
    "        Row number (0-indexed) containing column headers.\n",
    "    stop_feature : str, optional\n",
    "        Feature column name where feature list ends (columns after are not features).\n",
    "    split_keyword : str, optional\n",
    "        Keyword in 'Run' column that separates training data from predictions.\n",
    "    exploration_weight : float, default=0.01\n",
    "        Xi parameter for acquisition function. Higher values encourage exploration.\n",
    "    min_distance : float, default=0.1\n",
    "        Minimum normalized distance between suggested points (for diversity).\n",
    "    duplicate_threshold : float, default=0.05\n",
    "        Minimum distance from existing data points to avoid duplicates.\n",
    "    n_optimizer_restarts : int, default=25\n",
    "        Number of random restarts for acquisition function optimization.\n",
    "    patience : int, default=3\n",
    "        Number of iterations without improvement before stopping.\n",
    "    min_improvement : float, default=0.01\n",
    "        Minimum relative improvement (1% default) to count as progress.\n",
    "    fresh_start : bool, default=False\n",
    "        If True, ignore existing checkpoint and start fresh.\n",
    "    **kwargs\n",
    "        Additional arguments passed to Phase2Config.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Phase2Results\n",
    "        Results object containing:\n",
    "        - iteration: Current iteration number\n",
    "        - suggestions: DataFrame of suggested experiments\n",
    "        - current_best: Best response value in current data\n",
    "        - previous_best: Best response from previous iteration (if any)\n",
    "        - improved: Whether improvement was achieved\n",
    "        - should_continue: Whether to continue optimization\n",
    "        - no_improvement_count: Consecutive iterations without improvement\n",
    "        - model_metrics: Dict with 'r2' and 'rmse' of GP model\n",
    "        - suggestions_file: Path to saved suggestions CSV\n",
    "        - in_range_count: Number of experiments in target range (if range optimization)\n",
    "        - in_range_percentage: Percentage of experiments in range (if range optimization)\n",
    "        - optimization_mode: 'maximize', 'minimize', or 'range'\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    # Standard maximization\n",
    "    >>> results = run_bo_iteration('experiments.xlsx', 'Yield', maximize=True)\n",
    "    \n",
    "    # Standard minimization\n",
    "    >>> results = run_bo_iteration('experiments.xlsx', 'Impurity', maximize=False)\n",
    "    \n",
    "    # Range optimization (NEW) - optimize Yield to be between 5.0 and 7.0\n",
    "    >>> results = run_bo_iteration(\n",
    "    ...     'experiments.xlsx', 'Yield',\n",
    "    ...     target_range=(5.0, 7.0)\n",
    "    ... )\n",
    "    \n",
    "    # With discrete features (NEW)\n",
    "    >>> results = run_bo_iteration(\n",
    "    ...     'experiments.xlsx', 'Yield',\n",
    "    ...     discrete_features={\n",
    "    ...         'Temperature': [100, 150, 200, 250],\n",
    "    ...         'Catalyst_Type': [1, 2, 3]\n",
    "    ...     }\n",
    "    ... )\n",
    "    \n",
    "    # Combined: Range optimization with discrete features\n",
    "    >>> results = run_bo_iteration(\n",
    "    ...     'experiments.xlsx', 'Yield',\n",
    "    ...     target_range=(5.0, 7.0),\n",
    "    ...     discrete_features={'Temperature': [100, 150, 200]},\n",
    "    ...     bounds_margin=0.15,\n",
    "    ...     n_suggestions=10\n",
    "    ... )\n",
    "    \"\"\"\n",
    "    config = Phase2Config(\n",
    "        response_column=response_column,\n",
    "        sheet_name=sheet_name,\n",
    "        header_row=header_row,\n",
    "        split_keyword=split_keyword,\n",
    "        stop_feature=stop_feature,\n",
    "        maximize_response=maximize,\n",
    "        target_range=target_range,\n",
    "        discrete_features=discrete_features or {},\n",
    "        n_suggestions=n_suggestions,\n",
    "        selection_strategy=strategy,\n",
    "        min_distance=min_distance,\n",
    "        exploration_weight=exploration_weight,\n",
    "        bounds_margin=bounds_margin,\n",
    "        n_optimizer_restarts=n_optimizer_restarts,\n",
    "        patience=patience,\n",
    "        min_improvement=min_improvement,\n",
    "        duplicate_threshold=duplicate_threshold,\n",
    "        output_dir=output_dir,\n",
    "        fresh_start=fresh_start,\n",
    "        **kwargs\n",
    "    )\n",
    "    return Phase2Pipeline(config).run(data_file)\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Example Usage\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example 1: Standard maximization (original behavior)\n",
    "    config_maximize = Phase2Config(\n",
    "        response_column='Yield',\n",
    "        sheet_name='data',\n",
    "        header_row=5,\n",
    "        stop_feature='Batch ID',\n",
    "        maximize_response=True,\n",
    "        n_suggestions=5,\n",
    "        selection_strategy='diverse',\n",
    "        patience=3,\n",
    "        output_dir='bo_phase2_output'\n",
    "    )\n",
    "\n",
    "    # Example 2: Range optimization (NEW)\n",
    "    # Optimize yield to be between 5.0 and 7.0\n",
    "    config_range = Phase2Config(\n",
    "        response_column='Yield',\n",
    "        sheet_name='data',\n",
    "        header_row=5,\n",
    "        stop_feature='Batch ID',\n",
    "        target_range=(5.0, 7.0),  # NEW: Target range\n",
    "        n_suggestions=5,\n",
    "        selection_strategy='diverse',\n",
    "        patience=3,\n",
    "        output_dir='bo_phase2_range_output'\n",
    "    )\n",
    "\n",
    "    # Example 3: With discrete features (NEW)\n",
    "    # Define discrete features and their allowed values\n",
    "    config_discrete = Phase2Config(\n",
    "        response_column='Yield',\n",
    "        sheet_name='data',\n",
    "        header_row=5,\n",
    "        stop_feature='Batch ID',\n",
    "        maximize_response=True,\n",
    "        discrete_features={  # NEW: Discrete features configuration\n",
    "            'Temperature': [100, 150, 200, 250],\n",
    "            'Catalyst_Loading': [0.5, 1.0, 1.5, 2.0],\n",
    "            'Solvent_Type': [1, 2, 3]  # Encoded categorical\n",
    "        },\n",
    "        bounds_margin=0.15,  # Customizable margin for continuous features\n",
    "        n_suggestions=5,\n",
    "        selection_strategy='diverse',\n",
    "        patience=3,\n",
    "        output_dir='bo_phase2_discrete_output'\n",
    "    )\n",
    "\n",
    "    # Example 4: Combined - Range optimization with discrete features (NEW)\n",
    "    config_combined = Phase2Config(\n",
    "        response_column='Yield',\n",
    "        sheet_name='data',\n",
    "        header_row=5,\n",
    "        stop_feature='Batch ID',\n",
    "        target_range=(5.0, 7.0),  # Target range\n",
    "        discrete_features={  # Discrete features\n",
    "            'Temperature': [100, 150, 200],\n",
    "            'Catalyst_Type': [1, 2, 3]\n",
    "        },\n",
    "        bounds_margin=0.1,\n",
    "        n_suggestions=5,\n",
    "        selection_strategy='diverse',\n",
    "        patience=3,\n",
    "        output_dir='bo_phase2_combined_output'\n",
    "    )\n",
    "\n",
    "    # ─────────────────────────────────────────────────────────────────────────\n",
    "    # Run the pipeline\n",
    "    # ─────────────────────────────────────────────────────────────────────────\n",
    "    \n",
    "    # Uncomment the configuration you want to use:\n",
    "    # pipeline = Phase2Pipeline(config_maximize)\n",
    "    # pipeline = Phase2Pipeline(config_range)\n",
    "    # pipeline = Phase2Pipeline(config_discrete)\n",
    "    # pipeline = Phase2Pipeline(config_combined)\n",
    "    \n",
    "    # Example run:\n",
    "    # results = pipeline.run('experiments.xlsx')\n",
    "    \n",
    "    # Print results:\n",
    "    # print(\"\\n\" + \"=\" * 60)\n",
    "    # print(\"RESULTS SUMMARY\")\n",
    "    # print(\"=\" * 60)\n",
    "    # print(f\"\\nOptimization Mode: {results.optimization_mode}\")\n",
    "    # print(f\"Iteration: {results.iteration}\")\n",
    "    # print(f\"Current Best: {results.current_best:.4f}\")\n",
    "    # \n",
    "    # if results.previous_best is not None:\n",
    "    #     print(f\"Previous Best: {results.previous_best:.4f}\")\n",
    "    #     print(f\"Improved: {results.improved}\")\n",
    "    # \n",
    "    # if results.in_range_count is not None:\n",
    "    #     print(f\"In Range: {results.in_range_count} ({results.in_range_percentage:.1f}%)\")\n",
    "    # \n",
    "    # print(f\"\\nModel Performance:\")\n",
    "    # print(f\"  R²: {results.model_metrics['r2']:.4f}\")\n",
    "    # print(f\"  RMSE: {results.model_metrics['rmse']:.4f}\")\n",
    "    # \n",
    "    # print(f\"\\nSuggestions saved to: {results.suggestions_file}\")\n",
    "    # print(f\"Continue optimization: {results.should_continue}\")\n",
    "    # print(f\"No improvement count: {results.no_improvement_count}\")\n",
    "    # \n",
    "    # print(\"\\nSuggested Experiments:\")\n",
    "    # print(results.suggestions.to_string(index=False))\n",
    "\n",
    "    # ─────────────────────────────────────────────────────────────────────────\n",
    "    # Quick convenience function examples\n",
    "    # ─────────────────────────────────────────────────────────────────────────\n",
    "    \n",
    "    # Standard maximization:\n",
    "    # results = run_bo_iteration('experiments.xlsx', 'Yield', maximize=True)\n",
    "    \n",
    "    # Standard minimization:\n",
    "    # results = run_bo_iteration('experiments.xlsx', 'Impurity', maximize=False)\n",
    "    \n",
    "    # Range optimization (NEW):\n",
    "    # results = run_bo_iteration(\n",
    "    #     'experiments.xlsx', \n",
    "    #     'Yield', \n",
    "    #     target_range=(5.0, 7.0)\n",
    "    # )\n",
    "    \n",
    "    # With discrete features (NEW):\n",
    "    # results = run_bo_iteration(\n",
    "    #     'experiments.xlsx', \n",
    "    #     'Yield',\n",
    "    #     discrete_features={\n",
    "    #         'Temperature': [100, 150, 200],\n",
    "    #         'Pressure': [1.0, 2.0, 3.0, 4.0]\n",
    "    #     }\n",
    "    # )\n",
    "    \n",
    "    # Full example with all new features:\n",
    "    # results = run_bo_iteration(\n",
    "    #     data_file='experiments.xlsx',\n",
    "    #     response_column='Yield',\n",
    "    #     target_range=(5.0, 7.0),\n",
    "    #     discrete_features={\n",
    "    #         'Temperature': [100, 150, 200],\n",
    "    #         'Catalyst_Type': [1, 2, 3]\n",
    "    #     },\n",
    "    #     bounds_margin=0.15,\n",
    "    #     n_suggestions=10,\n",
    "    #     strategy='diverse',\n",
    "    #     output_dir='my_optimization_results',\n",
    "    #     patience=5\n",
    "    # )\n",
    "    \n",
    "    print(\"Pipeline ready. Uncomment the desired configuration and run.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
